{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccba6af-1867-491d-b9d0-d99965738c64",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "id": "5da368cd-3045-4ec0-86fb-2ce15b5d1b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# General includes.\n",
    "import os\n",
    "import gc\n",
    "import random\n",
    "import threading\n",
    "import math\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "#from termcolor import colored, cprint\n",
    "import colored\n",
    "from datetime import datetime, timedelta\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Typing includes.\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "# Numerical includes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# pyHealth includes.\n",
    "from pyhealth.datasets import BaseDataset, MIMIC3Dataset, SampleDataset , split_by_patient\n",
    "from pyhealth.data import Patient, Visit, Event\n",
    "from pyhealth.data import Event, Visit, Patient\n",
    "from pyhealth.datasets.utils import strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 443,
   "id": "95e714b1-ca5d-4d7b-9ace-6b4372adfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertTokenizerFast\n",
    "from transformers import TensorType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83fa50-9dd0-467b-bd36-634795e4a09c",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "ebef3528-0396-459a-8112-b272089f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU_ = False\n",
    "BERT_USE_GPU_ = True  # BERT embeddings \n",
    "DEV_ = True  # Uses a small subset of MIMIC data: https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html#pyhealth.datasets.MIMIC3Dataset\n",
    "GPU_STR_ = 'cuda'\n",
    "# DATA_DIR_ = os.path.join(os.getcwd(), DATA_DIR_)\n",
    "DATA_DIR_ = '~/sw/physionet.org/files/mimiciii/1.4'\n",
    "BATCH_SIZE_ = 32\n",
    "EMBEDDING_DIM_ = 264  # BERT requires a multiple of 12\n",
    "SHUFFLE_ = True\n",
    "SAMPLE_MULTIPLIER_ = 1\n",
    "\n",
    "# TBD cache for BERT embeddings.\n",
    "CACHE_DIR_ = 'cache'\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "seed = 90210\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f8271-44b2-44a2-b7cc-cd7339a70e87",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d5fa9-226e-4612-b28f-ed24de16399a",
   "metadata": {},
   "source": [
    "### Load MIMIC III Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "id": "607ef1d3-caa8-4db8-b664-86172bea936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malignant ascites\n",
      "78951 ancestors ['789.5', '789', '780-789.99', '780-799.99', '001-999.99']\n",
      "Ascites\n",
      "7895 ancestors ['789', '780-789.99', '780-799.99', '001-999.99']\n",
      "Abdominal rigidity\n",
      "7894 ancestors ['789', '780-789.99', '780-799.99', '001-999.99']\n",
      "Abdominal rigidity, right upper quadrant\n",
      "78941 ancestors ['789.4', '789', '780-789.99', '780-799.99', '001-999.99']\n",
      "789.51\n",
      "789.5\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.medcode import InnerMap, ICD9CM\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd9cm.lookup(\"428.0\") # get detailed info\n",
    "icd9cm.get_ancestors(\"428.0\") # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"78951\")) # get detailed info\n",
    "print(f'78951 ancestors {icd9cm.get_ancestors(\"78951\")}') # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"7895\")) # get detailed info\n",
    "print(f'7895 ancestors {icd9cm.get_ancestors(\"7895\")}') # get parents\n",
    "\n",
    "\n",
    "print(icd9cm.lookup(\"7894\")) # get detailed info\n",
    "print(f'7894 ancestors {icd9cm.get_ancestors(\"7894\")}') # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"78941\")) # get detailed info\n",
    "print(f'78941 ancestors {icd9cm.get_ancestors(\"78941\")}') # get parents\n",
    "\n",
    "print(ICD9CM.standardize('78951'))\n",
    "print(ICD9CM.standardize('7895'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "id": "dcfb3443-8dfa-4149-904b-04fc24d7a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_duration_minutes(start_datetime: str, end_datetime: str) -> float:\n",
    "    '''Return duration in minutes as a float.\n",
    "    '''\n",
    "    # MIMIC-III uses the following format: 2146-07-22 00:00:00\n",
    "    start = datetime.strptime(start_datetime, '%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strptime(end_datetime,   '%Y-%m-%d %H:%M:%S')\n",
    "    return float((end - start).seconds)\n",
    "\n",
    "class MIMIC3DatasetWrapper(MIMIC3Dataset):\n",
    "    ''' Add extra tables to the MIMIC III dataset.\n",
    "    \n",
    "      Some of the tables we need like \"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\"\n",
    "      are not supported out of the box. \n",
    "      \n",
    "      This class defines parsing methods to extract text data from these extra tables.\n",
    "      The text data is generally joined on the PATIENTID, HADMID, ITEMID to match the\n",
    "      pyHealth Vists class representation.\n",
    "    '''\n",
    "   \n",
    "    # We need to add storage for text-based lookup tables here.\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._valid_text_tables = [\"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\", \"D_LABITEMS\"]\n",
    "        self._text_descriptions = {x: {} for x in self._valid_text_tables}\n",
    "        self._text_luts = {x: {} for x in self._valid_text_tables}\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def get_all_tables(self) -> List[str]: \n",
    "        return list(self._text_descriptions.keys())\n",
    "        \n",
    "    def get_text_dict(self, table_name: str) -> Dict[str, Dict[Any, Any]]:\n",
    "        return self._text_descriptions.get(table_name)\n",
    "    \n",
    "    def set_text_lut(self, table_name: str, lut: Dict[Any, Any]) -> None:\n",
    "        self._text_luts[table_name] = lut\n",
    "    \n",
    "    def get_text_lut(self, table_name: str) -> Dict[Any, Any]:\n",
    "        return self._text_luts[table_name]\n",
    "    \n",
    "#     def _validate(self) -> Dict:\n",
    "#         \"\"\"Helper function which validates the samples.\n",
    "#         Will be called in `self.__init__()`.\n",
    "#         Returns:\n",
    "#             input_info: Dict, a dict whose keys are the same as the keys in the\n",
    "#                 samples, and values are the corresponding input information:\n",
    "#                 - \"type\": the element type of each key attribute, one of float,\n",
    "#                     int, str.\n",
    "#                 - \"dim\": the list dimension of each key attribute, one of 0, 1, 2, 3.\n",
    "#                 - \"len\": the length of the vector, only valid for vector-based\n",
    "#                     attributes.\n",
    "#         \"\"\"\n",
    "#         \"\"\" 1. Check if all samples are of type dict. \"\"\"\n",
    "#         assert all(\n",
    "#             [isinstance(s, dict) for s in self.samples],\n",
    "#         ), \"Each sample should be a dict\"\n",
    "#         keys = self.samples[0].keys()\n",
    "\n",
    "#         \"\"\" 2. Check if all samples have the same keys. \"\"\"\n",
    "#         assert all(\n",
    "#             [set(s.keys()) == set(keys) for s in self.samples]\n",
    "#         ), \"All samples should have the same keys\"\n",
    "\n",
    "#         \"\"\" 3. Check if \"patient_id\" and \"visit_id\" are in the keys.\"\"\"\n",
    "#         assert \"patient_id\" in keys, \"patient_id should be in the keys\"\n",
    "#         assert \"visit_id\" in keys, \"visit_id should be in the keys\"\n",
    "#         return {}\n",
    "\n",
    "    \n",
    "    def _add_events_to_patient_dict(\n",
    "        self,\n",
    "        patient_dict: Dict[str, Patient],\n",
    "        group_df: pd.DataFrame,\n",
    "    ) -> Dict[str, Patient]:\n",
    "        #TODO(botelho3) Imported from PyHealth Base dataset githubf to\n",
    "        #support parse_prescription\n",
    "        \"\"\"Helper function which adds the events column of a df.groupby object to the patient dict.\n",
    "        \n",
    "        Will be called at the end of each `self.parse_[table_name]()` function.\n",
    "        Args:\n",
    "            patient_dict: a dict mapping patient_id to `Patient` object.\n",
    "            group_df: a df.groupby object, having two columns: patient_id and events.\n",
    "                - the patient_id column is the index of the patient\n",
    "                - the events column is a list of <Event> objects\n",
    "        Returns:\n",
    "            The updated patient dict.\n",
    "        \"\"\"\n",
    "        for _, events in group_df.items():\n",
    "            for event in events:\n",
    "                patient_dict = self._add_event_to_patient_dict(patient_dict, event)\n",
    "        return patient_dict\n",
    "\n",
    "    \n",
    "    def parse_prescriptions(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:\n",
    "        \"\"\"Helper function which parses PRESCRIPTIONS table.\n",
    "        \n",
    "        TODO(botelho3) - we have to override this to include the text fields. The\n",
    "        prescriptions table does not link to a separate D_ICD_* table in MIMIC-III\n",
    "        thtat contains text descriptions of the prescription. The text descriptions\n",
    "        are in the columns of this table. Regular pyHealth ignores these columns. We\n",
    "        override this method to appent pyHealth Event objects containing the text\n",
    "        columns to each patient.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - PRESCRIPTIONS: https://mimic.mit.edu/docs/iii/tables/prescriptions/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The updated patients dict.\n",
    "        \"\"\"\n",
    "        table = \"PRESCRIPTIONS\"\n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            low_memory=False,\n",
    "            dtype={\"SUBJECT_ID\": str, \"HADM_ID\": str, \"NDC\": str,\n",
    "                   \"DRUG_TYPE\": str, \"DRUG\": str,\n",
    "                   \"PROD_STRENGTH\": str, \"ROUTE\": str, \"ENDDATE\": str},\n",
    "        )\n",
    "        # drop records of the other patients\n",
    "        df = df[df[\"SUBJECT_ID\"].isin(patients.keys())]\n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"SUBJECT_ID\", \"HADM_ID\", \"NDC\", \"DRUG_TYPE\", \"DRUG\"])\n",
    "        # sort by start date and end date\n",
    "        df = df.sort_values(\n",
    "            [\"SUBJECT_ID\", \"HADM_ID\", \"STARTDATE\", \"ENDDATE\"], ascending=True\n",
    "        )\n",
    "        # group by patient and visit\n",
    "        group_df = df.groupby(\"SUBJECT_ID\")\n",
    "        \n",
    "        # parallel unit for prescription (per patient)\n",
    "        def prescription_unit(p_id, p_info):\n",
    "            events = []\n",
    "            for v_id, v_info in p_info.groupby(\"HADM_ID\"):\n",
    "                zipped = zip(v_info[\"STARTDATE\"], v_info[\"NDC\"], v_info[\"DRUG_TYPE\"],\n",
    "                             v_info[\"DRUG\"], v_info[\"PROD_STRENGTH\"], v_info[\"ROUTE\"],\n",
    "                             v_info[\"ENDDATE\"])\n",
    "                for startdate, code, dtype, dname, dose, route, enddate in zipped:\n",
    "                    if not type(startdate) == str:\n",
    "                        startdate = '2142-07-18 00:00:00'\n",
    "                    if not type(enddate) == str:\n",
    "                        enddate = '2142-07-18 00:00:00'\n",
    "                    assert(type(dname) is str)\n",
    "                    # if not type(enddate) is str:\n",
    "                    #     print(f'Not matching enddate {enddate} startdate {startdate}')\n",
    "                    #     print(f'dname {dname}, hadm_id {v_id}, p_id {p_id}')\n",
    "                    assert(type(startdate) is str)\n",
    "                    assert(type(enddate) is str)\n",
    "                    event = Event(\n",
    "                        code=code,\n",
    "                        table=table,\n",
    "                        vocabulary=\"NDC\",\n",
    "                        visit_id=v_id,\n",
    "                        patient_id=p_id,\n",
    "                        timestamp=strptime(startdate),\n",
    "                        dtype=dtype,\n",
    "                        dname=dname,\n",
    "                        dose=dose,\n",
    "                        route=route,\n",
    "                        duration=_compute_duration_minutes(startdate, enddate),\n",
    "                    )\n",
    "                    events.append(event)\n",
    "            return events\n",
    "\n",
    "                # parallel apply\n",
    "        group_df = group_df.parallel_apply(\n",
    "            lambda x: prescription_unit(x.SUBJECT_ID.unique()[0], x)\n",
    "        )\n",
    "\n",
    "        # summarize the results\n",
    "        # print(f'BASE {dir(BaseDataset)}')\n",
    "        # print(f'MIMIC {dir(MIMIC3Dataset)}')\n",
    "        # print(f'WRAPPER {dir(self)}')\n",
    "        # # patients = BaseDataset._add_events_to_patient_dict(patients, group_df)\n",
    "        # # patients = self._add_events_to_patient_dict(patients, group_df)\n",
    "        patients = self._add_events_to_patient_dict(patients, group_df)\n",
    "        return patients\n",
    "    \n",
    "    # Note the name has to match the table name exactly.\n",
    "    # See https://github.com/sunlabuiuc/PyHealth/blob/master/pyhealth/datasets/mimic3.py#L71.\n",
    "    def parse_d_icd_diagnoses(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_DIAGNOSIS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_DIAGNOSIS: https://mimic.mit.edu/docs/iii/tables/d_icd_diagnoses/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_DIAGNOSES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text diagnosis description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_DIAGNOSES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    def parse_d_labitems(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_LABITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_LABITEMS: https://mimic.mit.edu/docs/iii/tables/d_labitems/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_LABITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text lab measurement description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_LABITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str, \"FLUID\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_items(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        # TODO(botelho3) - Note this may not be totally useable because the ITEMID\n",
    "        # uinqiue key only links to these tables using ITEMID\n",
    "        #   - INPUTEVENTS_MV \n",
    "        #   - OUTPUTEVENTS on ITEMID\n",
    "        #   - PROCEDUREEVENTS_MV on ITEMID\n",
    "        # \n",
    "        # Not to the tables we want e.g. \n",
    "        \"\"\"Helper function which parses D_ITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ITEMS: https://mimic.mit.edu/docs/iii/tables/d_items/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text inputs/output/procedure events lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_icd_procedures(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_PROCEDURES table.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_PROCEDURES: https://mimic.mit.edu/docs/iii/tables/d_icd_procedures/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_PROCEDURES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text procedure description lookup for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_PROCEDURES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "id": "427a42a6-441e-438c-96f4-b38ba82fd192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: `/home/abot/cs598dlh/github_mirror_cs598dlh/cs598_desc_emb_project/src/~/sw/physionet.org/files/mimiciii/1.4`\n",
      "\n",
      "Statistics of base dataset (dev=True):\n",
      "\t- Dataset: MIMIC3DatasetWrapper\n",
      "\t- Number of patients: 1000\n",
      "\t- Number of visits: 1295\n",
      "\t- Number of visits per patient: 1.2950\n",
      "\t- Number of events per visit in D_ICD_DIAGNOSES: 0.0000\n",
      "\t- Number of events per visit in D_ICD_PROCEDURES: 0.0000\n",
      "\t- Number of events per visit in D_ITEMS: 0.0000\n",
      "\t- Number of events per visit in D_LABITEMS: 0.0000\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 9.3544\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 4.3351\n",
      "\t- Number of events per visit in PRESCRIPTIONS: 59.2556\n",
      "\n",
      "\n",
      "dataset.patients: patient_id -> <Patient>\n",
      "\n",
      "<Patient>\n",
      "    - visits: visit_id -> <Visit> \n",
      "    - other patient-level info\n",
      "    \n",
      "    <Visit>\n",
      "        - event_list_dict: table_name -> List[Event]\n",
      "        - other visit-level info\n",
      "    \n",
      "        <Event>\n",
      "            - code: str\n",
      "            - other event-level info\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Reading data from: `{os.path.join(os.getcwd(), DATA_DIR_)}`')\n",
    "\n",
    "mimic3base = MIMIC3DatasetWrapper(\n",
    "    # root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
    "    root=DATA_DIR_,\n",
    "    tables=[\"D_ICD_DIAGNOSES\", \"D_ICD_PROCEDURES\", \"D_ITEMS\", \"D_LABITEMS\",\n",
    "            \"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"], # \"LABEVENTS\"],\n",
    "    # map all NDC codes to ATC 3-rd level codes in these tables\n",
    "    # See https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System.\n",
    "    code_mapping={\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})},\n",
    "    # Reads a subset of the data. Disable for full training run.\n",
    "    dev = DEV_,\n",
    "    # Slow, rebuilds the dataset instead of caching.\n",
    "    refresh_cache=False,\n",
    ")\n",
    "\n",
    "mimic3base.stat()\n",
    "mimic3base.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "id": "4cef4476-6bc8-4791-b246-4c329a80a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_ICD_DIAGNOSES', 'D_ITEMS', 'D_ICD_PROCEDURES', 'D_LABITEMS']\n",
      "\u001b[92m====Tables====\n",
      "\u001b[0m\n",
      "Table: D_ICD_DIAGNOSES\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'data'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[448], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m     d \u001b[38;5;241m=\u001b[39m mimic3base\u001b[38;5;241m.\u001b[39mget_text_dict(t)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTable: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43md\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[:\u001b[38;5;241m5\u001b[39m])\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Take the cached tables from the parse_tables function and build the {ICD9 -> (short_name, long_name)}\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# lookup tables.\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data'"
     ]
    }
   ],
   "source": [
    "table_names = mimic3base.get_all_tables()\n",
    "print(table_names)\n",
    "\n",
    "print('\\033[92m' '====Tables====\\n' '\\033[0m')\n",
    "# print(colored('====Tables====\\n', 'green'))\n",
    "# print(colored.fg('green') + '====Tables====\\n')\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_dict(t)\n",
    "    print(f\"Table: {t}\")\n",
    "    print(d['data'][:5])\n",
    "    print('\\n\\n')\n",
    "\n",
    "# Take the cached tables from the parse_tables function and build the {ICD9 -> (short_name, long_name)}\n",
    "# lookup tables.\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_dict(t)\n",
    "    d = d['data']\n",
    "    lut = {record[0]: record[1:] for record in d}\n",
    "    mimic3base.set_text_lut(t,  lut)\n",
    "    \n",
    "print('\\033[92m' '====Luts====\\n' '\\033[0m')\n",
    "# print(f'{colored.fg(\"green\")} ====Luts====\\n')\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_lut(t)\n",
    "    print(f\"Lut {t}:\\n{dict(itertools.islice(d.items(), 2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b8e28-23ac-4ac1-a9b6-ae546e28c905",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Declare tasks for 2 of the 5 prediction tasks specified in the paper. We will create dataloaders for each task that contain the ICD codes and the raw text for each (patient, visit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69f4f2-566a-428f-800a-8539fbb16a15",
   "metadata": {},
   "source": [
    "#### CodeEMB Pred tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "id": "08692aad-db67-487b-9b26-dbde0ab6adce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "def readmission_pred_task(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_task(MORTALITY_PER_VISIT_ICD_9_CODE_COUNT_, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient) - 1):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit = patient[i]\n",
    "        next_visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        if next_visit.discharge_status not in [0, 1]:\n",
    "            mortality_label = 0\n",
    "        else:\n",
    "            mortality_label = int(next_visit.discharge_status)\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": mortality_label,\n",
    "            }\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "def mortality_pred_task_cemb(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    each sample is a list of vists for 1 patient.\n",
    "    \"\"\"\n",
    "   \n",
    "    # TODO(botelho3) - stupid hack around the limitations of SampleDataset validator.\n",
    "    kMaxVisits = 5\n",
    "    if len(patient) < 1 or len(patient) > kMaxVisits:\n",
    "        return []\n",
    "        \n",
    "    samples = [{\n",
    "        \"visit_id\": patient[0].visit_id,\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"num_visits\": 0,\n",
    "        \"conditions\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures\": [[] for v in range(kMaxVisits)],\n",
    "        \"conditions_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs_text\": [[] for v in range(kMaxVisits)],\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": 0,\n",
    "    }]\n",
    "    \n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "\n",
    "    # loop over all visits\n",
    "    out_idx = 0\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        # Each field is a list of visits.\n",
    "        samples[0]['num_visits'] = samples[0]['num_visits'] + 1\n",
    "        samples[0]['conditions'][out_idx]=conditions\n",
    "        samples[0]['procedures'][out_idx]=procedures\n",
    "        samples[0]['conditions_text'][out_idx]= conditions_text\n",
    "        samples[0]['procedures_text'][out_idx] = procedures_text\n",
    "        samples[0]['drugs'][out_idx] = drugs\n",
    "        samples[0]['drugs_text'][out_idx] = drugs_text\n",
    "        out_idx = out_idx + 1\n",
    "    samples[0]['label'] = global_mortality_label\n",
    "   \n",
    "    # Record all unique codes and their frequency for LUT.\n",
    "    for visit in samples[0]['conditions']:\n",
    "        for code in visit:\n",
    "            CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "           \n",
    "    # If none of the samples met the criteria return an empty list.\n",
    "    if samples[0]['num_visits'] == 0:\n",
    "        return []\n",
    "\n",
    "    # Potentially multiply the sample n-times to increase dataset size.\n",
    "    samples.extend(\n",
    "        list(deepcopy(samples[0]) for s in range(SAMPLE_MULTIPLIER_-1))\n",
    "    )\n",
    "        \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30d309-9b95-4a1b-82ee-ae141960c868",
   "metadata": {},
   "source": [
    "#### DescEmb Pred Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "id": "634ef5a4-fa6d-4720-a4e7-e91cc3746ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "#{code: idx for idx, code in enumerate(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys())}\n",
    "def readmission_pred_task_per_patient(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "   \n",
    "    # Length 1 patients by defn are not readmitted.\n",
    "    if len(patient) < 1:\n",
    "        return samples\n",
    "\n",
    "    # we will drop the last visit\n",
    "    global_readmission_label = 0\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        global_readmission_label |= readmission_label  \n",
    "       \n",
    "    for i in range(len(patient)):\n",
    "        visit: Visit = patient[i]\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs = [x.code for x in visit.get_event_list(table=\"PRESCRIPTIONS\")]\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_readmission_label,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return samples\n",
    "    \n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data)) - 1\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size \n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs, drugs_pad = pad_field(\"drugs\", visits, '0')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    assert(drugs_pad == drugs_text_pad)\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs\": drugs,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_pad\": drugs_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_readmission_label,\n",
    "    }\n",
    "    # sample = {\n",
    "    #     \"patient_id\": patient.patient_id,\n",
    "    #     # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "    #     \"visit_id\": visits[0][\"visit_id\"],\n",
    "    #     \"num_visits\": num_visits,\n",
    "    #     # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "    #     \"conditions\": [[x[\"conditions\"] for x in visits]],\n",
    "    #     \"procedures\": [[x[\"procedures\"] for x in visits]],\n",
    "    #     \"conditions_text\": [[x[\"conditions_text\"] for x in visits]],\n",
    "    #     \"procedures_text\": [[x[\"procedures_text\"] for x in visits]],\n",
    "    #     \"drugs\": [[[x[\"drugs\"] for x in visits]]],\n",
    "    #     \"drugs_text\": [[x[\"drugs_text\"] for x in visits]],\n",
    "    #     # \"labevents\": labevents,\n",
    "    #     # \"labevents_text\": labevents_text\n",
    "    #     \"label\": global_readmission_label,\n",
    "    # }\n",
    "    for code in sample['conditions']:\n",
    "        READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_[code] = READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.get(code, 0) + 1\n",
    "\n",
    "    samples.extend(\n",
    "        list(deepcopy(sample) for s in range(SAMPLE_MULTIPLIER_))\n",
    "    )\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_task_per_patient(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "\n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "        \n",
    "    # loop over all visits but the last one\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit: Visit.\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs = [x.code for x in visit.get_event_list(table=\"PRESCRIPTIONS\")]\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples into a pyHealth Visit.\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_mortality_label,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return [] \n",
    "   \n",
    "    # pyHealth requires that all list fields in sample are equal size.\n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data))\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size\n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs, drugs_pad = pad_field(\"drugs\", visits, '0')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    assert(drugs_pad == drugs_text_pad)\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs\": drugs,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_pad\": drugs_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_mortality_label,\n",
    "    }\n",
    "    # sample = {\n",
    "    #     \"patient_id\": patient.patient_id,\n",
    "    #     # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "    #     \"visit_id\": visits[0][\"visit_id\"],\n",
    "    #     \"num_visits\": num_visits,\n",
    "    #     # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "    #     \"conditions\": [[x[\"conditions\"] for x in visits]],\n",
    "    #     \"procedures\": [[x[\"procedures\"] for x in visits]],\n",
    "    #     \"conditions_text\": [[x[\"conditions_text\"] for x in visits]],\n",
    "    #     \"procedures_text\": [[x[\"procedures_text\"] for x in visits]],\n",
    "    #     \"drugs\": [[[x[\"drugs\"] for x in visits]]],\n",
    "    #     \"drugs_text\": [[x[\"drugs_text\"] for x in visits]],\n",
    "    #     # \"labevents\": labevents,\n",
    "    #     # \"labevents_text\": labevents_text\n",
    "    #     \"label\": global_mortality_label,\n",
    "    # }\n",
    "   \n",
    "   \n",
    "    # For every condition in the sample (all visits). Record frequency.\n",
    "    # Will be used to build code->index LUT.\n",
    "    for code in sample['conditions']:\n",
    "        CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "\n",
    "    if SAMPLE_MULTIPLIER_:\n",
    "        samples.extend(\n",
    "            list(deepcopy(sample) for s in range(SAMPLE_MULTIPLIER_))\n",
    "        )\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371b36c-571d-482e-86b2-bbfb92522a24",
   "metadata": {},
   "source": [
    "#### Test Load Readmission Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "id": "96d047ff-6412-462c-9294-a5ba2d3bdba2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for readmission_pred_task_per_patient: 100%|███████████████████████████████████████| 1000/1000 [00:00<00:00, 4491.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3DatasetWrapper\n",
      "\t- Task: readmission_pred_task_per_patient\n",
      "\t- Number of samples: 893\n",
      "\t- Number of patients: 893\n",
      "\t- Number of visits: 893\n",
      "\t- Number of visits per patient: 1.0000\n",
      "\t- num_visits:\n",
      "\t\t- Number of num_visits per sample: 1.0000\n",
      "\t\t- Number of unique num_visits: 9\n",
      "\t\t- Distribution of num_visits (Top-10): [(1, 750), (2, 99), (3, 23), (4, 13), (5, 4), (28, 1), (9, 1), (6, 1), (8, 1)]\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 40.0000\n",
      "\t\t- Number of unique conditions: 1523\n",
      "\t\t- Distribution of conditions (Top-10): [('0', 26531), ('4019', 298), ('41401', 263), ('4280', 248), ('42731', 207), ('5849', 146), ('51881', 133), ('25000', 106), ('2720', 105), ('5990', 103)]\n",
      "\t- conditions_text:\n",
      "\t\t- Number of conditions_text per sample: 40.0000\n",
      "\t\t- Number of unique conditions_text: 1\n",
      "\t\t- Distribution of conditions_text (Top-10): [('', 35720)]\n",
      "\t- procedures:\n",
      "\t\t- Number of procedures per sample: 40.0000\n",
      "\t\t- Number of unique procedures: 519\n",
      "\t\t- Distribution of procedures (Top-10): [('0', 31288), ('9604', 214), ('3893', 207), ('966', 167), ('9904', 159), ('9671', 144), ('3961', 127), ('9672', 125), ('8856', 123), ('3615', 99)]\n",
      "\t- procedures_text:\n",
      "\t\t- Number of procedures_text per sample: 40.0000\n",
      "\t\t- Number of unique procedures_text: 1\n",
      "\t\t- Distribution of procedures_text (Top-10): [('', 35720)]\n",
      "\t- drugs:\n",
      "\t\t- Number of drugs per sample: 31.0907\n",
      "\t\t- Number of unique drugs: 150\n",
      "\t\t- Distribution of drugs (Top-10): [('0', 5516), ('B05X', 2660), ('A06A', 1513), ('V06D', 1232), ('B01A', 1182), ('N02B', 1110), ('N02A', 1063), ('C07A', 942), ('A02B', 895), ('C03C', 567)]\n",
      "\t- drugs_text:\n",
      "\t\t- Number of drugs_text per sample: 31.0907\n",
      "\t\t- Number of unique drugs_text: 1955\n",
      "\t\t- Distribution of drugs_text (Top-10): [('', 5516), ('Magnesium Sulfate MAIN 1g/2mL Vial IV 0.0', 547), ('Furosemide MAIN 40mg/4mL Vial IV 0.0', 347), ('D5W BASE 250mL Bag IV DRIP 0.0', 300), ('Calcium Gluconate MAIN 1g/10mL Vial IV 0.0', 254), ('LR BASE 1000ml Bag IV 0.0', 241), ('Heparin Sodium MAIN 25,000 unit Premix Bag IV 0.0', 231), ('NS BASE 500mL Bag IV 0.0', 223), ('Magnesium Sulfate MAIN 1gm / 2 mL Vial IV 0.0', 219), ('NS BASE 1000mL Bag IV 0.0', 216)]\n",
      "\t- conditions_pad:\n",
      "\t\t- Number of conditions_pad per sample: 1.0000\n",
      "\t\t- Number of unique conditions_pad: 40\n",
      "\t\t- Distribution of conditions_pad (Top-10): [(8, 180), (6, 81), (7, 72), (5, 71), (4, 65), (3, 63), (2, 37), (39, 35), (9, 34), (10, 31)]\n",
      "\t- procedures_pad:\n",
      "\t\t- Number of procedures_pad per sample: 1.0000\n",
      "\t\t- Number of unique procedures_pad: 31\n",
      "\t\t- Distribution of procedures_pad (Top-10): [(1, 145), (0, 114), (2, 100), (4, 81), (5, 79), (3, 77), (6, 69), (7, 45), (8, 35), (9, 29)]\n",
      "\t- conditions_text_pad:\n",
      "\t\t- Number of conditions_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique conditions_text_pad: 40\n",
      "\t\t- Distribution of conditions_text_pad (Top-10): [(8, 180), (6, 81), (7, 72), (5, 71), (4, 65), (3, 63), (2, 37), (39, 35), (9, 34), (10, 31)]\n",
      "\t- procedures_text_pad:\n",
      "\t\t- Number of procedures_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique procedures_text_pad: 31\n",
      "\t\t- Distribution of procedures_text_pad (Top-10): [(1, 145), (0, 114), (2, 100), (4, 81), (5, 79), (3, 77), (6, 69), (7, 45), (8, 35), (9, 29)]\n",
      "\t- drugs_pad:\n",
      "\t\t- Number of drugs_pad per sample: 1.0000\n",
      "\t\t- Number of unique drugs_pad: 41\n",
      "\t\t- Distribution of drugs_pad (Top-10): [(39, 475), (-1, 204), (1, 26), (34, 9), (26, 8), (11, 8), (36, 8), (12, 7), (33, 7), (19, 7)]\n",
      "\t- drugs_text_pad:\n",
      "\t\t- Number of drugs_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique drugs_text_pad: 41\n",
      "\t\t- Distribution of drugs_text_pad (Top-10): [(39, 475), (-1, 204), (1, 26), (34, 9), (26, 8), (11, 8), (36, 8), (12, 7), (33, 7), (19, 7)]\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(0, 799), (1, 94)]\n",
      "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 1523\n",
      "{'0': 0, '00845': 1, '0088': 2, '0090': 3, '0093': 4, '0360': 5, '0362': 6, '0380': 7, '03811': 8, '03812': 9, '03819': 10, '0382': 11, '0383': 12, '03840': 13, '03842': 14, '03843': 15, '03849': 16, '0388': 17, '0389': 18, '04100': 19, '04104': 20, '04109': 21, '04110': 22, '04111': 23, '04119': 24, '0413': 25, '0414': 26, '0416': 27, '0417': 28, '04184': 29, '04186': 30, '04189': 31, '042': 32, '0479': 33, '0490': 34, '05379': 35, '0543': 36, '05471': 37, '05479': 38, '0701': 39, '07020': 40, '07030': 41, '07032': 42, '07044': 43, '07051': 44, '07054': 45, '07070': 46, '0785': 47, '07989': 48, '07999': 49, '0839': 50, '08881': 51, '1120': 52, '1121': 53, '1122': 54, '1123': 55, '1124': 56, '1125': 57, '11284': 58, '11289': 59, '1173': 60, '1175': 61, '1177': 62, '1179': 63, '1270': 64, '1309': 65, '135': 66, '1363': 67, '1369': 68, '138': 69, '1505': 70, '1508': 71, '1509': 72, '1510': 73, '1520': 74, '1522': 75, '1534': 76, '1536': 77, '1550': 78, '1551': 79, '1552': 80, '1561': 81, '1562': 82, '1570': 83, '1573': 84, '1603': 85, '1622': 86, '1623': 87, '1624': 88, '1625': 89, '1628': 90, '1629': 91, '1642': 92, '1700': 93, '1713': 94, '185': 95, '1890': 96, '1892': 97, '1915': 98, '1916': 99, '1917': 100, '1919': 101, '1940': 102, '1960': 103, '1961': 104, '1962': 105, '1968': 106, '1969': 107, '1970': 108, '1971': 109, '1972': 110, '1973': 111, '1975': 112, '1976': 113, '1977': 114, '1978': 115, '1982': 116, '1983': 117, '1984': 118, '1985': 119, '1987': 120, '19889': 121, '1991': 122, '20000': 123, '20008': 124, '20028': 125, '20190': 126, '20280': 127, '20288': 128, '20300': 129, '20301': 130, '20400': 131, '20410': 132, '20411': 133, '20510': 134, '2113': 135, '2114': 136, '2127': 137, '2144': 138, '2148': 139, '2156': 140, '2161': 141, '2167': 142, '2181': 143, '2252': 144, '226': 145, '2270': 146, '22801': 147, '2356': 148, '2387': 149, '23875': 150, '2409': 151, '2411': 152, '24201': 153, '24290': 154, '2449': 155, '2469': 156, '25000': 157, '25001': 158, '25002': 159, '25010': 160, '25011': 161, '25013': 162, '25040': 163, '25041': 164, '25043': 165, '25050': 166, '25051': 167, '25053': 168, '25060': 169, '25061': 170, '25062': 171, '25063': 172, '25071': 173, '25073': 174, '25080': 175, '25082': 176, '25093': 177, '2518': 178, '2520': 179, '25200': 180, '2535': 181, '2536': 182, '2539': 183, '2550': 184, '2553': 185, '2554': 186, '25541': 187, '2559': 188, '2572': 189, '261': 190, '2630': 191, '2639': 192, '2662': 193, '2713': 194, '2720': 195, '2721': 196, '2724': 197, '2730': 198, '2732': 199, '2738': 200, '2740': 201, '27401': 202, '2749': 203, '2750': 204, '2752': 205, '2753': 206, '27541': 207, '27542': 208, '27549': 209, '2760': 210, '2761': 211, '2762': 212, '2763': 213, '2764': 214, '2765': 215, '27650': 216, '27651': 217, '27652': 218, '2766': 219, '2767': 220, '2768': 221, '2769': 222, '2773': 223, '27730': 224, '27800': 225, '27801': 226, '27949': 227, '2800': 228, '2809': 229, '2811': 230, '2819': 231, '28249': 232, '28319': 233, '2841': 234, '2848': 235, '2849': 236, '2851': 237, '28521': 238, '28522': 239, '28529': 240, '2858': 241, '2859': 242, '2861': 243, '2862': 244, '2866': 245, '2867': 246, '2869': 247, '2873': 248, '28731': 249, '2874': 250, '2875': 251, '2880': 252, '28800': 253, '28801': 254, '28803': 255, '28860': 256, '2888': 257, '2890': 258, '28959': 259, '28981': 260, '28982': 261, '28983': 262, '28984': 263, '2903': 264, '29040': 265, '2910': 266, '2911': 267, '29181': 268, '2920': 269, '29281': 270, '2930': 271, '29383': 272, '2939': 273, '29410': 274, '2948': 275, '29570': 276, '29590': 277, '29620': 278, '29633': 279, '29634': 280, '2967': 281, '29680': 282, '29689': 283, '2971': 284, '30000': 285, '30011': 286, '30029': 287, '3004': 288, '30303': 289, '30390': 290, '30391': 291, '30393': 292, '30400': 293, '30401': 294, '30421': 295, '30431': 296, '30471': 297, '30490': 298, '30500': 299, '30501': 300, '30503': 301, '3051': 302, '30530': 303, '30540': 304, '30550': 305, '30551': 306, '30560': 307, '30561': 308, '30562': 309, '30570': 310, '30591': 311, '311': 312, '31400': 313, '31401': 314, '317': 315, '3181': 316, '3202': 317, '3210': 318, '3236': 319, '3238': 320, '3240': 321, '3241': 322, '32723': 323, '3310': 324, '3313': 325, '3314': 326, '33182': 327, '3320': 328, '3332': 329, '33394': 330, '33399': 331, '33520': 332, '3363': 333, '3379': 334, '33811': 335, '33829': 336, '3383': 337, '340': 338, '34280': 339, '34290': 340, '3439': 341, '34400': 342, '3441': 343, '34461': 344, '34489': 345, '3453': 346, '34540': 347, '34550': 348, '34590': 349, '34690': 350, '3481': 351, '34830': 352, '34831': 353, '34839': 354, '3484': 355, '3485': 356, '3488': 357, '34982': 358, '3501': 359, '3510': 360, '3542': 361, '3543': 362, '3558': 363, '3559': 364, '3568': 365, '3569': 366, '3570': 367, '3572': 368, '3574': 369, '35781': 370, '35782': 371, '3581': 372, '35981': 373, '36201': 374, '36221': 375, '36250': 376, '36281': 377, '36284': 378, '3643': 379, '3649': 380, '3659': 381, '3682': 382, '36840': 383, '36900': 384, '3693': 385, '3694': 386, '37272': 387, '37741': 388, '37852': 389, '37854': 390, '37992': 391, '3804': 392, '38300': 393, '3899': 394, '3941': 395, '3949': 396, '3960': 397, '3962': 398, '3963': 399, '3968': 400, '3970': 401, '39890': 402, '39891': 403, '4019': 404, '40301': 405, '40310': 406, '40311': 407, '40390': 408, '40391': 409, '40501': 410, '41001': 411, '41011': 412, '41021': 413, '41031': 414, '41041': 415, '41042': 416, '41061': 417, '41071': 418, '41072': 419, '41081': 420, '41082': 421, '41091': 422, '4110': 423, '4111': 424, '41189': 425, '412': 426, '4139': 427, '41400': 428, '41401': 429, '41402': 430, '41404': 431, '41411': 432, '4142': 433, '4148': 434, '41511': 435, '41519': 436, '4160': 437, '4168': 438, '4169': 439, '4178': 440, '42090': 441, '42091': 442, '4210': 443, '4230': 444, '4232': 445, '4238': 446, '4239': 447, '4240': 448, '4241': 449, '4242': 450, '42490': 451, '4251': 452, '4254': 453, '4255': 454, '4260': 455, '42611': 456, '42612': 457, '42613': 458, '4262': 459, '4263': 460, '4264': 461, '4267': 462, '42682': 463, '42689': 464, '4270': 465, '4271': 466, '42731': 467, '42732': 468, '42741': 469, '4275': 470, '42781': 471, '42789': 472, '4280': 473, '4281': 474, '42820': 475, '42821': 476, '42822': 477, '42823': 478, '42830': 479, '42831': 480, '42832': 481, '42833': 482, '42840': 483, '42841': 484, '42843': 485, '4290': 486, '42989': 487, '4299': 488, '430': 489, '431': 490, '4321': 491, '4329': 492, '43300': 493, '43310': 494, '43330': 495, '43411': 496, '43491': 497, '4352': 498, '4359': 499, '4370': 500, '4371': 501, '4372': 502, '4373': 503, '4374': 504, '4375': 505, '4380': 506, '43811': 507, '43820': 508, '4387': 509, '43882': 510, '43883': 511, '43889': 512, '4389': 513, '4400': 514, '4401': 515, '44020': 516, '44021': 517, '44022': 518, '44023': 519, '44024': 520, '44029': 521, '44030': 522, '44101': 523, '44102': 524, '44103': 525, '4411': 526, '4412': 527, '4413': 528, '4414': 529, '4417': 530, '4422': 531, '4423': 532, '4430': 533, '44389': 534, '4439': 535, '4440': 536, '4441': 537, '44421': 538, '44422': 539, '44481': 540, '44489': 541, '4460': 542, '4465': 543, '4466': 544, '4470': 545, '4471': 546, '4472': 547, '4476': 548, '4478': 549, '45182': 550, '45184': 551, '4532': 552, '45341': 553, '45342': 554, '45352': 555, '4538': 556, '4550': 557, '4553': 558, '4558': 559, '4560': 560, '45620': 561, '45621': 562, '4568': 563, '4580': 564, '4582': 565, '45821': 566, '45829': 567, '4588': 568, '4589': 569, '4590': 570, '4592': 571, '45981': 572, '4613': 573, '462': 574, '46410': 575, '46451': 576, '4659': 577, '4660': 578, '4738': 579, '4739': 580, '4781': 581, '47829': 582, '47830': 583, '47831': 584, '4786': 585, '4809': 586, '481': 587, '4820': 588, '4821': 589, '4822': 590, '48230': 591, '48240': 592, '48241': 593, '48249': 594, '48282': 595, '48283': 596, '48289': 597, '4841': 598, '4846': 599, '4847': 600, '485': 601, '486': 602, '4870': 603, '4871': 604, '490': 605, '49120': 606, '49121': 607, '4920': 608, '4928': 609, '49320': 610, '49322': 611, '49390': 612, '49392': 613, '4940': 614, '496': 615, '500': 616, '501': 617, '5070': 618, '5081': 619, '5100': 620, '5109': 621, '5110': 622, '5111': 623, '5118': 624, '5119': 625, '5121': 626, '5128': 627, '5130': 628, '514': 629, '515': 630, '5163': 631, '5178': 632, '5180': 633, '5184': 634, '5185': 635, '5187': 636, '51881': 637, '51882': 638, '51883': 639, '51884': 640, '51889': 641, '51901': 642, '51902': 643, '5191': 644, '51919': 645, '5192': 646, '5193': 647, '5194': 648, '52100': 649, '5224': 650, '5289': 651, '5300': 652, '53010': 653, '53012': 654, '53019': 655, '5302': 656, '53020': 657, '53021': 658, '5303': 659, '5304': 660, '5307': 661, '53081': 662, '53082': 663, '53084': 664, '53085': 665, '53089': 666, '53100': 667, '53140': 668, '53190': 669, '53200': 670, '53240': 671, '53260': 672, '53291': 673, '53340': 674, '53390': 675, '53501': 676, '53510': 677, '53550': 678, '53551': 679, '53560': 680, '53561': 681, '5363': 682, '53641': 683, '53642': 684, '53783': 685, '53784': 686, '53789': 687, '55012': 688, '55090': 689, '55092': 690, '55221': 691, '5528': 692, '5531': 693, '55321': 694, '55329': 695, '5533': 696, '5552': 697, '5559': 698, '5569': 699, '5570': 700, '5571': 701, '5583': 702, '5600': 703, '5601': 704, '5602': 705, '56081': 706, '56089': 707, '5609': 708, '56210': 709, '56211': 710, '56212': 711, '56213': 712, '56400': 713, '56409': 714, '5641': 715, '566': 716, '5672': 717, '56721': 718, '56722': 719, '56723': 720, '56729': 721, '56731': 722, '56738': 723, '5678': 724, '56782': 725, '56789': 726, '5680': 727, '56881': 728, '56889': 729, '5693': 730, '56962': 731, '56981': 732, '56983': 733, '56985': 734, '570': 735, '5711': 736, '5712': 737, '57149': 738, '5715': 739, '5716': 740, '5718': 741, '5720': 742, '5722': 743, '5723': 744, '5724': 745, '5728': 746, '5730': 747, '5733': 748, '5738': 749, '57400': 750, '57401': 751, '57410': 752, '57420': 753, '57421': 754, '57450': 755, '57451': 756, '57460': 757, '57471': 758, '57490': 759, '57491': 760, '5750': 761, '57511': 762, '57512': 763, '5758': 764, '5759': 765, '5761': 766, '5762': 767, '5768': 768, '5770': 769, '5771': 770, '5772': 771, '5778': 772, '5779': 773, '5780': 774, '5781': 775, '5789': 776, '5790': 777, '5793': 778, '5798': 779, '5799': 780, '58089': 781, '58281': 782, '58381': 783, '58389': 784, '5845': 785, '5849': 786, '585': 787, '5852': 788, '5853': 789, '5854': 790, '5856': 791, '5859': 792, '5881': 793, '58881': 794, '58889': 795, '591': 796, '5920': 797, '5921': 798, '5932': 799, '59381': 800, '5939': 801, '59582': 802, '5960': 803, '5988': 804, '5990': 805, '5997': 806, '59970': 807, '6000': 808, '60000': 809, '60001': 810, '60091': 811, '6039': 812, '60499': 813, '6084': 814, '60883': 815, '6110': 816, '6144': 817, '6146': 818, '6191': 819, '6238': 820, '6822': 821, '6823': 822, '6824': 823, '6826': 824, '6827': 825, '6851': 826, '6910': 827, '6930': 828, '6961': 829, '6983': 830, '7019': 831, '7070': 832, '70701': 833, '70703': 834, '70705': 835, '70706': 836, '70707': 837, '70709': 838, '70713': 839, '70714': 840, '70715': 841, '70719': 842, '70721': 843, '70724': 844, '7098': 845, '7099': 846, '7100': 847, '7109': 848, '71101': 849, '71105': 850, '71107': 851, '71180': 852, '71230': 853, '7140': 854, '71535': 855, '71590': 856, '71596': 857, '71598': 858, '71847': 859, '71906': 860, '71941': 861, '71945': 862, '71946': 863, '71965': 864, '7197': 865, '7202': 866, '7209': 867, '7210': 868, '7211': 869, '7213': 870, '72141': 871, '7220': 872, '72210': 873, '72272': 874, '7230': 875, '7231': 876, '7234': 877, '7236': 878, '72400': 879, '72401': 880, '72402': 881, '7242': 882, '7245': 883, '725': 884, '72888': 885, '72889': 886, '7291': 887, '7299': 888, '72992': 889, '73004': 890, '73005': 891, '73007': 892, '73008': 893, '73009': 894, '73017': 895, '73018': 896, '73022': 897, '73025': 898, '73027': 899, '73028': 900, '7310': 901, '7318': 902, '73300': 903, '73311': 904, '73313': 905, '73315': 906, '73319': 907, '73342': 908, '73381': 909, '73382': 910, '73679': 911, '73730': 912, '73734': 913, '73743': 914, '7384': 915, '74101': 916, '74190': 917, '7421': 918, '7423': 919, '7450': 920, '7452': 921, '7454': 922, '7455': 923, '74561': 924, '74602': 925, '7464': 926, '7466': 927, '74689': 928, '7470': 929, '7473': 930, '74742': 931, '74783': 932, '7509': 933, '7515': 934, '75261': 935, '75263': 936, '75315': 937, '75322': 938, '75329': 939, '7533': 940, '75453': 941, '75470': 942, '75526': 943, '75529': 944, '75567': 945, '75610': 946, '75651': 947, '7580': 948, '75889': 949, '75981': 950, '7599': 951, '76072': 952, '76075': 953, '76077': 954, '7608': 955, '7615': 956, '7617': 957, '7630': 958, '76383': 959, '76402': 960, '76407': 961, '76408': 962, '76409': 963, '76492': 964, '76496': 965, '76502': 966, '76503': 967, '76513': 968, '76514': 969, '76515': 970, '76516': 971, '76517': 972, '76518': 973, '76519': 974, '76522': 975, '76523': 976, '76524': 977, '76525': 978, '76526': 979, '76527': 980, '76528': 981, '76529': 982, '7660': 983, '7661': 984, '7662': 985, '76621': 986, '76719': 987, '7678': 988, '7689': 989, '769': 990, '7701': 991, '7702': 992, '7705': 993, '7706': 994, '7707': 995, '7708': 996, '77081': 997, '77082': 998, '77083': 999, '77084': 1000, '77089': 1001, '7716': 1002, '7717': 1003, '77181': 1004, '77183': 1005, '77189': 1006, '77211': 1007, '77214': 1008, '7724': 1009, '7726': 1010, '7731': 1011, '7742': 1012, '7746': 1013, '7750': 1014, '7755': 1015, '7756': 1016, '7757': 1017, '77581': 1018, '7761': 1019, '7766': 1020, '7767': 1021, '7775': 1022, '7778': 1023, '7783': 1024, '7784': 1025, '7788': 1026, '7790': 1027, '7793': 1028, '77981': 1029, '77989': 1030, '78001': 1031, '78003': 1032, '78009': 1033, '7802': 1034, '78039': 1035, '78051': 1036, '78052': 1037, '78057': 1038, '7806': 1039, '78060': 1040, '78079': 1041, '7808': 1042, '7809': 1043, '78097': 1044, '7812': 1045, '7813': 1046, '7820': 1047, '7821': 1048, '7830': 1049, '78321': 1050, '7837': 1051, '7840': 1052, '7841': 1053, '7843': 1054, '78451': 1055, '7847': 1056, '7850': 1057, '7852': 1058, '7854': 1059, '78551': 1060, '78552': 1061, '78559': 1062, '7856': 1063, '78609': 1064, '7861': 1065, '7863': 1066, '78650': 1067, '7866': 1068, '78701': 1069, '78703': 1070, '7872': 1071, '7873': 1072, '78791': 1073, '78820': 1074, '78829': 1075, '78843': 1076, '7885': 1077, '78900': 1078, '78904': 1079, '78907': 1080, '7892': 1081, '7895': 1082, '78959': 1083, '79001': 1084, '79029': 1085, '7904': 1086, '7905': 1087, '7907': 1088, '79092': 1089, '79099': 1090, '7921': 1091, '7935': 1092, '7936': 1093, '79431': 1094, '79439': 1095, '7960': 1096, '7990': 1097, '79902': 1098, '7993': 1099, '7994': 1100, '80009': 1101, '80012': 1102, '80070': 1103, '80101': 1104, '80111': 1105, '80114': 1106, '80120': 1107, '80122': 1108, '80126': 1109, '8020': 1110, '8021': 1111, '80229': 1112, '8024': 1113, '8026': 1114, '8028': 1115, '80320': 1116, '80325': 1117, '80416': 1118, '80502': 1119, '80504': 1120, '80505': 1121, '80507': 1122, '8052': 1123, '8054': 1124, '8056': 1125, '80604': 1126, '80605': 1127, '80607': 1128, '80621': 1129, '80625': 1130, '80700': 1131, '80701': 1132, '80703': 1133, '80705': 1134, '80706': 1135, '80708': 1136, '80709': 1137, '8072': 1138, '8074': 1139, '8080': 1140, '8082': 1141, '8083': 1142, '80841': 1143, '80842': 1144, '80849': 1145, '81000': 1146, '81100': 1147, '81200': 1148, '81201': 1149, '81301': 1150, '81305': 1151, '81341': 1152, '81500': 1153, '82009': 1154, '82021': 1155, '82022': 1156, '82032': 1157, '8208': 1158, '82101': 1159, '82111': 1160, '82121': 1161, '82123': 1162, '82133': 1163, '8220': 1164, '82300': 1165, '82302': 1166, '82320': 1167, '82332': 1168, '82380': 1169, '82382': 1170, '8248': 1171, '8249': 1172, '83100': 1173, '83501': 1174, '83900': 1175, '83901': 1176, '83961': 1177, '8470': 1178, '8501': 1179, '8505': 1180, '85142': 1181, '85180': 1182, '85181': 1183, '85182': 1184, '85200': 1185, '85202': 1186, '85220': 1187, '85221': 1188, '85222': 1189, '85300': 1190, '85306': 1191, '85405': 1192, '8600': 1193, '8602': 1194, '8604': 1195, '86113': 1196, '86121': 1197, '86344': 1198, '86402': 1199, '86405': 1200, '86500': 1201, '86503': 1202, '86509': 1203, '86602': 1204, '86612': 1205, '8670': 1206, '86803': 1207, '86804': 1208, '86819': 1209, '8700': 1210, '8711': 1211, '8730': 1212, '87342': 1213, '87343': 1214, '87344': 1215, '87352': 1216, '8738': 1217, '87402': 1218, '8748': 1219, '8760': 1220, '8794': 1221, '88002': 1222, '88122': 1223, '8840': 1224, '8860': 1225, '8910': 1226, '9020': 1227, '90229': 1228, '9032': 1229, '9033': 1230, '9041': 1231, '90441': 1232, '9051': 1233, '9072': 1234, '9092': 1235, '9110': 1236, '9130': 1237, '9181': 1238, '920': 1239, '9221': 1240, '92300': 1241, '92400': 1242, '92421': 1243, '92811': 1244, '9331': 1245, '9351': 1246, '936': 1247, '94224': 1248, '9500': 1249, '9551': 1250, '9552': 1251, '9555': 1252, '9581': 1253, '9584': 1254, '9587': 1255, '95892': 1256, '95901': 1257, '9604': 1258, '9623': 1259, '96500': 1260, '96501': 1261, '96502': 1262, '96509': 1263, '9654': 1264, '9661': 1265, '9678': 1266, '9690': 1267, '9693': 1268, '9694': 1269, '9695': 1270, '9696': 1271, '9697': 1272, '9698': 1273, '9701': 1274, '9708': 1275, '97081': 1276, '9726': 1277, '9778': 1278, '9800': 1279, '9809': 1280, '9828': 1281, '990': 1282, '99591': 1283, '99592': 1284, '99594': 1285, '99601': 1286, '99602': 1287, '99604': 1288, '9961': 1289, '9962': 1290, '9964': 1291, '99644': 1292, '99659': 1293, '99661': 1294, '99662': 1295, '99664': 1296, '99667': 1297, '99669': 1298, '99671': 1299, '99672': 1300, '99673': 1301, '99674': 1302, '99679': 1303, '99681': 1304, '99682': 1305, '99685': 1306, '99702': 1307, '99709': 1308, '9971': 1309, '9972': 1310, '9973': 1311, '99731': 1312, '99739': 1313, '9974': 1314, '9975': 1315, '99762': 1316, '99769': 1317, '99771': 1318, '9980': 1319, '99811': 1320, '99812': 1321, '99813': 1322, '9982': 1323, '9983': 1324, '99831': 1325, '99832': 1326, '9984': 1327, '99859': 1328, '9986': 1329, '99881': 1330, '99883': 1331, '99889': 1332, '9991': 1333, '9992': 1334, '99931': 1335, '9998': 1336, '9999': 1337, 'E8120': 1338, 'E8122': 1339, 'E8147': 1340, 'E8150': 1341, 'E8160': 1342, 'E8190': 1343, 'E8192': 1344, 'E8217': 1345, 'E8230': 1346, 'E8348': 1347, 'E8490': 1348, 'E8495': 1349, 'E8497': 1350, 'E8498': 1351, 'E8499': 1352, 'E8500': 1353, 'E8501': 1354, 'E8502': 1355, 'E8532': 1356, 'E8538': 1357, 'E8541': 1358, 'E8543': 1359, 'E8550': 1360, 'E8580': 1361, 'E8600': 1362, 'E8668': 1363, 'E8708': 1364, 'E8780': 1365, 'E8781': 1366, 'E8782': 1367, 'E8786': 1368, 'E8788': 1369, 'E8789': 1370, 'E8790': 1371, 'E8791': 1372, 'E8792': 1373, 'E8796': 1374, 'E8798': 1375, 'E8799': 1376, 'E8801': 1377, 'E8809': 1378, 'E8810': 1379, 'E882': 1380, 'E8842': 1381, 'E8849': 1382, 'E8859': 1383, 'E8889': 1384, 'E9102': 1385, 'E915': 1386, 'E918': 1387, 'E9248': 1388, 'E9289': 1389, 'E9290': 1390, 'E9305': 1391, 'E9310': 1392, 'E9323': 1393, 'E9331': 1394, 'E9342': 1395, 'E9344': 1396, 'E9347': 1397, 'E9352': 1398, 'E9359': 1399, 'E9379': 1400, 'E9383': 1401, 'E9398': 1402, 'E9421': 1403, 'E9478': 1404, 'E9500': 1405, 'E9502': 1406, 'E9503': 1407, 'E9509': 1408, 'E956': 1409, 'E9600': 1410, 'E966': 1411, 'E9689': 1412, 'E9803': 1413, 'E9809': 1414, 'V0254': 1415, 'V0259': 1416, 'V0381': 1417, 'V053': 1418, 'V058': 1419, 'V063': 1420, 'V08': 1421, 'V090': 1422, 'V0980': 1423, 'V1000': 1424, 'V1005': 1425, 'V1006': 1426, 'V1007': 1427, 'V1009': 1428, 'V1011': 1429, 'V103': 1430, 'V1041': 1431, 'V1042': 1432, 'V1046': 1433, 'V1047': 1434, 'V1051': 1435, 'V1052': 1436, 'V1053': 1437, 'V1079': 1438, 'V1082': 1439, 'V1083': 1440, 'V113': 1441, 'V1201': 1442, 'V1209': 1443, 'V122': 1444, 'V1251': 1445, 'V1259': 1446, 'V1271': 1447, 'V1301': 1448, 'V141': 1449, 'V1507': 1450, 'V153': 1451, 'V155': 1452, 'V1581': 1453, 'V1582': 1454, 'V160': 1455, 'V173': 1456, 'V290': 1457, 'V293': 1458, 'V298': 1459, 'V3000': 1460, 'V3001': 1461, 'V3100': 1462, 'V3101': 1463, 'V3401': 1464, 'V420': 1465, 'V422': 1466, 'V427': 1467, 'V4281': 1468, 'V4282': 1469, 'V4283': 1470, 'V433': 1471, 'V4364': 1472, 'V4365': 1473, 'V440': 1474, 'V441': 1475, 'V442': 1476, 'V4501': 1477, 'V4502': 1478, 'V4511': 1479, 'V4512': 1480, 'V452': 1481, 'V453': 1482, 'V454': 1483, 'V4573': 1484, 'V4579': 1485, 'V4581': 1486, 'V4582': 1487, 'V4586': 1488, 'V4589': 1489, 'V461': 1490, 'V4611': 1491, 'V4972': 1492, 'V4975': 1493, 'V4976': 1494, 'V4983': 1495, 'V4986': 1496, 'V502': 1497, 'V5331': 1498, 'V5332': 1499, 'V5391': 1500, 'V5417': 1501, 'V550': 1502, 'V552': 1503, 'V556': 1504, 'V5843': 1505, 'V5861': 1506, 'V5865': 1507, 'V5867': 1508, 'V5869': 1509, 'V600': 1510, 'V625': 1511, 'V632': 1512, 'V641': 1513, 'V644': 1514, 'V6441': 1515, 'V6442': 1516, 'V667': 1517, 'V707': 1518, 'V721': 1519, 'V8535': 1520, 'V854': 1521, 'V8741': 1522}\n"
     ]
    }
   ],
   "source": [
    "# set_task() returns a SampleEHRDataset object\n",
    "readm_dataset = mimic3base.set_task(readmission_pred_task_per_patient)\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "readm_dataset.stat()\n",
    "readm_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "print(f\"READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(READMISSION_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "      f\"{READMISSION_PER_PATIENT_ICD_9_CODE2IDX_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2738557-c9c4-4a33-9fd6-b2dd00bd1d22",
   "metadata": {},
   "source": [
    "#### Test Load Mortality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "id": "70586b66-a814-4898-892b-d1bdd339ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for mortality_pred_task_per_patient: 100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 4558.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3DatasetWrapper\n",
      "\t- Task: mortality_pred_task_per_patient\n",
      "\t- Number of samples: 893\n",
      "\t- Number of patients: 893\n",
      "\t- Number of visits: 893\n",
      "\t- Number of visits per patient: 1.0000\n",
      "\t- num_visits:\n",
      "\t\t- Number of num_visits per sample: 1.0000\n",
      "\t\t- Number of unique num_visits: 9\n",
      "\t\t- Distribution of num_visits (Top-10): [(1, 750), (2, 99), (3, 23), (4, 13), (5, 4), (28, 1), (9, 1), (6, 1), (8, 1)]\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 40.0000\n",
      "\t\t- Number of unique conditions: 1625\n",
      "\t\t- Distribution of conditions (Top-10): [('0', 25638), ('4019', 352), ('41401', 274), ('4280', 252), ('42731', 210), ('5849', 149), ('51881', 133), ('V053', 131), ('V290', 129), ('2720', 127)]\n",
      "\t- conditions_text:\n",
      "\t\t- Number of conditions_text per sample: 40.0000\n",
      "\t\t- Number of unique conditions_text: 1\n",
      "\t\t- Distribution of conditions_text (Top-10): [('', 35720)]\n",
      "\t- procedures:\n",
      "\t\t- Number of procedures per sample: 40.0000\n",
      "\t\t- Number of unique procedures: 547\n",
      "\t\t- Distribution of procedures (Top-10): [('0', 30395), ('3893', 282), ('9604', 240), ('966', 225), ('9904', 221), ('9671', 180), ('3961', 154), ('8856', 142), ('9955', 133), ('9672', 132)]\n",
      "\t- procedures_text:\n",
      "\t\t- Number of procedures_text per sample: 40.0000\n",
      "\t\t- Number of unique procedures_text: 1\n",
      "\t\t- Distribution of procedures_text (Top-10): [('', 35720)]\n",
      "\t- drugs:\n",
      "\t\t- Number of drugs per sample: 40.0000\n",
      "\t\t- Number of unique drugs: 151\n",
      "\t\t- Distribution of drugs (Top-10): [('0', 12783), ('B05X', 2721), ('A06A', 1550), ('V06D', 1271), ('B01A', 1203), ('N02B', 1136), ('N02A', 1101), ('C07A', 975), ('A02B', 921), ('C03C', 588)]\n",
      "\t- drugs_text:\n",
      "\t\t- Number of drugs_text per sample: 40.0000\n",
      "\t\t- Number of unique drugs_text: 1984\n",
      "\t\t- Distribution of drugs_text (Top-10): [('', 12783), ('Magnesium Sulfate MAIN 1g/2mL Vial IV 0.0', 559), ('Furosemide MAIN 40mg/4mL Vial IV 0.0', 359), ('D5W BASE 250mL Bag IV DRIP 0.0', 310), ('Calcium Gluconate MAIN 1g/10mL Vial IV 0.0', 258), ('LR BASE 1000ml Bag IV 0.0', 246), ('Heparin Sodium MAIN 25,000 unit Premix Bag IV 0.0', 233), ('NS BASE 500mL Bag IV 0.0', 226), ('NS BASE 1000mL Bag IV 0.0', 224), ('Magnesium Sulfate MAIN 1gm / 2 mL Vial IV 0.0', 222)]\n",
      "\t- conditions_pad:\n",
      "\t\t- Number of conditions_pad per sample: 1.0000\n",
      "\t\t- Number of unique conditions_pad: 40\n",
      "\t\t- Distribution of conditions_pad (Top-10): [(9, 180), (7, 81), (8, 72), (6, 71), (5, 65), (4, 63), (3, 37), (40, 35), (10, 34), (11, 31)]\n",
      "\t- procedures_pad:\n",
      "\t\t- Number of procedures_pad per sample: 1.0000\n",
      "\t\t- Number of unique procedures_pad: 31\n",
      "\t\t- Distribution of procedures_pad (Top-10): [(2, 145), (1, 114), (3, 100), (5, 81), (6, 79), (4, 77), (7, 69), (8, 45), (9, 35), (10, 29)]\n",
      "\t- conditions_text_pad:\n",
      "\t\t- Number of conditions_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique conditions_text_pad: 40\n",
      "\t\t- Distribution of conditions_text_pad (Top-10): [(9, 180), (7, 81), (8, 72), (6, 71), (5, 65), (4, 63), (3, 37), (40, 35), (10, 34), (11, 31)]\n",
      "\t- procedures_text_pad:\n",
      "\t\t- Number of procedures_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique procedures_text_pad: 31\n",
      "\t\t- Distribution of procedures_text_pad (Top-10): [(2, 145), (1, 114), (3, 100), (5, 81), (6, 79), (4, 77), (7, 69), (8, 45), (9, 35), (10, 29)]\n",
      "\t- drugs_pad:\n",
      "\t\t- Number of drugs_pad per sample: 1.0000\n",
      "\t\t- Number of unique drugs_pad: 41\n",
      "\t\t- Distribution of drugs_pad (Top-10): [(40, 475), (0, 204), (2, 26), (35, 9), (27, 8), (12, 8), (37, 8), (13, 7), (34, 7), (20, 7)]\n",
      "\t- drugs_text_pad:\n",
      "\t\t- Number of drugs_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique drugs_text_pad: 41\n",
      "\t\t- Distribution of drugs_text_pad (Top-10): [(40, 475), (0, 204), (2, 26), (35, 9), (27, 8), (12, 8), (37, 8), (13, 7), (34, 7), (20, 7)]\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(0, 771), (1, 122)]\n",
      "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 1625\n",
      "{'0': 0, '00845': 1, '0088': 2, '0090': 3, '0093': 4, '0360': 5, '0362': 6, '0380': 7, '03811': 8, '03812': 9, '03819': 10, '0382': 11, '0383': 12, '03840': 13, '03842': 14, '03843': 15, '03849': 16, '0388': 17, '0389': 18, '04100': 19, '04104': 20, '04109': 21, '04110': 22, '04111': 23, '04119': 24, '0413': 25, '0414': 26, '0416': 27, '0417': 28, '04184': 29, '04186': 30, '04189': 31, '042': 32, '0479': 33, '0490': 34, '05379': 35, '0542': 36, '0543': 37, '05471': 38, '05479': 39, '0549': 40, '0701': 41, '07020': 42, '07030': 43, '07032': 44, '07044': 45, '07051': 46, '07054': 47, '07070': 48, '0785': 49, '07989': 50, '07999': 51, '0839': 52, '0880': 53, '08881': 54, '1120': 55, '1121': 56, '1122': 57, '1123': 58, '1124': 59, '1125': 60, '11284': 61, '11289': 62, '1173': 63, '1175': 64, '1177': 65, '1179': 66, '1270': 67, '1309': 68, '135': 69, '1363': 70, '1369': 71, '138': 72, '1505': 73, '1508': 74, '1509': 75, '1510': 76, '1520': 77, '1522': 78, '1534': 79, '1536': 80, '1550': 81, '1551': 82, '1552': 83, '1561': 84, '1562': 85, '1570': 86, '1573': 87, '1603': 88, '1622': 89, '1623': 90, '1624': 91, '1625': 92, '1628': 93, '1629': 94, '1642': 95, '1700': 96, '1713': 97, '185': 98, '1890': 99, '1892': 100, '1913': 101, '1915': 102, '1916': 103, '1917': 104, '1919': 105, '1940': 106, '1960': 107, '1961': 108, '1962': 109, '1968': 110, '1969': 111, '1970': 112, '1971': 113, '1972': 114, '1973': 115, '1975': 116, '1976': 117, '1977': 118, '1978': 119, '1982': 120, '1983': 121, '1984': 122, '1985': 123, '1987': 124, '19889': 125, '1991': 126, '20000': 127, '20008': 128, '20028': 129, '20190': 130, '20280': 131, '20288': 132, '20300': 133, '20301': 134, '20400': 135, '20410': 136, '20411': 137, '20510': 138, '2113': 139, '2114': 140, '2127': 141, '2144': 142, '2148': 143, '2156': 144, '2161': 145, '2167': 146, '2181': 147, '220': 148, '2252': 149, '226': 150, '2270': 151, '22801': 152, '22802': 153, '22809': 154, '2356': 155, '2387': 156, '23875': 157, '2409': 158, '2411': 159, '24200': 160, '24201': 161, '24290': 162, '2449': 163, '2469': 164, '25000': 165, '25001': 166, '25002': 167, '25010': 168, '25011': 169, '25013': 170, '25040': 171, '25041': 172, '25043': 173, '25050': 174, '25051': 175, '25053': 176, '25060': 177, '25061': 178, '25062': 179, '25063': 180, '25070': 181, '25071': 182, '25073': 183, '25080': 184, '25082': 185, '25093': 186, '2511': 187, '2518': 188, '2520': 189, '25200': 190, '2535': 191, '2536': 192, '2539': 193, '2550': 194, '2553': 195, '2554': 196, '25541': 197, '2559': 198, '2572': 199, '261': 200, '2630': 201, '2639': 202, '2662': 203, '2707': 204, '2713': 205, '2720': 206, '2721': 207, '2724': 208, '2730': 209, '2732': 210, '2738': 211, '2740': 212, '27401': 213, '2749': 214, '2750': 215, '2752': 216, '2753': 217, '27541': 218, '27542': 219, '27549': 220, '2760': 221, '2761': 222, '2762': 223, '2763': 224, '2764': 225, '2765': 226, '27650': 227, '27651': 228, '27652': 229, '2766': 230, '2767': 231, '2768': 232, '2769': 233, '2773': 234, '27730': 235, '27800': 236, '27801': 237, '27949': 238, '2800': 239, '2809': 240, '2811': 241, '2819': 242, '28249': 243, '28319': 244, '2841': 245, '2848': 246, '2849': 247, '2851': 248, '28521': 249, '28522': 250, '28529': 251, '2858': 252, '2859': 253, '2861': 254, '2862': 255, '2866': 256, '2867': 257, '2869': 258, '2873': 259, '28731': 260, '2874': 261, '2875': 262, '2880': 263, '28800': 264, '28801': 265, '28803': 266, '28850': 267, '28860': 268, '2888': 269, '2890': 270, '28959': 271, '2897': 272, '28981': 273, '28982': 274, '28983': 275, '28984': 276, '2903': 277, '29040': 278, '2910': 279, '2911': 280, '29181': 281, '2920': 282, '29281': 283, '2930': 284, '29383': 285, '2939': 286, '29410': 287, '2948': 288, '29570': 289, '29590': 290, '29620': 291, '29633': 292, '29634': 293, '2967': 294, '29680': 295, '29689': 296, '2971': 297, '2989': 298, '30000': 299, '30011': 300, '30029': 301, '3003': 302, '3004': 303, '3010': 304, '30303': 305, '30390': 306, '30391': 307, '30393': 308, '30400': 309, '30401': 310, '30421': 311, '30431': 312, '30471': 313, '30490': 314, '30500': 315, '30501': 316, '30503': 317, '3051': 318, '30530': 319, '30540': 320, '30550': 321, '30551': 322, '30560': 323, '30561': 324, '30562': 325, '30570': 326, '30590': 327, '30591': 328, '3079': 329, '30928': 330, '3102': 331, '311': 332, '31400': 333, '31401': 334, '317': 335, '3181': 336, '319': 337, '3202': 338, '3210': 339, '3236': 340, '3238': 341, '3240': 342, '3241': 343, '32723': 344, '3310': 345, '3313': 346, '3314': 347, '33182': 348, '3320': 349, '3332': 350, '33394': 351, '33399': 352, '33520': 353, '3363': 354, '3379': 355, '33811': 356, '33829': 357, '3383': 358, '340': 359, '34280': 360, '34290': 361, '3439': 362, '34400': 363, '3441': 364, '34461': 365, '34489': 366, '3453': 367, '34540': 368, '34550': 369, '34590': 370, '34690': 371, '3481': 372, '3483': 373, '34830': 374, '34831': 375, '34839': 376, '3484': 377, '3485': 378, '3488': 379, '3489': 380, '34982': 381, '3501': 382, '3510': 383, '3542': 384, '3543': 385, '3558': 386, '3559': 387, '3568': 388, '3569': 389, '3570': 390, '3572': 391, '3574': 392, '3575': 393, '3578': 394, '35781': 395, '35782': 396, '3581': 397, '3594': 398, '35981': 399, '36201': 400, '36221': 401, '36250': 402, '36281': 403, '36284': 404, '3643': 405, '3649': 406, '3659': 407, '3669': 408, '3682': 409, '36840': 410, '3688': 411, '36900': 412, '3693': 413, '3694': 414, '37272': 415, '37741': 416, '37852': 417, '37854': 418, '37992': 419, '3804': 420, '38300': 421, '38611': 422, '3899': 423, '3941': 424, '3949': 425, '3960': 426, '3962': 427, '3963': 428, '3968': 429, '3970': 430, '39890': 431, '39891': 432, '4019': 433, '40301': 434, '40310': 435, '40311': 436, '40390': 437, '40391': 438, '40501': 439, '41001': 440, '41011': 441, '41021': 442, '41031': 443, '41041': 444, '41042': 445, '41061': 446, '41071': 447, '41072': 448, '41081': 449, '41082': 450, '41091': 451, '4110': 452, '4111': 453, '41189': 454, '412': 455, '4139': 456, '41400': 457, '41401': 458, '41402': 459, '41404': 460, '41411': 461, '4142': 462, '4148': 463, '41511': 464, '41519': 465, '4160': 466, '4168': 467, '4169': 468, '4178': 469, '42090': 470, '42091': 471, '4210': 472, '4230': 473, '4232': 474, '4238': 475, '4239': 476, '4240': 477, '4241': 478, '4242': 479, '42490': 480, '4251': 481, '4254': 482, '4255': 483, '4260': 484, '42611': 485, '42612': 486, '42613': 487, '4262': 488, '4263': 489, '4264': 490, '4267': 491, '42682': 492, '42689': 493, '4270': 494, '4271': 495, '42731': 496, '42732': 497, '42741': 498, '4275': 499, '42781': 500, '42789': 501, '4280': 502, '4281': 503, '42820': 504, '42821': 505, '42822': 506, '42823': 507, '42830': 508, '42831': 509, '42832': 510, '42833': 511, '42840': 512, '42841': 513, '42843': 514, '4290': 515, '42989': 516, '4299': 517, '430': 518, '431': 519, '4321': 520, '4329': 521, '43300': 522, '43310': 523, '43320': 524, '43330': 525, '43411': 526, '43491': 527, '4352': 528, '4359': 529, '4370': 530, '4371': 531, '4372': 532, '4373': 533, '4374': 534, '4375': 535, '4380': 536, '43811': 537, '43819': 538, '43820': 539, '4387': 540, '43882': 541, '43883': 542, '43889': 543, '4389': 544, '4400': 545, '4401': 546, '44020': 547, '44021': 548, '44022': 549, '44023': 550, '44024': 551, '44029': 552, '44030': 553, '44100': 554, '44101': 555, '44102': 556, '44103': 557, '4411': 558, '4412': 559, '4413': 560, '4414': 561, '4417': 562, '4422': 563, '4423': 564, '4430': 565, '44389': 566, '4439': 567, '4440': 568, '4441': 569, '44421': 570, '44422': 571, '44481': 572, '44489': 573, '4460': 574, '4465': 575, '4466': 576, '4470': 577, '4471': 578, '4472': 579, '4476': 580, '4478': 581, '45182': 582, '45184': 583, '4532': 584, '45341': 585, '45342': 586, '45352': 587, '4538': 588, '4542': 589, '4550': 590, '4553': 591, '4558': 592, '4560': 593, '45620': 594, '45621': 595, '4568': 596, '4580': 597, '4582': 598, '45821': 599, '45829': 600, '4588': 601, '4589': 602, '4590': 603, '4592': 604, '45981': 605, '4613': 606, '462': 607, '46410': 608, '46451': 609, '4659': 610, '4660': 611, '4738': 612, '4739': 613, '4781': 614, '47829': 615, '47830': 616, '47831': 617, '4786': 618, '4809': 619, '481': 620, '4820': 621, '4821': 622, '4822': 623, '48230': 624, '48240': 625, '48241': 626, '48249': 627, '48282': 628, '48283': 629, '48289': 630, '4841': 631, '4846': 632, '4847': 633, '485': 634, '486': 635, '4870': 636, '4871': 637, '490': 638, '49120': 639, '49121': 640, '4920': 641, '4928': 642, '49320': 643, '49322': 644, '49390': 645, '49392': 646, '4940': 647, '496': 648, '500': 649, '501': 650, '5070': 651, '5081': 652, '5100': 653, '5109': 654, '5110': 655, '5111': 656, '5118': 657, '5119': 658, '5121': 659, '5128': 660, '5130': 661, '514': 662, '515': 663, '5163': 664, '5178': 665, '5180': 666, '5183': 667, '5184': 668, '5185': 669, '5187': 670, '51881': 671, '51882': 672, '51883': 673, '51884': 674, '51889': 675, '51901': 676, '51902': 677, '5191': 678, '51919': 679, '5192': 680, '5193': 681, '5194': 682, '52100': 683, '5224': 684, '5289': 685, '5300': 686, '53010': 687, '53012': 688, '53019': 689, '5302': 690, '53020': 691, '53021': 692, '5303': 693, '5304': 694, '5307': 695, '53081': 696, '53082': 697, '53084': 698, '53085': 699, '53089': 700, '53100': 701, '53140': 702, '53190': 703, '53200': 704, '53240': 705, '53260': 706, '53291': 707, '53340': 708, '53390': 709, '53501': 710, '53510': 711, '53550': 712, '53551': 713, '53560': 714, '53561': 715, '5363': 716, '53641': 717, '53642': 718, '53783': 719, '53784': 720, '53789': 721, '55012': 722, '55090': 723, '55092': 724, '55221': 725, '5528': 726, '5531': 727, '55320': 728, '55321': 729, '55329': 730, '5533': 731, '5552': 732, '5559': 733, '5569': 734, '5570': 735, '5571': 736, '5583': 737, '5589': 738, '5600': 739, '5601': 740, '5602': 741, '56081': 742, '56089': 743, '5609': 744, '56210': 745, '56211': 746, '56212': 747, '56213': 748, '56400': 749, '56409': 750, '5641': 751, '566': 752, '5672': 753, '56721': 754, '56722': 755, '56723': 756, '56729': 757, '56731': 758, '56738': 759, '5678': 760, '56782': 761, '56789': 762, '5680': 763, '56881': 764, '56889': 765, '5693': 766, '56962': 767, '56981': 768, '56983': 769, '56985': 770, '570': 771, '5711': 772, '5712': 773, '57149': 774, '5715': 775, '5716': 776, '5718': 777, '5720': 778, '5722': 779, '5723': 780, '5724': 781, '5728': 782, '5730': 783, '5733': 784, '5738': 785, '57400': 786, '57401': 787, '57410': 788, '57420': 789, '57421': 790, '57450': 791, '57451': 792, '57460': 793, '57471': 794, '57490': 795, '57491': 796, '5750': 797, '57511': 798, '57512': 799, '5758': 800, '5759': 801, '5761': 802, '5762': 803, '5768': 804, '5770': 805, '5771': 806, '5772': 807, '5778': 808, '5779': 809, '5780': 810, '5781': 811, '5789': 812, '5790': 813, '5793': 814, '5798': 815, '5799': 816, '58089': 817, '5821': 818, '58281': 819, '5829': 820, '58381': 821, '58389': 822, '5845': 823, '5849': 824, '585': 825, '5852': 826, '5853': 827, '5854': 828, '5856': 829, '5859': 830, '586': 831, '5881': 832, '58881': 833, '58889': 834, '591': 835, '5920': 836, '5921': 837, '5932': 838, '59381': 839, '59389': 840, '5939': 841, '59582': 842, '5960': 843, '5968': 844, '5988': 845, '5990': 846, '5997': 847, '59970': 848, '6000': 849, '60000': 850, '60001': 851, '60091': 852, '6039': 853, '60499': 854, '6084': 855, '60883': 856, '6110': 857, '6144': 858, '6146': 859, '6191': 860, '6238': 861, '68100': 862, '6822': 863, '6823': 864, '6824': 865, '6826': 866, '6827': 867, '6851': 868, '6910': 869, '6930': 870, '6951': 871, '6961': 872, '6983': 873, '7019': 874, '7070': 875, '70701': 876, '70703': 877, '70705': 878, '70706': 879, '70707': 880, '70709': 881, '70713': 882, '70714': 883, '70715': 884, '70719': 885, '70721': 886, '70724': 887, '7098': 888, '7099': 889, '7100': 890, '7109': 891, '71101': 892, '71104': 893, '71105': 894, '71107': 895, '71180': 896, '71230': 897, '7140': 898, '71535': 899, '71590': 900, '71596': 901, '71598': 902, '71847': 903, '71906': 904, '71941': 905, '71945': 906, '71946': 907, '71947': 908, '71965': 909, '7197': 910, '7202': 911, '7209': 912, '7210': 913, '7211': 914, '7213': 915, '72141': 916, '7220': 917, '72210': 918, '72272': 919, '7230': 920, '7231': 921, '7234': 922, '7236': 923, '72400': 924, '72401': 925, '72402': 926, '7242': 927, '7243': 928, '7245': 929, '7248': 930, '725': 931, '72888': 932, '72889': 933, '7291': 934, '72972': 935, '7299': 936, '72992': 937, '73004': 938, '73005': 939, '73007': 940, '73008': 941, '73009': 942, '73017': 943, '73018': 944, '73022': 945, '73025': 946, '73027': 947, '73028': 948, '7310': 949, '7318': 950, '73300': 951, '73311': 952, '73313': 953, '73315': 954, '73319': 955, '73342': 956, '73381': 957, '73382': 958, '73390': 959, '73679': 960, '73710': 961, '73730': 962, '73734': 963, '73743': 964, '7384': 965, '74101': 966, '74190': 967, '7421': 968, '7423': 969, '7450': 970, '7452': 971, '7454': 972, '7455': 973, '74561': 974, '74602': 975, '7464': 976, '7466': 977, '74689': 978, '7470': 979, '7473': 980, '74742': 981, '74783': 982, '7484': 983, '7509': 984, '7515': 985, '75251': 986, '75261': 987, '75263': 988, '75315': 989, '75322': 990, '75329': 991, '7533': 992, '75453': 993, '75470': 994, '75526': 995, '75529': 996, '75559': 997, '75567': 998, '75610': 999, '75651': 1000, '7580': 1001, '75889': 1002, '75981': 1003, '7599': 1004, '76072': 1005, '76075': 1006, '76077': 1007, '7608': 1008, '7615': 1009, '7617': 1010, '7630': 1011, '76383': 1012, '76402': 1013, '76407': 1014, '76408': 1015, '76409': 1016, '76492': 1017, '76493': 1018, '76496': 1019, '76502': 1020, '76503': 1021, '76513': 1022, '76514': 1023, '76515': 1024, '76516': 1025, '76517': 1026, '76518': 1027, '76519': 1028, '76522': 1029, '76523': 1030, '76524': 1031, '76525': 1032, '76526': 1033, '76527': 1034, '76528': 1035, '76529': 1036, '7660': 1037, '7661': 1038, '7662': 1039, '76621': 1040, '76711': 1041, '76719': 1042, '7678': 1043, '7689': 1044, '769': 1045, '7701': 1046, '7702': 1047, '7705': 1048, '7706': 1049, '7707': 1050, '7708': 1051, '77081': 1052, '77082': 1053, '77083': 1054, '77084': 1055, '77089': 1056, '7716': 1057, '7717': 1058, '77181': 1059, '77183': 1060, '77189': 1061, '77211': 1062, '77214': 1063, '7724': 1064, '7726': 1065, '7731': 1066, '7742': 1067, '7746': 1068, '7750': 1069, '7755': 1070, '7756': 1071, '7757': 1072, '77581': 1073, '7761': 1074, '7764': 1075, '7766': 1076, '7767': 1077, '7775': 1078, '7778': 1079, '7783': 1080, '7784': 1081, '7788': 1082, '7790': 1083, '7793': 1084, '77981': 1085, '77989': 1086, '78001': 1087, '78003': 1088, '78009': 1089, '7802': 1090, '78039': 1091, '78051': 1092, '78052': 1093, '78057': 1094, '7806': 1095, '78060': 1096, '78079': 1097, '7808': 1098, '7809': 1099, '78097': 1100, '7812': 1101, '7813': 1102, '7820': 1103, '7821': 1104, '7824': 1105, '7830': 1106, '78321': 1107, '7837': 1108, '7840': 1109, '7841': 1110, '7843': 1111, '78451': 1112, '7847': 1113, '7850': 1114, '7852': 1115, '7854': 1116, '78551': 1117, '78552': 1118, '78559': 1119, '7856': 1120, '78609': 1121, '7861': 1122, '7863': 1123, '78650': 1124, '78659': 1125, '7866': 1126, '78701': 1127, '78703': 1128, '7872': 1129, '7873': 1130, '78791': 1131, '78820': 1132, '78829': 1133, '78839': 1134, '78841': 1135, '78843': 1136, '7885': 1137, '78900': 1138, '78904': 1139, '78907': 1140, '7892': 1141, '7895': 1142, '78959': 1143, '79001': 1144, '79029': 1145, '7904': 1146, '7905': 1147, '7906': 1148, '7907': 1149, '79092': 1150, '79099': 1151, '7921': 1152, '7935': 1153, '7936': 1154, '79400': 1155, '79431': 1156, '79439': 1157, '7960': 1158, '7990': 1159, '79902': 1160, '7993': 1161, '7994': 1162, '80009': 1163, '80012': 1164, '80070': 1165, '80101': 1166, '80111': 1167, '80114': 1168, '80120': 1169, '80122': 1170, '80126': 1171, '8020': 1172, '8021': 1173, '80229': 1174, '8024': 1175, '8026': 1176, '8028': 1177, '80320': 1178, '80325': 1179, '80416': 1180, '80502': 1181, '80504': 1182, '80505': 1183, '80507': 1184, '8052': 1185, '8054': 1186, '8056': 1187, '80604': 1188, '80605': 1189, '80607': 1190, '80621': 1191, '80625': 1192, '80700': 1193, '80701': 1194, '80703': 1195, '80705': 1196, '80706': 1197, '80708': 1198, '80709': 1199, '8072': 1200, '8074': 1201, '8080': 1202, '8082': 1203, '8083': 1204, '80841': 1205, '80842': 1206, '80849': 1207, '81000': 1208, '81003': 1209, '81100': 1210, '81200': 1211, '81201': 1212, '81220': 1213, '81301': 1214, '81305': 1215, '81341': 1216, '81500': 1217, '82009': 1218, '82021': 1219, '82022': 1220, '82032': 1221, '8208': 1222, '82101': 1223, '82111': 1224, '82121': 1225, '82123': 1226, '82133': 1227, '8220': 1228, '82300': 1229, '82302': 1230, '82320': 1231, '82332': 1232, '82380': 1233, '82382': 1234, '8242': 1235, '8248': 1236, '8249': 1237, '83100': 1238, '83501': 1239, '83900': 1240, '83901': 1241, '83961': 1242, '8404': 1243, '8470': 1244, '8501': 1245, '8505': 1246, '85142': 1247, '85180': 1248, '85181': 1249, '85182': 1250, '85200': 1251, '85202': 1252, '85220': 1253, '85221': 1254, '85222': 1255, '85300': 1256, '85306': 1257, '85405': 1258, '8600': 1259, '8602': 1260, '8604': 1261, '86113': 1262, '86121': 1263, '86344': 1264, '86402': 1265, '86405': 1266, '86500': 1267, '86503': 1268, '86509': 1269, '86602': 1270, '86612': 1271, '8670': 1272, '86803': 1273, '86804': 1274, '86819': 1275, '8700': 1276, '8711': 1277, '87200': 1278, '8730': 1279, '87342': 1280, '87343': 1281, '87344': 1282, '87352': 1283, '87363': 1284, '8738': 1285, '87402': 1286, '8748': 1287, '8760': 1288, '8794': 1289, '88002': 1290, '88122': 1291, '8840': 1292, '8860': 1293, '8910': 1294, '9020': 1295, '90229': 1296, '9032': 1297, '9033': 1298, '9041': 1299, '90441': 1300, '9051': 1301, '9072': 1302, '9092': 1303, '9110': 1304, '9130': 1305, '9181': 1306, '920': 1307, '9219': 1308, '9221': 1309, '92300': 1310, '92400': 1311, '92421': 1312, '92811': 1313, '9331': 1314, '9351': 1315, '936': 1316, '94224': 1317, '9500': 1318, '9518': 1319, '9551': 1320, '9552': 1321, '9555': 1322, '9570': 1323, '9581': 1324, '9584': 1325, '9587': 1326, '95892': 1327, '95901': 1328, '9604': 1329, '9623': 1330, '96500': 1331, '96501': 1332, '96502': 1333, '96509': 1334, '9654': 1335, '9661': 1336, '9671': 1337, '9678': 1338, '9690': 1339, '9693': 1340, '9694': 1341, '9695': 1342, '9696': 1343, '9697': 1344, '9698': 1345, '9701': 1346, '9708': 1347, '97081': 1348, '9726': 1349, '9778': 1350, '9800': 1351, '9809': 1352, '9828': 1353, '990': 1354, '99591': 1355, '99592': 1356, '99594': 1357, '99601': 1358, '99602': 1359, '99603': 1360, '99604': 1361, '9961': 1362, '9962': 1363, '9964': 1364, '99644': 1365, '99659': 1366, '99661': 1367, '99662': 1368, '99664': 1369, '99667': 1370, '99669': 1371, '99671': 1372, '99672': 1373, '99673': 1374, '99674': 1375, '99679': 1376, '99681': 1377, '99682': 1378, '99685': 1379, '99702': 1380, '99709': 1381, '9971': 1382, '9972': 1383, '9973': 1384, '99731': 1385, '99739': 1386, '9974': 1387, '9975': 1388, '99762': 1389, '99769': 1390, '99771': 1391, '9980': 1392, '99811': 1393, '99812': 1394, '99813': 1395, '9982': 1396, '9983': 1397, '99831': 1398, '99832': 1399, '9984': 1400, '99859': 1401, '9986': 1402, '99881': 1403, '99883': 1404, '99889': 1405, '9991': 1406, '9992': 1407, '99931': 1408, '9998': 1409, '9999': 1410, 'E8120': 1411, 'E8122': 1412, 'E8136': 1413, 'E8147': 1414, 'E8150': 1415, 'E8160': 1416, 'E8190': 1417, 'E8192': 1418, 'E8210': 1419, 'E8217': 1420, 'E8230': 1421, 'E8348': 1422, 'E8490': 1423, 'E8495': 1424, 'E8497': 1425, 'E8498': 1426, 'E8499': 1427, 'E8500': 1428, 'E8501': 1429, 'E8502': 1430, 'E8532': 1431, 'E8538': 1432, 'E8541': 1433, 'E8543': 1434, 'E8550': 1435, 'E8580': 1436, 'E8600': 1437, 'E8668': 1438, 'E8708': 1439, 'E8780': 1440, 'E8781': 1441, 'E8782': 1442, 'E8786': 1443, 'E8788': 1444, 'E8789': 1445, 'E8790': 1446, 'E8791': 1447, 'E8792': 1448, 'E8796': 1449, 'E8798': 1450, 'E8799': 1451, 'E8801': 1452, 'E8809': 1453, 'E8810': 1454, 'E882': 1455, 'E8842': 1456, 'E8849': 1457, 'E8851': 1458, 'E8859': 1459, 'E8888': 1460, 'E8889': 1461, 'E9102': 1462, 'E911': 1463, 'E915': 1464, 'E916': 1465, 'E918': 1466, 'E9194': 1467, 'E9248': 1468, 'E927': 1469, 'E9289': 1470, 'E9290': 1471, 'E9305': 1472, 'E9308': 1473, 'E9310': 1474, 'E9317': 1475, 'E9320': 1476, 'E9323': 1477, 'E9331': 1478, 'E9342': 1479, 'E9344': 1480, 'E9347': 1481, 'E9352': 1482, 'E9359': 1483, 'E9379': 1484, 'E9383': 1485, 'E9398': 1486, 'E9421': 1487, 'E9478': 1488, 'E9500': 1489, 'E9502': 1490, 'E9503': 1491, 'E9504': 1492, 'E9509': 1493, 'E956': 1494, 'E9600': 1495, 'E966': 1496, 'E9682': 1497, 'E9689': 1498, 'E9800': 1499, 'E9803': 1500, 'E9804': 1501, 'E9809': 1502, 'V0254': 1503, 'V0259': 1504, 'V0262': 1505, 'V0381': 1506, 'V0382': 1507, 'V0489': 1508, 'V053': 1509, 'V058': 1510, 'V063': 1511, 'V071': 1512, 'V08': 1513, 'V090': 1514, 'V0980': 1515, 'V1000': 1516, 'V1005': 1517, 'V1006': 1518, 'V1007': 1519, 'V1009': 1520, 'V1011': 1521, 'V103': 1522, 'V1041': 1523, 'V1042': 1524, 'V1046': 1525, 'V1047': 1526, 'V1051': 1527, 'V1052': 1528, 'V1053': 1529, 'V1079': 1530, 'V1082': 1531, 'V1083': 1532, 'V1085': 1533, 'V113': 1534, 'V1201': 1535, 'V1203': 1536, 'V1209': 1537, 'V122': 1538, 'V1251': 1539, 'V1259': 1540, 'V1271': 1541, 'V1272': 1542, 'V1301': 1543, 'V141': 1544, 'V1507': 1545, 'V153': 1546, 'V155': 1547, 'V1581': 1548, 'V1582': 1549, 'V1584': 1550, 'V160': 1551, 'V170': 1552, 'V173': 1553, 'V174': 1554, 'V1749': 1555, 'V180': 1556, 'V290': 1557, 'V293': 1558, 'V298': 1559, 'V3000': 1560, 'V3001': 1561, 'V3100': 1562, 'V3101': 1563, 'V3401': 1564, 'V420': 1565, 'V422': 1566, 'V427': 1567, 'V4281': 1568, 'V4282': 1569, 'V4283': 1570, 'V433': 1571, 'V4364': 1572, 'V4365': 1573, 'V440': 1574, 'V441': 1575, 'V442': 1576, 'V4501': 1577, 'V4502': 1578, 'V4511': 1579, 'V4512': 1580, 'V452': 1581, 'V453': 1582, 'V454': 1583, 'V4573': 1584, 'V4579': 1585, 'V4581': 1586, 'V4582': 1587, 'V4586': 1588, 'V4589': 1589, 'V461': 1590, 'V4611': 1591, 'V4972': 1592, 'V4975': 1593, 'V4976': 1594, 'V4983': 1595, 'V4986': 1596, 'V502': 1597, 'V5331': 1598, 'V5332': 1599, 'V5391': 1600, 'V5417': 1601, 'V550': 1602, 'V552': 1603, 'V556': 1604, 'V5843': 1605, 'V5861': 1606, 'V5865': 1607, 'V5866': 1608, 'V5867': 1609, 'V5869': 1610, 'V600': 1611, 'V6149': 1612, 'V625': 1613, 'V632': 1614, 'V641': 1615, 'V644': 1616, 'V6441': 1617, 'V6442': 1618, 'V667': 1619, 'V707': 1620, 'V721': 1621, 'V8535': 1622, 'V854': 1623, 'V8741': 1624}\n"
     ]
    }
   ],
   "source": [
    "MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "task_fn = functools.partial(mortality_pred_task_per_patient, MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_)\n",
    "mor_dataset = mimic3base.set_task(task_fn, task_name=mortality_pred_task_per_patient.__name__)\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "mor_dataset.stat()\n",
    "mor_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "print(f\"MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "      f\"{MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "id": "e67dcd2f-cb6f-4b21-b692-4526d6aff390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "def readmission_pred_no_visit_task(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_no_visit_task(patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient) - 1):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit = patient[i]\n",
    "        next_visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        if next_visit.discharge_status not in [0, 1]:\n",
    "            mortality_label = 0\n",
    "        else:\n",
    "            mortality_label = int(next_visit.discharge_status)\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0: continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": mortality_label,\n",
    "            }\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "\n",
    "# mimic3sample = mimic3base.set_task(task_fn=drug_recommendation_mimic3_fn) # use default task\n",
    "# train_ds, val_ds, test_ds = split_by_patient(mimic3sample, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0281a-f95b-4991-abd5-5824fc37c520",
   "metadata": {},
   "source": [
    "### DataLoaders and Collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9be7a-3736-4bda-af61-5dcf331bb664",
   "metadata": {},
   "source": [
    "#### Bert Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "924c8e3c-0482-4688-b2d6-d83d490060a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def per_visit_collate_fn(data):\n",
    "#     \"\"\"\n",
    "#     Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "#         sequences to the sample shape (max # visits, len(freq_codes)). The padding infomation\n",
    "#         is stored in `mask`.\n",
    "    \n",
    "#     Arguments:\n",
    "#         data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "#     Outputs:\n",
    "#         x: a tensor of shape (# patiens, max # visits, len(freq_codes)) of type torch.float\n",
    "#         masks: a tensor of shape (# patiens, max # visits) of type torch.bool\n",
    "#         y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "#     Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "#         using: `sequences, labels = zip(*data)`\n",
    "#     \"\"\"\n",
    "\n",
    "#     sequences, labels = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "#     num_patients = len(sequences)\n",
    "#     num_visits = [len(patient) for patient in sequences]\n",
    "#     max_num_visits = max(num_visits)\n",
    "    \n",
    "#     x = torch.zeros((num_patients, max_num_visits, len(freq_codes)), dtype=torch.float)    \n",
    "#     for i_patient, patient in enumerate(sequences):\n",
    "#         kMaxVisits = len(patient)\n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             l = len(visit)\n",
    "#             for code in visit:\n",
    "#                 \"\"\"\n",
    "#                 TODO: 1. check if code is in freq_codes;\n",
    "#                       2. obtain the code index using code2idx;\n",
    "#                       3. set the correspoindg element in x to 1.\n",
    "#                 \"\"\"\n",
    "#                 try:\n",
    "#                     idx = code2idx[code]\n",
    "#                     x[i_patient, j_visit, idx] = 1\n",
    "#                 except KeyError as e:\n",
    "#                     pass\n",
    "    \n",
    "#     masks = torch.sum(x, dim=-1) > 0\n",
    "    \n",
    "#     return x, masks, y\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     \"\"\"\n",
    "#     TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "#         sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "#         is stored in `mask`.\n",
    "    \n",
    "#     Arguments:\n",
    "#         data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "#     Outputs:\n",
    "#         x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "#         masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "#         rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "#         rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "#         y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "#     Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "#         using: `sequences, labels = zip(*data)`\n",
    "#     \"\"\"\n",
    "\n",
    "#     sequences, labels = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "#     num_patients = len(sequences)\n",
    "#     num_visits = [len(patient) for patient in sequences]\n",
    "#     num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "#     max_num_visits = max(num_visits)\n",
    "#     max_num_codes = max(num_codes)\n",
    "    \n",
    "#     x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     for i_patient, patient in enumerate(sequences):\n",
    "#         kMaxVisits = len(patient)\n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             \"\"\"\n",
    "#             TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "#             \"\"\"\n",
    "#             l = len(visit)\n",
    "#             x[i_patient, j_visit, 0:l] = torch.LongTensor(visit)\n",
    "#             rev_x[i_patient, kMaxVisits-1-j_visit, 0:l] = torch.LongTensor(visit)\n",
    "#             masks[i_patient, j_visit, 0:l] = 1\n",
    "#             rev_masks[i_patient, kMaxVisits-1-j_visit, 0:l] = 1\n",
    "#         # print(f\"------ v: {kMaxVisits} ------\")\n",
    "#         # print(x[i_patient, :, ])\n",
    "#         # print(rev_x[i_patient, :, ])\n",
    "#         # print(masks[i_patient, :, ])\n",
    "#         # print(rev_masks[i_patient, :, ])\n",
    "    \n",
    "#     return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "\n",
    "def bert_per_patient_collate_function(data):\n",
    "    \"\"\"\n",
    "    Collates a tensor of (batch_size, seq_len, bert_emb_len) i.e. (32, <events per patient>, 768)\n",
    "    Need to pad the second dimension to max(<variable_per_patient>).\n",
    "    \n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (batch_size, max #conditions, max embedding size) of type torch.float\n",
    "        masks: a tensor of shape (batch_size, max #conditions, max embedding_size) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (batch_size) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    # print(f\"bert_per_patient_collate_function data[0] {data[0]}\")\n",
    "    sequences, labels = zip(*data)\n",
    "    \n",
    "    # Quick stats on the amount of memory in each batch.\n",
    "    sizes = [t.element_size() * t.nelement() for t in sequences]\n",
    "    print(f'BertCollate tensor sizes\\n'\n",
    "          f'cum_size:{sum(sizes) / 1.0e6} MB\\n'\n",
    "          f'sizes:{sizes}\\n'\n",
    "          f'{[tuple(x.shape) for x in sequences]}\\n')\n",
    "    \n",
    "    # Convert output labels for each sample in batch to tensor.\n",
    "    # shape: (batch_size, 1)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "   \n",
    "    num_patients = len(sequences)\n",
    "    num_events = [patient.shape[0] for patient in sequences]\n",
    "    embedding_length = [patient.shape[1] for patient in sequences]\n",
    "\n",
    "    max_num_events = max(num_events)\n",
    "    max_embedding_length = max(embedding_length)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    rev_x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    # Mask dimensions are 1 less than inputs.\n",
    "    masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        # Patient (#events, 768)\n",
    "        j_visits = patient.shape[0]\n",
    "        # for j_visit, visit in enumerate(patient):\n",
    "        \"\"\"\n",
    "        TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "        \"\"\"\n",
    "        # l = len(visit)\n",
    "        x[i_patient, :j_visits, :] = patient[:, :].unsqueeze(0)\n",
    "        # The tensor is (seq_length, emb_size). Leave embeddings,\n",
    "        # flip temporal order of code/event sequence.\n",
    "        rev_x[i_patient, :j_visits, :] = torch.flip(patient, dims=[0]).unsqueeze(0)\n",
    "        masks[i_patient, :j_visits] = 1\n",
    "        rev_masks[i_patient, :j_visits] = 1\n",
    "      \n",
    "        # TODO(botelho3) - comment this out to reduce spew.\n",
    "        # if i_patient == 0:\n",
    "        #     print(f\"------ p: {i_patient} ------\")\n",
    "        #     print(x[i_patient, :, :25])\n",
    "        #     print(rev_x[i_patient, :, :25])\n",
    "        #     print(masks[i_patient, :, :25])\n",
    "        #     print(rev_masks[i_patient, :, :25])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32036013-ce5d-407b-af7a-312cf2bc5f15",
   "metadata": {},
   "source": [
    "#### Bert Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "f6afd1f5-0f3b-4f4b-9c85-f74bb367ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use DistilBERT to decrease embedding time.\n",
    "# https://huggingface.co/docs/transformers/model_doc/distilbert?highlight=distilberttokenizerfast#distilbert\n",
    "\n",
    "class BertTextEmbedTransform(object):\n",
    "    \"\"\"Transform a sample's (a single visit's) text into 1 embedding vector.\n",
    "    \n",
    "    The embeddings of each text field are combined by embedding\n",
    "    each separately then summing.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/69517460/bert-get-sentence-embedding\n",
    "    # https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
    "    # https://gmihaila.github.io/tutorial_notebooks/bert_inner_workings/\n",
    "\n",
    "    def __init__(self, bert_model: Any, embedding_size: int, use_tokenizer_fast: bool):\n",
    "        assert isinstance(embedding_size, (int, tuple))\n",
    "        self.use_gpu = BERT_USE_GPU_\n",
    "        self.cuda = GPU_STR_\n",
    "        self.cache_dir = os.path.join(os.getcwd(), CACHE_DIR_)\n",
    "        self.bert_config = BertConfig(hidden_size=EMBEDDING_DIM_)\n",
    "        # self.bert_model = BertModel(self.bert_config).from_pretrained('bert-base-uncased', config=self.bert_config)\n",
    "        self.bert_model = BertModel(self.bert_config).from_pretrained('bert-base-uncased')\n",
    "        if self.use_gpu:\n",
    "            self.bert_model.to(self.cuda)\n",
    "        self.bert_config = self.bert_model.config\n",
    "        self.bert_model.eval()\n",
    "        # We unfortunately can't put the tokenizer on the GPU.\n",
    "        # https://stackoverflow.com/questions/66096703/running-huggingface-bert-tokenizer-on-gpu\n",
    "        if use_tokenizer_fast:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "    \n",
    "    def _tokenize_text(self, text: str) -> str:\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        return tokenized_text\n",
    "    \n",
    "    def _get_embeddings_of_sentences_with_mask(self, field, pad) -> torch.tensor:\n",
    "        return self._get_embeddings_of_sentences(field[:pad])\n",
    "        \n",
    "    \n",
    "    def _get_embeddings_of_sentences(self, sentences: List[str]) -> torch.tensor:\n",
    "        # tokenized_sentences = [self.tokenizer.tokenize(t, padding=True) for t in sentences]\n",
    "        # print(batch_enc)\n",
    "        # Tokenize the input sentence with attention masks.\n",
    "        batch_enc = self.tokenizer.batch_encode_plus(\n",
    "                                sentences, padding=True,\n",
    "                                return_attention_mask=True, return_length=True)\n",
    "        # print(f'batch_enc: {batch_enc.keys()}')\n",
    "        # print(f'batch_enc: {[type(val[0]) for key, val in batch_enc.items()]}')\n",
    "        # print(f'batch_enc {batch_enc}')\n",
    "        batch_enc_tensor = batch_enc.convert_to_tensors(tensor_type=TensorType.PYTORCH)\n",
    "        if self.use_gpu:\n",
    "            batch_enc_tensor.to(self.cuda)\n",
    "      \n",
    "        # Run BERT model forward pass on tokenized input.\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.bert_model(input_ids=batch_enc_tensor['input_ids'],\n",
    "                                         attention_mask=batch_enc_tensor['attention_mask'],\n",
    "                                         # could turn off so we're faster\n",
    "                                         output_attentions=True)\n",
    "        # embeddings, _ = self.bert_model(**batch_enc)\n",
    "        # print(f'embeddings:\\n {dir(embeddings)}')\n",
    "        #attention = encoded['attention_mask'].reshape((lhs.size()[0], lhs.size()[1], -1)).expand(-1, -1, 768)\n",
    "       \n",
    "        # These may be on GPU.\n",
    "        return embeddings.last_hidden_state, embeddings.attentions \n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        len_embeddings = (len(sample['conditions_text']) +\n",
    "                          len(sample['procedures_text']) +\n",
    "                          len(sample['drugs_text']))\n",
    "      \n",
    "        sample_text = []\n",
    "        sample_masks = []\n",
    "        if sample.get('conditions_text_pad'):\n",
    "            pad = sample['conditions_text_pad']\n",
    "            sample_text.extend(sample['conditions_text'][:pad])\n",
    "        if sample.get('procedures_text_pad'):\n",
    "            pad = sample['procedures_text_pad']\n",
    "            sample_text.extend(sample['procedures_text'][:pad])\n",
    "        if sample.get('drugs_text_pad'):\n",
    "            pad = sample['drugs_text_pad']\n",
    "            sample_text.extend(sample['drugs_text'][:pad])\n",
    "           \n",
    "        sample_embeddings, sample_attentions = self._get_embeddings_of_sentences(sample_text)\n",
    "        \n",
    "        # Take only the last hidden state embeddings from BERT.\n",
    "        embeddings = torch.squeeze(sample_embeddings[:, -1, :], dim=1)\n",
    "        if self.use_gpu:\n",
    "            assert(embeddings.device == torch.device('cuda:0'))\n",
    "        else:\n",
    "            assert(embeddings.device == torch.device('cpu'))\n",
    "        if self.use_gpu:\n",
    "            embeddings = embeddings.to('cpu')\n",
    "        \n",
    "        # We could multiply by attentions here:\n",
    "        # attentions = torch.squeeze(sample_attentions[:, -1, :], dim=1)\n",
    "        # embeddings = embeddings * attentions\n",
    "\n",
    "        # The 1st dimension is seq length. The second dimension is embedding length of each sentence.\n",
    "        assert(embeddings.shape[-1] == self.bert_config.hidden_size)\n",
    "        assert(len(embeddings.shape) == 2)\n",
    "        \n",
    "        # Return the ((model_inputs), label) \n",
    "        return (embeddings, sample['label'])\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     def __call__(self, sample):\n",
    "#         condition_embeddings = [None] * len(sample['conditions_text'])\n",
    "#         procedure_embeddings = [None] * len(sample['procedures_text'])\n",
    "#         drug_embeddings = [None] * len(sample['drugs_text'])\n",
    "       \n",
    "#         if sample.get('conditions_text_pad'):\n",
    "#             condition_embeddings, condition_attentions = self._get_embeddings_of_sentences_with_mask(\n",
    "#                 sample['conditions_text'], sample['conditions_text_pad'])\n",
    "#         else:\n",
    "#             condition_embeddings, condition_attentions = self._get_embeddings_of_sentences(sample['conditions_text'])\n",
    "#         # for cond in sample['conditions_text']:\n",
    "#         #     #bert_model(cond)\n",
    "#         #     condition_embeddings.append(cond)\n",
    "#         #     pass\n",
    "#         if sample.get('procedures_text_pad'):\n",
    "#             procedure_embeddings, procedure_attentions = self._get_embeddings_of_sentences_with_mask(\n",
    "#                 sample['procedures_text'], sample['procedures_text_pad'])\n",
    "#         else:\n",
    "#             procedure_embeddings, procedure_attentions = self._get_embeddings_of_sentences(sample['procedures_text'])\n",
    "#         # for proc in sample['procedures_text']:\n",
    "#         #     #bert_model(proc)\n",
    "#         #     procedure_embeddings.append(proc)\n",
    "#         #     pass\n",
    "#         if (sample.get('drugs_text_pad')):\n",
    "#             drug_embeddings, drug_attentions = self._get_embeddings_of_sentences_with_mask(\n",
    "#                 sample['drugs_text'], sample['drugs_text_pad'])\n",
    "#         else:\n",
    "#             drug_embeddings, drug_attentions = self._get_embeddings_of_sentences(sample['drugs_text'])\n",
    "#         # for drug in sample['drugs_text']:\n",
    "#         #     #bert_model(drug)\n",
    "#         #     procedure_embeddings.append(drug)\n",
    "#         #     pass\n",
    "        \n",
    "#         # Take only the last hidden state embeddings from BERT.\n",
    "#         condition_embeddings = torch.squeeze(condition_embeddings[:, -1, :], dim=1)\n",
    "#         procedure_embeddings = torch.squeeze(procedure_embeddings[:, -1, :], dim=1)\n",
    "#         drug_embeddings = torch.squeeze(drug_embeddings[:, -1, :], dim=1)\n",
    "#         print(f'ce: {condition_embeddings.shape} '\n",
    "#               f'pe: {procedure_embeddings.shape} '\n",
    "#               f'de: {drug_embeddings.shape} ')\n",
    "       \n",
    "#         stacked = torch.cat([condition_embeddings, procedure_embeddings, drug_embeddings], dim=0)\n",
    "        \n",
    "#         # We don't want to normalize here because we need a sequence of embeddings for each sample. \n",
    "#         # normalize the final row (across columns)\n",
    "#         # summed_embeddings = torch.sum(stacked, dim=0, keepdim=True)  # sum across rows\n",
    "#         # summed_embeddings = torch.nn.functional.normalize(summed_embeddings, dim=1)\n",
    "#         # print(summed_embeddings.shape)\n",
    "#         # assert(summed_embeddings.shape == (1, self.bert_config.hidden_size))\n",
    "       \n",
    "#         # The 1st dimension is seq length. The second dimension is embedding length of each sentence.\n",
    "#         assert(stacked.shape[-1] == self.bert_config.hidden_size)\n",
    "#         assert(len(stacked.shape) == 2)\n",
    "#         return (stacked, sample['label'])\n",
    "    \n",
    "    \n",
    "class TextEmbedDataset(SampleDataset):\n",
    "    '''The BERT text embedding process is very slow. We want to avoid it.\n",
    "   \n",
    "    To prevent re-processing of the same input cache the sample locally.\n",
    "    Some suggestions here:\n",
    "        https://stackoverflow.com/questions/61393613/pytorch-speed-up-data-loading.\n",
    "        https://discuss.pytorch.org/t/best-practice-to-cache-the-entire-dataset-during-first-epoch/19608\n",
    "        \n",
    "    1. Preprocess and write the preprocessed text back out to disk.\n",
    "    2. Cache the transform output in a hashtable. See functools.lru_cache().\n",
    "    3. https://pytorch.org/data/main/ ?\n",
    "    \n",
    "    Some concerns related to num_workers > 1, i.e. multiprocessing enabled.\n",
    "    See torch.save() to cache a tensor.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dataset: SampleDataset, transform=None):\n",
    "        \"\"\"Wraps a SampleEHRDataset with transforms.\n",
    "        Arguments:\n",
    "            dataset: dataset to transform\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.transformed = [False for x in dataset]\n",
    "        super().__init__([x for x in dataset])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            assert(False)\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        # Cache the transformed version of the data.\n",
    "        if self.transform and not self.transformed[idx]:\n",
    "            sample = self.transform(sample)\n",
    "            self.samples[idx] = sample\n",
    "            self.transformed[idx] = True\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "1056b60a-5861-4e47-b694-891053cc8470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Generating samples for mortality_pred_task_per_patient: 100%|█████████████████████████████████████████| 1000/1000 [00:00<00:00, 5277.71it/s]\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_xform = BertTextEmbedTransform(None, EMBEDDING_DIM_, use_tokenizer_fast=True)\n",
    "BERT_EMBEDDING_SIZE = bert_xform.bert_config.hidden_size\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "MORT_DEMB_CODE2IDX = {}\n",
    "MORT_DEMB_CODE_COUNT = {}\n",
    "task_fn = functools.partial(mortality_pred_task_per_patient, MORT_DEMB_CODE_COUNT)\n",
    "mortality_demb_dataset = mimic3base.set_task(task_fn, task_name=mortality_pred_task_per_patient.__name__)\n",
    "MORT_DEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORT_DEMB_CODE_COUNT.keys()))\n",
    "}\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "mortality_demb_dataset = TextEmbedDataset(mortality_demb_dataset, transform=bert_xform)\n",
    "mort_demb_train_ds, mort_demb_val_ds, mort_demb_test_ds = split_by_patient(mortality_demb_dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# create dataloaders (torch.data.DataLoader)\n",
    "# mort_train_loader = get_dataloader(train_ds, batch_size=32, shuffle=True, collate_fn)\n",
    "# mort_val_loader = get_dataloader(val_ds, batch_size=32, shuffle=False)\n",
    "# mort_test_loader = get_dataloader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "mort_demb_train_loader = DataLoader(\n",
    "    mort_demb_train_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n",
    "mort_demb_val_loader = DataLoader(\n",
    "    mort_demb_val_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n",
    "mort_demb_test_loader = DataLoader(\n",
    "    mort_demb_test_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "#     '''\n",
    "#     TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "#     Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "#     Arguments:\n",
    "#         train dataset: train dataset of type `CustomDataset`\n",
    "#         val dataset: validation dataset of type `CustomDataset`\n",
    "#         collate_fn: collate function\n",
    "        \n",
    "#     Outputs:\n",
    "#         train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "#     Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "#     '''\n",
    "    \n",
    "#     batch_size = 32\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "\n",
    "\n",
    "#     return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "035fccee-d5da-428c-a54a-0678a2d19b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertCollate tensor sizes\n",
      "cum_size:4.386816 MB\n",
      "sizes:[181248, 24576, 18432, 187392, 215040, 21504, 172032, 193536, 297984, 156672, 178176, 39936, 159744, 125952, 30720, 168960, 135168, 190464, 24576, 168960, 64512, 184320, 187392, 276480, 187392, 24576, 199680, 39936, 36864, 153600, 67584, 273408]\n",
      "[(59, 768), (8, 768), (6, 768), (61, 768), (70, 768), (7, 768), (56, 768), (63, 768), (97, 768), (51, 768), (58, 768), (13, 768), (52, 768), (41, 768), (10, 768), (55, 768), (44, 768), (62, 768), (8, 768), (55, 768), (21, 768), (60, 768), (61, 768), (90, 768), (61, 768), (8, 768), (65, 768), (13, 768), (12, 768), (50, 768), (22, 768), (89, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.534272 MB\n",
      "sizes:[172032, 18432, 181248, 175104, 156672, 55296, 215040, 261120, 33792, 178176, 98304, 178176, 159744, 150528, 27648, 156672, 43008, 202752, 264192, 178176, 24576, 181248, 129024, 156672, 168960, 245760, 184320, 113664, 165888, 64512, 172032, 21504]\n",
      "[(56, 768), (6, 768), (59, 768), (57, 768), (51, 768), (18, 768), (70, 768), (85, 768), (11, 768), (58, 768), (32, 768), (58, 768), (52, 768), (49, 768), (9, 768), (51, 768), (14, 768), (66, 768), (86, 768), (58, 768), (8, 768), (59, 768), (42, 768), (51, 768), (55, 768), (80, 768), (60, 768), (37, 768), (54, 768), (21, 768), (56, 768), (7, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.432896 MB\n",
      "sizes:[79872, 55296, 141312, 193536, 52224, 168960, 64512, 153600, 172032, 33792, 86016, 135168, 46080, 147456, 236544, 156672, 24576, 159744, 135168, 165888, 181248, 221184, 15360, 168960, 55296, 153600, 135168, 251904, 181248, 150528, 172032, 337920]\n",
      "[(26, 768), (18, 768), (46, 768), (63, 768), (17, 768), (55, 768), (21, 768), (50, 768), (56, 768), (11, 768), (28, 768), (44, 768), (15, 768), (48, 768), (77, 768), (51, 768), (8, 768), (52, 768), (44, 768), (54, 768), (59, 768), (72, 768), (5, 768), (55, 768), (18, 768), (50, 768), (44, 768), (82, 768), (59, 768), (49, 768), (56, 768), (110, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.534272 MB\n",
      "sizes:[199680, 43008, 178176, 187392, 73728, 138240, 162816, 190464, 184320, 187392, 172032, 21504, 39936, 141312, 175104, 175104, 110592, 153600, 18432, 175104, 181248, 159744, 110592, 233472, 27648, 159744, 165888, 30720, 337920, 193536, 187392, 18432]\n",
      "[(65, 768), (14, 768), (58, 768), (61, 768), (24, 768), (45, 768), (53, 768), (62, 768), (60, 768), (61, 768), (56, 768), (7, 768), (13, 768), (46, 768), (57, 768), (57, 768), (36, 768), (50, 768), (6, 768), (57, 768), (59, 768), (52, 768), (36, 768), (76, 768), (9, 768), (52, 768), (54, 768), (10, 768), (110, 768), (63, 768), (61, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.537344 MB\n",
      "sizes:[129024, 172032, 236544, 36864, 30720, 175104, 15360, 221184, 175104, 181248, 159744, 43008, 156672, 18432, 122880, 175104, 178176, 168960, 33792, 267264, 86016, 165888, 301056, 21504, 175104, 178176, 156672, 162816, 138240, 196608, 162816, 95232]\n",
      "[(42, 768), (56, 768), (77, 768), (12, 768), (10, 768), (57, 768), (5, 768), (72, 768), (57, 768), (59, 768), (52, 768), (14, 768), (51, 768), (6, 768), (40, 768), (57, 768), (58, 768), (55, 768), (11, 768), (87, 768), (28, 768), (54, 768), (98, 768), (7, 768), (57, 768), (58, 768), (51, 768), (53, 768), (45, 768), (64, 768), (53, 768), (31, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.786176 MB\n",
      "sizes:[104448, 58368, 27648, 122880, 165888, 165888, 162816, 168960, 144384, 368640, 190464, 147456, 58368, 208896, 147456, 288768, 113664, 150528, 122880, 193536, 267264, 79872, 21504, 73728, 156672, 205824, 122880, 193536, 101376, 236544, 162816, 52224]\n",
      "[(34, 768), (19, 768), (9, 768), (40, 768), (54, 768), (54, 768), (53, 768), (55, 768), (47, 768), (120, 768), (62, 768), (48, 768), (19, 768), (68, 768), (48, 768), (94, 768), (37, 768), (49, 768), (40, 768), (63, 768), (87, 768), (26, 768), (7, 768), (24, 768), (51, 768), (67, 768), (40, 768), (63, 768), (33, 768), (77, 768), (53, 768), (17, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.813824 MB\n",
      "sizes:[184320, 58368, 159744, 150528, 24576, 184320, 159744, 172032, 18432, 79872, 33792, 347136, 33792, 162816, 18432, 129024, 125952, 316416, 215040, 178176, 153600, 156672, 242688, 181248, 215040, 73728, 24576, 294912, 261120, 172032, 15360, 270336]\n",
      "[(60, 768), (19, 768), (52, 768), (49, 768), (8, 768), (60, 768), (52, 768), (56, 768), (6, 768), (26, 768), (11, 768), (113, 768), (11, 768), (53, 768), (6, 768), (42, 768), (41, 768), (103, 768), (70, 768), (58, 768), (50, 768), (51, 768), (79, 768), (59, 768), (70, 768), (24, 768), (8, 768), (96, 768), (85, 768), (56, 768), (5, 768), (88, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.926016 MB\n",
      "sizes:[175104, 95232, 144384, 138240, 245760, 18432, 15360, 141312, 168960, 15360, 168960, 135168, 178176, 18432, 55296, 236544, 211968, 193536, 27648, 168960, 36864, 178176, 36864, 190464, 159744, 190464, 58368, 24576, 30720, 181248, 159744, 125952]\n",
      "[(57, 768), (31, 768), (47, 768), (45, 768), (80, 768), (6, 768), (5, 768), (46, 768), (55, 768), (5, 768), (55, 768), (44, 768), (58, 768), (6, 768), (18, 768), (77, 768), (69, 768), (63, 768), (9, 768), (55, 768), (12, 768), (58, 768), (12, 768), (62, 768), (52, 768), (62, 768), (19, 768), (8, 768), (10, 768), (59, 768), (52, 768), (41, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.896768 MB\n",
      "sizes:[218112, 172032, 153600, 153600, 15360, 18432, 184320, 261120, 162816, 159744, 162816, 30720, 43008, 159744, 153600, 92160, 285696, 224256, 162816, 199680, 21504, 202752, 301056, 175104, 144384, 144384, 135168, 64512, 205824, 282624, 33792, 172032]\n",
      "[(71, 768), (56, 768), (50, 768), (50, 768), (5, 768), (6, 768), (60, 768), (85, 768), (53, 768), (52, 768), (53, 768), (10, 768), (14, 768), (52, 768), (50, 768), (30, 768), (93, 768), (73, 768), (53, 768), (65, 768), (7, 768), (66, 768), (98, 768), (57, 768), (47, 768), (47, 768), (44, 768), (21, 768), (67, 768), (92, 768), (11, 768), (56, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.631104 MB\n",
      "sizes:[153600, 64512, 18432, 168960, 30720, 104448, 150528, 58368, 362496, 175104, 24576, 147456, 33792, 58368, 162816, 98304, 15360, 156672, 165888, 162816, 36864, 172032, 36864, 46080, 144384, 18432, 33792, 150528, 21504, 294912, 193536, 168960]\n",
      "[(50, 768), (21, 768), (6, 768), (55, 768), (10, 768), (34, 768), (49, 768), (19, 768), (118, 768), (57, 768), (8, 768), (48, 768), (11, 768), (19, 768), (53, 768), (32, 768), (5, 768), (51, 768), (54, 768), (53, 768), (12, 768), (56, 768), (12, 768), (15, 768), (47, 768), (6, 768), (11, 768), (49, 768), (7, 768), (96, 768), (63, 768), (55, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.420608 MB\n",
      "sizes:[162816, 18432, 39936, 104448, 368640, 181248, 202752, 138240, 70656, 301056, 43008, 30720, 138240, 70656, 150528, 113664, 172032, 236544, 233472, 156672, 39936, 27648, 178176, 218112, 153600, 184320, 33792, 33792, 190464, 227328, 144384, 55296]\n",
      "[(53, 768), (6, 768), (13, 768), (34, 768), (120, 768), (59, 768), (66, 768), (45, 768), (23, 768), (98, 768), (14, 768), (10, 768), (45, 768), (23, 768), (49, 768), (37, 768), (56, 768), (77, 768), (76, 768), (51, 768), (13, 768), (9, 768), (58, 768), (71, 768), (50, 768), (60, 768), (11, 768), (11, 768), (62, 768), (74, 768), (47, 768), (18, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.538944 MB\n",
      "sizes:[43008, 43008, 181248, 92160, 159744, 208896, 138240, 18432, 89088, 162816, 254976, 21504, 325632, 98304, 15360, 113664, 24576, 196608, 221184, 18432, 33792, 52224, 27648, 125952, 92160, 18432, 30720, 245760, 144384, 150528, 153600, 36864]\n",
      "[(14, 768), (14, 768), (59, 768), (30, 768), (52, 768), (68, 768), (45, 768), (6, 768), (29, 768), (53, 768), (83, 768), (7, 768), (106, 768), (32, 768), (5, 768), (37, 768), (8, 768), (64, 768), (72, 768), (6, 768), (11, 768), (17, 768), (9, 768), (41, 768), (30, 768), (6, 768), (10, 768), (80, 768), (47, 768), (49, 768), (50, 768), (12, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.43904 MB\n",
      "sizes:[39936, 70656, 18432, 135168, 162816, 138240, 230400, 162816, 353280, 110592, 279552, 162816, 46080, 55296, 30720, 64512, 175104, 199680, 258048, 218112, 172032, 215040, 73728, 181248, 61440, 175104, 15360, 39936, 147456, 236544, 165888, 43008]\n",
      "[(13, 768), (23, 768), (6, 768), (44, 768), (53, 768), (45, 768), (75, 768), (53, 768), (115, 768), (36, 768), (91, 768), (53, 768), (15, 768), (18, 768), (10, 768), (21, 768), (57, 768), (65, 768), (84, 768), (71, 768), (56, 768), (70, 768), (24, 768), (59, 768), (20, 768), (57, 768), (5, 768), (13, 768), (48, 768), (77, 768), (54, 768), (14, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.629504 MB\n",
      "sizes:[153600, 159744, 165888, 156672, 156672, 27648, 208896, 18432, 24576, 33792, 288768, 193536, 144384, 138240, 141312, 150528, 150528, 150528, 181248, 159744, 15360, 304128, 156672, 181248, 15360, 43008, 236544, 215040, 175104, 224256, 202752, 55296]\n",
      "[(50, 768), (52, 768), (54, 768), (51, 768), (51, 768), (9, 768), (68, 768), (6, 768), (8, 768), (11, 768), (94, 768), (63, 768), (47, 768), (45, 768), (46, 768), (49, 768), (49, 768), (49, 768), (59, 768), (52, 768), (5, 768), (99, 768), (51, 768), (59, 768), (5, 768), (14, 768), (77, 768), (70, 768), (57, 768), (73, 768), (66, 768), (18, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.677184 MB\n",
      "sizes:[242688, 27648, 202752, 276480, 162816, 30720, 310272, 67584, 49152, 55296, 125952, 43008, 46080, 36864, 187392, 36864, 30720, 156672, 36864, 165888, 18432, 39936, 165888, 156672, 24576, 190464, 55296, 159744, 178176, 39936, 147456, 208896]\n",
      "[(79, 768), (9, 768), (66, 768), (90, 768), (53, 768), (10, 768), (101, 768), (22, 768), (16, 768), (18, 768), (41, 768), (14, 768), (15, 768), (12, 768), (61, 768), (12, 768), (10, 768), (51, 768), (12, 768), (54, 768), (6, 768), (13, 768), (54, 768), (51, 768), (8, 768), (62, 768), (18, 768), (52, 768), (58, 768), (13, 768), (48, 768), (68, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.098048 MB\n",
      "sizes:[49152, 156672, 147456, 190464, 39936, 175104, 230400, 33792, 30720, 159744, 18432, 27648, 245760, 33792, 58368, 55296, 153600, 224256, 181248, 156672, 261120, 153600, 49152, 153600, 144384, 156672, 144384, 15360, 172032, 245760, 30720, 202752]\n",
      "[(16, 768), (51, 768), (48, 768), (62, 768), (13, 768), (57, 768), (75, 768), (11, 768), (10, 768), (52, 768), (6, 768), (9, 768), (80, 768), (11, 768), (19, 768), (18, 768), (50, 768), (73, 768), (59, 768), (51, 768), (85, 768), (50, 768), (16, 768), (50, 768), (47, 768), (51, 768), (47, 768), (5, 768), (56, 768), (80, 768), (10, 768), (66, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.803136 MB\n",
      "sizes:[193536, 43008, 162816, 205824, 205824, 89088, 224256, 82944, 86016, 30720, 15360, 162816, 150528, 64512, 18432, 82944, 172032, 76800, 208896, 150528, 165888, 153600, 147456, 141312, 211968, 24576, 125952, 52224, 27648, 110592, 175104, 39936]\n",
      "[(63, 768), (14, 768), (53, 768), (67, 768), (67, 768), (29, 768), (73, 768), (27, 768), (28, 768), (10, 768), (5, 768), (53, 768), (49, 768), (21, 768), (6, 768), (27, 768), (56, 768), (25, 768), (68, 768), (49, 768), (54, 768), (50, 768), (48, 768), (46, 768), (69, 768), (8, 768), (41, 768), (17, 768), (9, 768), (36, 768), (57, 768), (13, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.033536 MB\n",
      "sizes:[39936, 168960, 153600, 107520, 172032, 58368, 153600, 162816, 147456, 168960, 21504, 175104, 21504, 33792, 168960, 36864, 181248, 46080, 138240, 215040, 64512, 150528, 218112, 30720, 132096, 227328, 159744, 138240, 184320, 21504, 184320, 150528]\n",
      "[(13, 768), (55, 768), (50, 768), (35, 768), (56, 768), (19, 768), (50, 768), (53, 768), (48, 768), (55, 768), (7, 768), (57, 768), (7, 768), (11, 768), (55, 768), (12, 768), (59, 768), (15, 768), (45, 768), (70, 768), (21, 768), (49, 768), (71, 768), (10, 768), (43, 768), (74, 768), (52, 768), (45, 768), (60, 768), (7, 768), (60, 768), (49, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.975168 MB\n",
      "sizes:[202752, 273408, 150528, 21504, 36864, 18432, 202752, 92160, 95232, 43008, 159744, 135168, 116736, 64512, 175104, 181248, 76800, 52224, 239616, 175104, 43008, 147456, 61440, 153600, 276480, 67584, 153600, 33792, 239616, 27648, 239616, 18432]\n",
      "[(66, 768), (89, 768), (49, 768), (7, 768), (12, 768), (6, 768), (66, 768), (30, 768), (31, 768), (14, 768), (52, 768), (44, 768), (38, 768), (21, 768), (57, 768), (59, 768), (25, 768), (17, 768), (78, 768), (57, 768), (14, 768), (48, 768), (20, 768), (50, 768), (90, 768), (22, 768), (50, 768), (11, 768), (78, 768), (9, 768), (78, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.972096 MB\n",
      "sizes:[46080, 168960, 122880, 279552, 39936, 156672, 193536, 187392, 43008, 159744, 156672, 27648, 245760, 264192, 196608, 30720, 153600, 76800, 95232, 172032, 24576, 211968, 113664, 165888, 36864, 18432, 215040, 61440, 215040, 18432, 18432, 55296]\n",
      "[(15, 768), (55, 768), (40, 768), (91, 768), (13, 768), (51, 768), (63, 768), (61, 768), (14, 768), (52, 768), (51, 768), (9, 768), (80, 768), (86, 768), (64, 768), (10, 768), (50, 768), (25, 768), (31, 768), (56, 768), (8, 768), (69, 768), (37, 768), (54, 768), (12, 768), (6, 768), (70, 768), (20, 768), (70, 768), (6, 768), (6, 768), (18, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.926016 MB\n",
      "sizes:[113664, 156672, 147456, 138240, 24576, 110592, 24576, 150528, 288768, 301056, 150528, 21504, 110592, 36864, 196608, 175104, 27648, 21504, 181248, 24576, 153600, 141312, 141312, 55296, 156672, 156672, 119808, 132096, 162816, 153600, 55296, 95232]\n",
      "[(37, 768), (51, 768), (48, 768), (45, 768), (8, 768), (36, 768), (8, 768), (49, 768), (94, 768), (98, 768), (49, 768), (7, 768), (36, 768), (12, 768), (64, 768), (57, 768), (9, 768), (7, 768), (59, 768), (8, 768), (50, 768), (46, 768), (46, 768), (18, 768), (51, 768), (51, 768), (39, 768), (43, 768), (53, 768), (50, 768), (18, 768), (31, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.236288 MB\n",
      "sizes:[236544, 21504, 168960, 86016, 172032, 162816, 30720, 168960, 328704, 264192, 86016, 248832, 159744, 107520, 18432, 156672, 15360, 55296, 82944, 187392, 150528, 288768, 172032, 46080, 159744, 39936, 27648, 18432, 21504, 316416, 79872, 156672]\n",
      "[(77, 768), (7, 768), (55, 768), (28, 768), (56, 768), (53, 768), (10, 768), (55, 768), (107, 768), (86, 768), (28, 768), (81, 768), (52, 768), (35, 768), (6, 768), (51, 768), (5, 768), (18, 768), (27, 768), (61, 768), (49, 768), (94, 768), (56, 768), (15, 768), (52, 768), (13, 768), (9, 768), (6, 768), (7, 768), (103, 768), (26, 768), (51, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:1.207296 MB\n",
      "sizes:[116736, 70656, 211968, 30720, 181248, 153600, 175104, 175104, 61440, 30720]\n",
      "[(38, 768), (23, 768), (69, 768), (10, 768), (59, 768), (50, 768), (57, 768), (57, 768), (20, 768), (10, 768)]\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[578], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m masks\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m rev_masks\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (BATCH_SIZE_, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m105\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (BATCH_SIZE_, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m masks\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (BATCH_SIZE_, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Verify DataLoader properties.\n",
    "# Quick test without running the whole RNN training process.\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(mort_demb_train_loader)\n",
    "for _ in loader_iter:\n",
    "    pass\n",
    "try:\n",
    "    x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "except StopIteration as e:\n",
    "    print(e)\n",
    "\n",
    "assert x.dtype == torch.float\n",
    "assert rev_x.dtype == torch.float\n",
    "assert y.dtype == torch.float\n",
    "assert masks.dtype == torch.bool\n",
    "assert rev_masks.dtype == torch.bool\n",
    "\n",
    "assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "assert y.shape == (BATCH_SIZE_, 1)\n",
    "assert masks.shape == (BATCH_SIZE_, 10, 3)\n",
    "\n",
    "# assert x[0][0].sum() == 9\n",
    "# assert masks[0].sum() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c1852-2654-4d7c-9caa-4cd29ae6fd91",
   "metadata": {},
   "source": [
    "#### CodeEmb Collate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "992467f7-d834-4d4a-872a-714389fa203c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def code_emb_per_patient_collate_function(data):\n",
    "    \"\"\"\n",
    "    UNUSED DONT PAY ATTENTION\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "    sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "    is stored in `mask`.\n",
    "                                   \n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "    x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "    masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "    rev_x: same as x but in reversed time. This will be used in our RNN model for masking\n",
    "    rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "    y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\\n\",\n",
    "    using: `sequences, labels = zip(*data)`\\n\",\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = data\n",
    "    print(f'code_emb_per_patient_collate_function data[0]:\\n{data[0]}')\n",
    "    \n",
    "    y = torch.tensor([s['label'] for s in samples], dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(samples)\n",
    "    num_events = [len(samples['conditions']) for patient in samples]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_events = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        kMaxVisits = len(patient)\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            l = len(visit)\n",
    "            x[i_patient, j_visit, 0:l] = torch.LongTensor(visit)\n",
    "            rev_x[i_patient, kMaxVisits-1-j_visit, 0:l] = torch.LongTensor(visit)\n",
    "            masks[i_patient, j_visit, 0:l] = 1\n",
    "            rev_masks[i_patient, kMaxVisits-1-j_visit, 0:l] = 1\n",
    "\n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "def code_emb_per_visit_collate_function(code2idx: Dict[str, int], data: List[Any]) -> Tuple[Any]:\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "    sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "    is stored in `mask`.\n",
    "                                   \n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "    x: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.long\n",
    "    masks: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.bool\n",
    "    rev_x: same as x but in reversed time. This will be used in our RNN model for masking\n",
    "    rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "    y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\\n\",\n",
    "    using: `sequences, labels = zip(*data)`\\n\",\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = data\n",
    "    # print(f'num samples: {len(samples)}')\n",
    "    # print(f'code_emb_per_patient_collate_function data[0]:\\n{data[0]}')\n",
    "    \n",
    "    y = torch.tensor([s['label'] for s in samples], dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(samples)\n",
    "    num_visits = [patient['num_visits'] for patient in samples]\n",
    "    num_codes = []\n",
    "    for patient_idx, _ in enumerate(num_visits):\n",
    "        num_codes.extend([len(visit) for visit in samples[patient_idx]['conditions']])\n",
    " \n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    # max_num_codes = len(MORTALITY_PER_VISIT_ICD_9_CODE2IDX_)\n",
    "    assert(max_num_codes > 0)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(samples):\n",
    "        kMaxVisits = patient['num_visits']\n",
    "        for j_visit in range(patient['num_visits']):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            codes = patient['conditions'][j_visit] \n",
    "            indices = [code2idx[code] for code in codes]\n",
    "            l = len(codes)\n",
    "            if len(indices) < 1:\n",
    "                print(f'No code indices after lookup')\n",
    "                print(f'codes {codes}'+'\\n'+f'indices{indices}')\n",
    "                print(f'patient\\n{patient}')\n",
    "                assert(len(indices) >= 1)\n",
    "            # for idx in indices:\n",
    "            x[i_patient, j_visit, 0:l] = torch.tensor(indices)\n",
    "            rev_x[i_patient, kMaxVisits-1-j_visit, 0:l] = torch.tensor(indices)\n",
    "            masks[i_patient, j_visit, 0:l] = 1\n",
    "            rev_masks[i_patient, kMaxVisits-1-j_visit, 0:l] = 1\n",
    "        # if i_patient == 0:\n",
    "        #     print(f\"------ code_emb_per_visit_collate_function p: {i_patient} ------\")\n",
    "        #     print(x[i_patient, :, ])\n",
    "        #     print(rev_x[i_patient, :, ])\n",
    "        #     print(masks[i_patient, :, ])\n",
    "        #     print(rev_masks[i_patient, :, ])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9670e0-b22c-4384-b8ad-f0fca7f1a80e",
   "metadata": {},
   "source": [
    "#### CodeEmb Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "id": "cde4ae73-f079-47ae-ae6f-890d0c5fde07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for mortality_pred_task_cemb: 100%|███████████████████████████████████████████████| 1000/1000 [00:00<00:00, 10321.26it/s]\n"
     ]
    }
   ],
   "source": [
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "MORT_CEMB_CODE2IDX = {}\n",
    "MORT_CEMB_CODE_COUNT = {}\n",
    "mortality_cemb_ds = mimic3base.set_task(\n",
    "    task_fn=functools.partial(mortality_pred_task_cemb, MORT_CEMB_CODE_COUNT),\n",
    "    task_name=mortality_pred_task_cemb.__name__)\n",
    "# The set_task(...) function iterates over all samples.\n",
    "# Applying the task to each before returning a new dataset.\n",
    "# Since all samples have been processed we have observed all codes and can\n",
    "# build the code->index LUT.\n",
    "MORT_CEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(set(MORT_CEMB_CODE_COUNT.keys())))\n",
    "}\n",
    "\n",
    "# We need to provide the code->index LUT to the collate function.\n",
    "wrap_code_emb_per_visit_collate_function = functools.partial(\n",
    "    code_emb_per_visit_collate_function,\n",
    "    MORT_CEMB_CODE2IDX)\n",
    "\n",
    "# Split the dataset into train, val, and test.\n",
    "mort_cemb_train_ds, mort_cemb_val_ds, mort_cemb_test_ds = split_by_patient(mortality_cemb_ds, [0.8, 0.1, 0.1])\n",
    "mort_cemb_train_loader = DataLoader(\n",
    "    mort_cemb_train_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n",
    "mort_cemb_val_loader = DataLoader(\n",
    "    mort_cemb_val_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n",
    "mort_cemb_test_loader = DataLoader(\n",
    "    mort_cemb_test_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "320ac8e5-cab2-4540-b072-43a5b7404dd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Verify DataLoader properties.\n",
    "# Quick test without running the whole RNN training process.\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(mort_cemb_train_loader)\n",
    "for _ in loader_iter:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "except StopIteration as e:\n",
    "    print(e)\n",
    "\n",
    "# assert x.dtype == torch.float\n",
    "# assert rev_x.dtype == torch.float\n",
    "# assert y.dtype == torch.float\n",
    "# assert masks.dtype == torch.bool\n",
    "# assert rev_masks.dtype == torch.bool\n",
    "\n",
    "# assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "# assert y.shape == (BATCH_SIZE_, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc740d9c-bb55-4e54-b877-f1354ea793fc",
   "metadata": {},
   "source": [
    "## Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "id": "e9b9414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    print(f'y_pred: {torch.sum(y_pred)}, y_true {torch.sum(y_true)}\\n'\n",
    "          f'y_score\\n{y_score}\\n'\n",
    "          f'y_pred\\n{y_pred}\\n'\n",
    "          f'y_true\\n{y_true}\\n')\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    rcurve = roc_curve(y_true, y_score)\n",
    "    \n",
    "    \n",
    "\n",
    "    # your code here\n",
    "    return p, r, f, roc_auc, rcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "82cae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs, criterion, loss_fn):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, masks, rev_x, rev_masks)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # your code here\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc, rcurve = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e77bd4-a894-45ea-b621-e6da4d0029ee",
   "metadata": {},
   "source": [
    "### Model: CodeEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "d556f942-8df2-4f8b-a89f-98731840ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cemb_sum_embeddings_with_mask(x, masks):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: the embeddings of diagnosis sequence of shape\n",
    "           (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "    \n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    '''\n",
    "    tmp = torch.zeros(x.shape)\n",
    "    m = torch.sum(masks, dim=2) > 0\n",
    "    tmp[m, :, :] = 1\n",
    "    tmp = x * tmp\n",
    "    tmp = torch.sum(tmp, dim=2)\n",
    "    \n",
    "    a = x\n",
    "    a[~masks] = 0\n",
    "    tmp = torch.sum(a, dim=2)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def cemb_get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "          and then use this length vector as index to select the last visit.\n",
    "    \"\"\"\n",
    "    m = torch.sum(masks, dim=2)\n",
    "    m[m > 0] = 1\n",
    "    m = torch.sum(m, dim=1)\n",
    "    m[m > 0] = m[m > 0] - 1\n",
    "    \n",
    "    tmp1 = m\n",
    "    tmp = torch.reshape(m, (-1,1,1))\n",
    "    tmp = tmp.expand(-1, -1, hidden_states.shape[2])\n",
    "    # print(tmp)\n",
    "    # print(tmp.shape)\n",
    "    last_hidden_state = torch.gather(hidden_states, axis=1, index=tmp)\n",
    "    # print(last_hidden_state.shape)\n",
    "    last_hidden_state = torch.squeeze(last_hidden_state)\n",
    "    \n",
    "    # print(last_hidden_state[3, :])\n",
    "    # print(torch.squeeze(hidden_states[3, tmp1[3], :]))\n",
    "    assert(torch.equal(last_hidden_state[0, :], torch.squeeze(hidden_states[0, tmp1[0], :])))\n",
    "    \n",
    "    return last_hidden_state\n",
    "\n",
    "\n",
    "class CembNaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128) \n",
    "        self.rnn = nn.GRU(input_size=128, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(input_size=128, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=256, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        # print(f\"pre embedding: {x.shape}\")\n",
    "        e = self.embedding(x)\n",
    "        # print(f\"post embedding: {e.shape}\")\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        e = cemb_sum_embeddings_with_mask(e, masks)\n",
    "        # print(f\"post sum_embeddings_with_mask: {e.shape}\")\n",
    "        \n",
    "        # 3. Pass the embeddings through the RNN layer;\n",
    "        output, _ = self.rnn(e)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = cemb_get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order (rev_x), and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        rev_e = self.embedding(rev_x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_e = sum_embeddings_with_mask(rev_e, rev_masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, _ = self.rnn(rev_e)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n_rev = get_last_visit(output, rev_masks)\n",
    "        \n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "04566514-f29b-4683-802f-f370867a907d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CembNaiveRNN(\n",
       "  (embedding): Embedding(1490, 128)\n",
       "  (rnn): GRU(128, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(128, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load the model here\n",
    "mort_code_naive_rnn = CembNaiveRNN(num_codes = len(MORT_CEMB_CODE2IDX))\n",
    "mort_code_naive_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938907a3-04c8-40f7-a772-f66d923e0832",
   "metadata": {},
   "source": [
    "### Model: DescEmb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307f782",
   "metadata": {},
   "source": [
    "RNN running and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "ffbe233a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DembNaiveRNN(\n",
       "  (rnn): GRU(768, 128, batch_first=True)\n",
       "  (rev_rnn): GRU(768, 128, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 593,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def demb_sum_embeddings_with_mask(x, masks):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # events(diag+proc+presc), embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # events(diag+proc+presc))\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, embeddings_dim)\n",
    "    '''\n",
    "    # tmp = torch.zeros(x.shape)\n",
    "    # m = torch.sum(masks, dim=1) > 0\n",
    "    # tmp[m, :] = 1\n",
    "    # tmp = x * tmp\n",
    "    # tmp = torch.sum(tmp, dim=1)\n",
    "    return x\n",
    "    \n",
    "    a = x\n",
    "    a[~masks, :] = 0\n",
    "    # tmp = torch.sum(a, dim=1)\n",
    "    tmp = a\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def demb_get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape\n",
    "                       (batch_size, # events(diag+proc+presc), embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # events(diag+proc+presc))\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "          and then use this length vector as index to select the last visit.\n",
    "    \"\"\"\n",
    "    m = torch.sum(masks, dim=1)\n",
    "    # m[m > 0] = 1\n",
    "    # m = torch.sum(m, dim=1)\n",
    "    m[m > 0] = m[m > 0] - 1\n",
    "    # print(f'selecting {m}')\n",
    "    \n",
    "    tmp1 = m\n",
    "    tmp = torch.reshape(m, (-1,1,1))\n",
    "    tmp = tmp.expand(-1, -1, hidden_states.shape[2])\n",
    "    last_hidden_state = torch.gather(hidden_states, axis=1, index=tmp)\n",
    "    last_hidden_state = torch.squeeze(last_hidden_state)\n",
    "    # print(f'tmp.shape {tmp.shape}\\n'\n",
    "    #       f'last_hidden_state {last_hidden_state.shape}\\n{last_hidden_state}')\n",
    "    assert(torch.equal(last_hidden_state[0, :], torch.squeeze(hidden_states[0, tmp1[0], :])))\n",
    "    \n",
    "    return last_hidden_state\n",
    "\n",
    "\n",
    "class DembNaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_emb_size:int=BERT_EMBEDDING_SIZE):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        self.bert_emb_size = bert_emb_size\n",
    "        # self.embedding = nn.Embedding(num_embeddings=self.bert_emb_size, embedding_dim=128) \n",
    "        self.rnn = nn.GRU(input_size=self.bert_emb_size, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(input_size=self.bert_emb_size, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=256,out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, #events(diag+proc+presc), embedding_dim)\n",
    "            masks: the padding masks of shape (batch_size, #events(diag+proc+presc))\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        # e = self.embedding(x)\n",
    "        \n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = demb_sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embeddings through the RNN layer;\n",
    "        output, _ = self.rnn(x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = demb_get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order (rev_x), and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "        # xr = self.embedding(rev_x)\n",
    "        xr = demb_sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        routput, _ = self.rev_rnn(xr)\n",
    "        true_h_n_rev = demb_get_last_visit(routput, rev_masks)\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "mort_desc_naive_rnn = DembNaiveRNN()\n",
    "mort_desc_naive_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeef88e-bc4b-454e-adb5-d65eee2169bf",
   "metadata": {},
   "source": [
    "### Train: CodeEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "id": "479ded72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CodeEmb.\n",
      "Epoch: 1 \t Training Loss: 0.552467\n",
      "Epoch: 1 \t Validation p: 0.00, r:0.00, f: 0.00, roc_auc: 0.60\n",
      "Epoch: 2 \t Training Loss: 0.422516\n",
      "Epoch: 2 \t Validation p: 0.00, r:0.00, f: 0.00, roc_auc: 0.69\n",
      "Epoch: 3 \t Training Loss: 0.291385\n",
      "Epoch: 3 \t Validation p: 0.33, r:0.09, f: 0.14, roc_auc: 0.74\n",
      "Epoch: 4 \t Training Loss: 0.229469\n",
      "Epoch: 4 \t Validation p: 0.67, r:0.18, f: 0.29, roc_auc: 0.77\n",
      "Epoch: 5 \t Training Loss: 0.157095\n",
      "Epoch: 5 \t Validation p: 0.67, r:0.18, f: 0.29, roc_auc: 0.79\n",
      "Epoch: 6 \t Training Loss: 0.110257\n",
      "Epoch: 6 \t Validation p: 0.50, r:0.18, f: 0.27, roc_auc: 0.81\n",
      "Epoch: 7 \t Training Loss: 0.073944\n",
      "Epoch: 7 \t Validation p: 0.50, r:0.18, f: 0.27, roc_auc: 0.83\n",
      "Epoch: 8 \t Training Loss: 0.050900\n",
      "Epoch: 8 \t Validation p: 0.50, r:0.09, f: 0.15, roc_auc: 0.81\n",
      "Epoch: 9 \t Training Loss: 0.036159\n",
      "Epoch: 9 \t Validation p: 0.50, r:0.09, f: 0.15, roc_auc: 0.82\n",
      "Epoch: 10 \t Training Loss: 0.026370\n",
      "Epoch: 10 \t Validation p: 0.50, r:0.09, f: 0.15, roc_auc: 0.83\n"
     ]
    }
   ],
   "source": [
    "print('Training CodeEmb.')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mort_code_naive_rnn.parameters(), lr=0.001)\n",
    "# number of epochs to train the model\n",
    "n_epochs =10 \n",
    "train(mort_code_naive_rnn, mort_cemb_train_loader, mort_cemb_val_loader,\n",
    "      n_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "id": "264fde9e-4bcc-4914-b6f4-6f9a8e8b8e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p, r, f, roc_auc, rcurve = eval_model(mort_code_naive_rnn, mort_cemb_val_loader)\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "id": "2769dec9-05ef-42cd-be38-076c25d5e064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p, r, f, roc_auc, rcurve = eval_model(mort_code_naive_rnn, mort_cemb_test_loader)\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b02354-bdfd-4064-8ffe-7d0dc807c8a1",
   "metadata": {},
   "source": [
    "### Train: DescEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "id": "12d00785-0639-4365-b5dc-b00e4cc3ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mort_desc_naive_rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "11370bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertCollate tensor sizes\n",
      "cum_size:4.54656 MB\n",
      "sizes:[86016, 150528, 178176, 156672, 153600, 52224, 159744, 172032, 162816, 236544, 144384, 202752, 43008, 159744, 199680, 15360, 162816, 273408, 190464, 125952, 30720, 156672, 221184, 36864, 181248, 153600, 156672, 236544, 15360, 55296, 242688, 33792]\n",
      "[(28, 768), (49, 768), (58, 768), (51, 768), (50, 768), (17, 768), (52, 768), (56, 768), (53, 768), (77, 768), (47, 768), (66, 768), (14, 768), (52, 768), (65, 768), (5, 768), (53, 768), (89, 768), (62, 768), (41, 768), (10, 768), (51, 768), (72, 768), (12, 768), (59, 768), (50, 768), (51, 768), (77, 768), (5, 768), (18, 768), (79, 768), (11, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.048896 MB\n",
      "sizes:[242688, 147456, 55296, 181248, 153600, 33792, 24576, 144384, 129024, 215040, 175104, 110592, 55296, 150528, 76800, 43008, 227328, 18432, 175104, 104448, 150528, 159744, 181248, 153600, 153600, 208896, 49152, 224256, 15360, 58368, 175104, 55296]\n",
      "[(79, 768), (48, 768), (18, 768), (59, 768), (50, 768), (11, 768), (8, 768), (47, 768), (42, 768), (70, 768), (57, 768), (36, 768), (18, 768), (49, 768), (25, 768), (14, 768), (74, 768), (6, 768), (57, 768), (34, 768), (49, 768), (52, 768), (59, 768), (50, 768), (50, 768), (68, 768), (16, 768), (73, 768), (5, 768), (19, 768), (57, 768), (18, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.190208 MB\n",
      "sizes:[24576, 211968, 156672, 33792, 153600, 30720, 144384, 181248, 82944, 125952, 58368, 215040, 18432, 98304, 218112, 49152, 347136, 159744, 184320, 165888, 156672, 175104, 267264, 156672, 21504, 328704, 18432, 43008, 165888, 15360, 86016, 95232]\n",
      "[(8, 768), (69, 768), (51, 768), (11, 768), (50, 768), (10, 768), (47, 768), (59, 768), (27, 768), (41, 768), (19, 768), (70, 768), (6, 768), (32, 768), (71, 768), (16, 768), (113, 768), (52, 768), (60, 768), (54, 768), (51, 768), (57, 768), (87, 768), (51, 768), (7, 768), (107, 768), (6, 768), (14, 768), (54, 768), (5, 768), (28, 768), (31, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.795392 MB\n",
      "sizes:[175104, 211968, 190464, 168960, 70656, 233472, 172032, 125952, 178176, 288768, 168960, 165888, 261120, 181248, 15360, 116736, 122880, 168960, 153600, 138240, 55296, 125952, 184320, 39936, 150528, 168960, 24576, 202752, 15360, 156672, 181248, 181248]\n",
      "[(57, 768), (69, 768), (62, 768), (55, 768), (23, 768), (76, 768), (56, 768), (41, 768), (58, 768), (94, 768), (55, 768), (54, 768), (85, 768), (59, 768), (5, 768), (38, 768), (40, 768), (55, 768), (50, 768), (45, 768), (18, 768), (41, 768), (60, 768), (13, 768), (49, 768), (55, 768), (8, 768), (66, 768), (5, 768), (51, 768), (59, 768), (59, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.54656 MB\n",
      "sizes:[187392, 39936, 95232, 52224, 236544, 33792, 30720, 46080, 67584, 141312, 58368, 150528, 196608, 46080, 236544, 239616, 288768, 184320, 159744, 159744, 162816, 61440, 39936, 156672, 159744, 165888, 362496, 150528, 27648, 264192, 172032, 172032]\n",
      "[(61, 768), (13, 768), (31, 768), (17, 768), (77, 768), (11, 768), (10, 768), (15, 768), (22, 768), (46, 768), (19, 768), (49, 768), (64, 768), (15, 768), (77, 768), (78, 768), (94, 768), (60, 768), (52, 768), (52, 768), (53, 768), (20, 768), (13, 768), (51, 768), (52, 768), (54, 768), (118, 768), (49, 768), (9, 768), (86, 768), (56, 768), (56, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.144128 MB\n",
      "sizes:[178176, 125952, 36864, 175104, 21504, 181248, 64512, 190464, 218112, 211968, 129024, 279552, 181248, 70656, 101376, 55296, 21504, 215040, 162816, 193536, 144384, 162816, 150528, 36864, 21504, 21504, 92160, 168960, 39936, 254976, 39936, 196608]\n",
      "[(58, 768), (41, 768), (12, 768), (57, 768), (7, 768), (59, 768), (21, 768), (62, 768), (71, 768), (69, 768), (42, 768), (91, 768), (59, 768), (23, 768), (33, 768), (18, 768), (7, 768), (70, 768), (53, 768), (63, 768), (47, 768), (53, 768), (49, 768), (12, 768), (7, 768), (7, 768), (30, 768), (55, 768), (13, 768), (83, 768), (13, 768), (64, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.303872 MB\n",
      "sizes:[15360, 156672, 132096, 190464, 187392, 162816, 144384, 368640, 150528, 196608, 61440, 199680, 36864, 230400, 43008, 39936, 153600, 18432, 138240, 79872, 175104, 175104, 184320, 172032, 138240, 36864, 181248, 288768, 39936, 30720, 156672, 18432]\n",
      "[(5, 768), (51, 768), (43, 768), (62, 768), (61, 768), (53, 768), (47, 768), (120, 768), (49, 768), (64, 768), (20, 768), (65, 768), (12, 768), (75, 768), (14, 768), (13, 768), (50, 768), (6, 768), (45, 768), (26, 768), (57, 768), (57, 768), (60, 768), (56, 768), (45, 768), (12, 768), (59, 768), (94, 768), (13, 768), (10, 768), (51, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.076544 MB\n",
      "sizes:[24576, 27648, 153600, 92160, 165888, 21504, 156672, 211968, 55296, 107520, 159744, 33792, 64512, 52224, 175104, 276480, 153600, 224256, 202752, 55296, 236544, 304128, 27648, 199680, 181248, 15360, 165888, 125952, 55296, 147456, 24576, 178176]\n",
      "[(8, 768), (9, 768), (50, 768), (30, 768), (54, 768), (7, 768), (51, 768), (69, 768), (18, 768), (35, 768), (52, 768), (11, 768), (21, 768), (17, 768), (57, 768), (90, 768), (50, 768), (73, 768), (66, 768), (18, 768), (77, 768), (99, 768), (9, 768), (65, 768), (59, 768), (5, 768), (54, 768), (41, 768), (18, 768), (48, 768), (8, 768), (58, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.374528 MB\n",
      "sizes:[294912, 153600, 172032, 156672, 30720, 172032, 144384, 175104, 144384, 294912, 159744, 98304, 144384, 79872, 156672, 64512, 153600, 282624, 138240, 15360, 138240, 27648, 27648, 184320, 135168, 181248, 153600, 110592, 18432, 132096, 215040, 18432]\n",
      "[(96, 768), (50, 768), (56, 768), (51, 768), (10, 768), (56, 768), (47, 768), (57, 768), (47, 768), (96, 768), (52, 768), (32, 768), (47, 768), (26, 768), (51, 768), (21, 768), (50, 768), (92, 768), (45, 768), (5, 768), (45, 768), (9, 768), (9, 768), (60, 768), (44, 768), (59, 768), (50, 768), (36, 768), (6, 768), (43, 768), (70, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.606528 MB\n",
      "sizes:[43008, 153600, 21504, 110592, 39936, 30720, 175104, 175104, 150528, 276480, 156672, 153600, 64512, 190464, 159744, 18432, 159744, 156672, 30720, 175104, 24576, 30720, 159744, 147456, 175104, 24576, 33792, 67584, 27648, 301056, 36864, 135168]\n",
      "[(14, 768), (50, 768), (7, 768), (36, 768), (13, 768), (10, 768), (57, 768), (57, 768), (49, 768), (90, 768), (51, 768), (50, 768), (21, 768), (62, 768), (52, 768), (6, 768), (52, 768), (51, 768), (10, 768), (57, 768), (8, 768), (10, 768), (52, 768), (48, 768), (57, 768), (8, 768), (11, 768), (22, 768), (9, 768), (98, 768), (12, 768), (44, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.703232 MB\n",
      "sizes:[39936, 27648, 92160, 18432, 18432, 205824, 153600, 156672, 187392, 135168, 202752, 43008, 89088, 215040, 156672, 310272, 325632, 273408, 27648, 18432, 89088, 135168, 270336, 218112, 215040, 156672, 162816, 208896, 113664, 104448, 138240, 193536]\n",
      "[(13, 768), (9, 768), (30, 768), (6, 768), (6, 768), (67, 768), (50, 768), (51, 768), (61, 768), (44, 768), (66, 768), (14, 768), (29, 768), (70, 768), (51, 768), (101, 768), (106, 768), (89, 768), (9, 768), (6, 768), (29, 768), (44, 768), (88, 768), (71, 768), (70, 768), (51, 768), (53, 768), (68, 768), (37, 768), (34, 768), (45, 768), (63, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.17792 MB\n",
      "sizes:[172032, 122880, 21504, 190464, 18432, 153600, 153600, 64512, 172032, 39936, 175104, 147456, 107520, 129024, 82944, 138240, 122880, 202752, 184320, 110592, 337920, 18432, 36864, 135168, 368640, 187392, 33792, 30720, 21504, 193536, 58368, 245760]\n",
      "[(56, 768), (40, 768), (7, 768), (62, 768), (6, 768), (50, 768), (50, 768), (21, 768), (56, 768), (13, 768), (57, 768), (48, 768), (35, 768), (42, 768), (27, 768), (45, 768), (40, 768), (66, 768), (60, 768), (36, 768), (110, 768), (6, 768), (12, 768), (44, 768), (120, 768), (61, 768), (11, 768), (10, 768), (7, 768), (63, 768), (19, 768), (80, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.328448 MB\n",
      "sizes:[36864, 18432, 27648, 95232, 110592, 184320, 162816, 18432, 153600, 337920, 168960, 264192, 181248, 239616, 193536, 159744, 24576, 147456, 165888, 27648, 193536, 36864, 144384, 33792, 141312, 64512, 178176, 159744, 159744, 15360, 236544, 245760]\n",
      "[(12, 768), (6, 768), (9, 768), (31, 768), (36, 768), (60, 768), (53, 768), (6, 768), (50, 768), (110, 768), (55, 768), (86, 768), (59, 768), (78, 768), (63, 768), (52, 768), (8, 768), (48, 768), (54, 768), (9, 768), (63, 768), (12, 768), (47, 768), (11, 768), (46, 768), (21, 768), (58, 768), (52, 768), (52, 768), (5, 768), (77, 768), (80, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.245504 MB\n",
      "sizes:[251904, 288768, 175104, 92160, 18432, 168960, 61440, 58368, 52224, 165888, 239616, 110592, 33792, 141312, 316416, 156672, 162816, 18432, 30720, 258048, 181248, 190464, 49152, 150528, 43008, 24576, 46080, 187392, 113664, 150528, 141312, 165888]\n",
      "[(82, 768), (94, 768), (57, 768), (30, 768), (6, 768), (55, 768), (20, 768), (19, 768), (17, 768), (54, 768), (78, 768), (36, 768), (11, 768), (46, 768), (103, 768), (51, 768), (53, 768), (6, 768), (10, 768), (84, 768), (59, 768), (62, 768), (16, 768), (49, 768), (14, 768), (8, 768), (15, 768), (61, 768), (37, 768), (49, 768), (46, 768), (54, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.122624 MB\n",
      "sizes:[202752, 147456, 181248, 172032, 24576, 193536, 55296, 43008, 73728, 162816, 168960, 175104, 178176, 215040, 205824, 162816, 150528, 178176, 190464, 196608, 208896, 18432, 24576, 27648, 156672, 18432, 58368, 55296, 58368, 113664, 122880, 181248]\n",
      "[(66, 768), (48, 768), (59, 768), (56, 768), (8, 768), (63, 768), (18, 768), (14, 768), (24, 768), (53, 768), (55, 768), (57, 768), (58, 768), (70, 768), (67, 768), (53, 768), (49, 768), (58, 768), (62, 768), (64, 768), (68, 768), (6, 768), (8, 768), (9, 768), (51, 768), (6, 768), (19, 768), (18, 768), (19, 768), (37, 768), (40, 768), (59, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.1472 MB\n",
      "sizes:[43008, 138240, 316416, 36864, 159744, 15360, 153600, 153600, 46080, 150528, 36864, 178176, 18432, 181248, 165888, 153600, 156672, 30720, 159744, 141312, 98304, 147456, 122880, 172032, 224256, 30720, 245760, 208896, 18432, 230400, 52224, 159744]\n",
      "[(14, 768), (45, 768), (103, 768), (12, 768), (52, 768), (5, 768), (50, 768), (50, 768), (15, 768), (49, 768), (12, 768), (58, 768), (6, 768), (59, 768), (54, 768), (50, 768), (51, 768), (10, 768), (52, 768), (46, 768), (32, 768), (48, 768), (40, 768), (56, 768), (73, 768), (10, 768), (80, 768), (68, 768), (6, 768), (75, 768), (17, 768), (52, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.534272 MB\n",
      "sizes:[202752, 172032, 353280, 27648, 168960, 135168, 205824, 297984, 205824, 168960, 187392, 27648, 175104, 24576, 276480, 168960, 36864, 64512, 113664, 162816, 30720, 184320, 21504, 86016, 199680, 30720, 138240, 18432, 162816, 39936, 279552, 165888]\n",
      "[(66, 768), (56, 768), (115, 768), (9, 768), (55, 768), (44, 768), (67, 768), (97, 768), (67, 768), (55, 768), (61, 768), (9, 768), (57, 768), (8, 768), (90, 768), (55, 768), (12, 768), (21, 768), (37, 768), (53, 768), (10, 768), (60, 768), (7, 768), (28, 768), (65, 768), (10, 768), (45, 768), (6, 768), (53, 768), (13, 768), (91, 768), (54, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.62496 MB\n",
      "sizes:[46080, 104448, 162816, 76800, 208896, 55296, 172032, 162816, 147456, 18432, 76800, 24576, 15360, 64512, 181248, 187392, 39936, 301056, 224256, 215040, 150528, 21504, 33792, 193536, 21504, 79872, 24576, 261120, 21504, 18432, 147456, 165888]\n",
      "[(15, 768), (34, 768), (53, 768), (25, 768), (68, 768), (18, 768), (56, 768), (53, 768), (48, 768), (6, 768), (25, 768), (8, 768), (5, 768), (21, 768), (59, 768), (61, 768), (13, 768), (98, 768), (73, 768), (70, 768), (49, 768), (7, 768), (11, 768), (63, 768), (7, 768), (26, 768), (8, 768), (85, 768), (7, 768), (6, 768), (48, 768), (54, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.859904 MB\n",
      "sizes:[215040, 178176, 175104, 175104, 73728, 168960, 36864, 193536, 172032, 168960, 138240, 33792, 187392, 301056, 162816, 30720, 221184, 24576, 147456, 39936, 162816, 168960, 245760, 267264, 162816, 218112, 33792, 159744, 70656, 141312, 156672, 227328]\n",
      "[(70, 768), (58, 768), (57, 768), (57, 768), (24, 768), (55, 768), (12, 768), (63, 768), (56, 768), (55, 768), (45, 768), (11, 768), (61, 768), (98, 768), (53, 768), (10, 768), (72, 768), (8, 768), (48, 768), (13, 768), (53, 768), (55, 768), (80, 768), (87, 768), (53, 768), (71, 768), (11, 768), (52, 768), (23, 768), (46, 768), (51, 768), (74, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.707904 MB\n",
      "sizes:[172032, 30720, 172032, 162816, 172032, 138240, 248832, 46080, 236544, 86016, 119808, 15360, 159744, 73728, 61440, 153600, 193536, 178176, 24576, 172032, 18432, 221184, 21504, 156672, 178176, 67584, 21504, 33792, 18432, 95232, 55296, 202752]\n",
      "[(56, 768), (10, 768), (56, 768), (53, 768), (56, 768), (45, 768), (81, 768), (15, 768), (77, 768), (28, 768), (39, 768), (5, 768), (52, 768), (24, 768), (20, 768), (50, 768), (63, 768), (58, 768), (8, 768), (56, 768), (6, 768), (72, 768), (7, 768), (51, 768), (58, 768), (22, 768), (7, 768), (11, 768), (6, 768), (31, 768), (18, 768), (66, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.643392 MB\n",
      "sizes:[30720, 95232, 46080, 55296, 70656, 21504, 162816, 261120, 79872, 162816, 18432, 27648, 162816, 285696, 24576, 168960, 236544, 261120, 36864, 135168, 43008, 168960, 175104, 43008, 150528, 168960, 33792, 116736, 168960, 156672, 30720, 43008]\n",
      "[(10, 768), (31, 768), (15, 768), (18, 768), (23, 768), (7, 768), (53, 768), (85, 768), (26, 768), (53, 768), (6, 768), (9, 768), (53, 768), (93, 768), (8, 768), (55, 768), (77, 768), (85, 768), (12, 768), (44, 768), (14, 768), (55, 768), (57, 768), (14, 768), (49, 768), (55, 768), (11, 768), (38, 768), (55, 768), (51, 768), (10, 768), (14, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.19328 MB\n",
      "sizes:[33792, 15360, 181248, 113664, 184320, 172032, 113664, 156672, 147456, 135168, 30720, 233472, 175104, 86016, 150528, 150528, 264192, 39936, 64512, 156672, 301056, 153600, 43008, 187392, 159744, 159744, 141312, 43008, 58368, 150528, 172032, 18432]\n",
      "[(11, 768), (5, 768), (59, 768), (37, 768), (60, 768), (56, 768), (37, 768), (51, 768), (48, 768), (44, 768), (10, 768), (76, 768), (57, 768), (28, 768), (49, 768), (49, 768), (86, 768), (13, 768), (21, 768), (51, 768), (98, 768), (50, 768), (14, 768), (61, 768), (52, 768), (52, 768), (46, 768), (14, 768), (19, 768), (49, 768), (56, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:1.514496 MB\n",
      "sizes:[82944, 33792, 73728, 178176, 165888, 193536, 245760, 245760, 150528, 144384]\n",
      "[(27, 768), (11, 768), (24, 768), (58, 768), (54, 768), (63, 768), (80, 768), (80, 768), (49, 768), (47, 768)]\n",
      "\n",
      "Epoch: 1 \t Training Loss: 0.477983\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.002816 MB\n",
      "sizes:[52224, 101376, 24576, 18432, 52224, 159744, 101376, 242688, 135168, 110592, 46080, 181248, 215040, 162816, 24576, 104448, 178176, 184320, 39936, 153600, 288768, 18432, 184320, 24576, 27648, 242688, 187392, 135168, 193536, 92160, 156672, 162816]\n",
      "[(17, 768), (33, 768), (8, 768), (6, 768), (17, 768), (52, 768), (33, 768), (79, 768), (44, 768), (36, 768), (15, 768), (59, 768), (70, 768), (53, 768), (8, 768), (34, 768), (58, 768), (60, 768), (13, 768), (50, 768), (94, 768), (6, 768), (60, 768), (8, 768), (9, 768), (79, 768), (61, 768), (44, 768), (63, 768), (30, 768), (51, 768), (53, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.926016 MB\n",
      "sizes:[141312, 172032, 150528, 49152, 181248, 159744, 21504, 153600, 30720, 33792, 178176, 172032, 36864, 227328, 135168, 304128, 15360, 178176, 70656, 282624, 175104, 18432, 43008, 178176, 24576, 15360, 175104, 15360, 156672, 147456, 92160, 190464]\n",
      "[(46, 768), (56, 768), (49, 768), (16, 768), (59, 768), (52, 768), (7, 768), (50, 768), (10, 768), (11, 768), (58, 768), (56, 768), (12, 768), (74, 768), (44, 768), (99, 768), (5, 768), (58, 768), (23, 768), (92, 768), (57, 768), (6, 768), (14, 768), (58, 768), (8, 768), (5, 768), (57, 768), (5, 768), (51, 768), (48, 768), (30, 768), (62, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.268608 MB\n",
      "sizes:[162816, 254976, 144384, 27648, 30720, 113664, 156672, 46080, 15360, 141312, 18432, 33792, 236544, 104448, 135168, 165888, 193536, 144384, 331776, 67584, 251904, 165888, 24576, 233472, 67584]\n",
      "[(53, 768), (83, 768), (47, 768), (9, 768), (10, 768), (37, 768), (51, 768), (15, 768), (5, 768), (46, 768), (6, 768), (11, 768), (77, 768), (34, 768), (44, 768), (54, 768), (63, 768), (47, 768), (108, 768), (22, 768), (82, 768), (54, 768), (8, 768), (76, 768), (22, 768)]\n",
      "\n",
      "y_pred: 0, y_true 7.0\n",
      "y_score\n",
      "tensor([0.1357, 0.1677, 0.0585, 0.1192, 0.1425, 0.1687, 0.1476, 0.1564, 0.1653,\n",
      "        0.1365, 0.1397, 0.1762, 0.1419, 0.1668, 0.1255, 0.1453, 0.1544, 0.1326,\n",
      "        0.1365, 0.1543, 0.1698, 0.1192, 0.1578, 0.1255, 0.1282, 0.1588, 0.1779,\n",
      "        0.1199, 0.1443, 0.0701, 0.1922, 0.1592, 0.1651, 0.1655, 0.1457, 0.1411,\n",
      "        0.1702, 0.1657, 0.1225, 0.1719, 0.0891, 0.0898, 0.1662, 0.1598, 0.1085,\n",
      "        0.1961, 0.1633, 0.1759, 0.1154, 0.0666, 0.1185, 0.1689, 0.1588, 0.0578,\n",
      "        0.1307, 0.1257, 0.1125, 0.1154, 0.1451, 0.1154, 0.1452, 0.1425, 0.1403,\n",
      "        0.1314, 0.1521, 0.1803, 0.1791, 0.1282, 0.1306, 0.1550, 0.1476, 0.1397,\n",
      "        0.1154, 0.1607, 0.1192, 0.1327, 0.1663, 0.1559, 0.1673, 0.1777, 0.1731,\n",
      "        0.1292, 0.1829, 0.1217, 0.1583, 0.1475, 0.1255, 0.1666, 0.1478])\n",
      "y_pred\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "y_true\n",
      "tensor([0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "Epoch: 1 \t Validation p: 0.00, r:0.00, f: 0.00, roc_auc: 0.78\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.199424 MB\n",
      "sizes:[24576, 144384, 125952, 113664, 211968, 162816, 153600, 301056, 24576, 46080, 18432, 294912, 215040, 144384, 172032, 153600, 153600, 46080, 187392, 39936, 199680, 27648, 190464, 159744, 27648, 168960, 33792, 21504, 156672, 175104, 89088, 215040]\n",
      "[(8, 768), (47, 768), (41, 768), (37, 768), (69, 768), (53, 768), (50, 768), (98, 768), (8, 768), (15, 768), (6, 768), (96, 768), (70, 768), (47, 768), (56, 768), (50, 768), (50, 768), (15, 768), (61, 768), (13, 768), (65, 768), (9, 768), (62, 768), (52, 768), (9, 768), (55, 768), (11, 768), (7, 768), (51, 768), (57, 768), (29, 768), (70, 768)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abot/sw/anaconda3/envs/cs598_dlh/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertCollate tensor sizes\n",
      "cum_size:3.830784 MB\n",
      "sizes:[76800, 236544, 288768, 86016, 18432, 175104, 347136, 92160, 141312, 58368, 21504, 153600, 178176, 21504, 15360, 125952, 156672, 15360, 15360, 24576, 205824, 21504, 202752, 36864, 168960, 156672, 98304, 64512, 218112, 190464, 193536, 24576]\n",
      "[(25, 768), (77, 768), (94, 768), (28, 768), (6, 768), (57, 768), (113, 768), (30, 768), (46, 768), (19, 768), (7, 768), (50, 768), (58, 768), (7, 768), (5, 768), (41, 768), (51, 768), (5, 768), (5, 768), (8, 768), (67, 768), (7, 768), (66, 768), (12, 768), (55, 768), (51, 768), (32, 768), (21, 768), (71, 768), (62, 768), (63, 768), (8, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.435968 MB\n",
      "sizes:[187392, 147456, 30720, 153600, 156672, 190464, 233472, 150528, 116736, 202752, 67584, 254976, 282624, 18432, 18432, 21504, 33792, 193536, 129024, 110592, 138240, 82944, 110592, 39936, 215040, 122880, 172032, 239616, 150528, 43008, 261120, 159744]\n",
      "[(61, 768), (48, 768), (10, 768), (50, 768), (51, 768), (62, 768), (76, 768), (49, 768), (38, 768), (66, 768), (22, 768), (83, 768), (92, 768), (6, 768), (6, 768), (7, 768), (11, 768), (63, 768), (42, 768), (36, 768), (45, 768), (27, 768), (36, 768), (13, 768), (70, 768), (40, 768), (56, 768), (78, 768), (49, 768), (14, 768), (85, 768), (52, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.294656 MB\n",
      "sizes:[264192, 15360, 92160, 79872, 162816, 125952, 61440, 159744, 30720, 156672, 168960, 79872, 224256, 55296, 168960, 245760, 236544, 159744, 156672, 36864, 144384, 239616, 196608, 24576, 175104, 150528, 248832, 33792, 138240, 202752, 30720, 27648]\n",
      "[(86, 768), (5, 768), (30, 768), (26, 768), (53, 768), (41, 768), (20, 768), (52, 768), (10, 768), (51, 768), (55, 768), (26, 768), (73, 768), (18, 768), (55, 768), (80, 768), (77, 768), (52, 768), (51, 768), (12, 768), (47, 768), (78, 768), (64, 768), (8, 768), (57, 768), (49, 768), (81, 768), (11, 768), (45, 768), (66, 768), (10, 768), (9, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.156416 MB\n",
      "sizes:[55296, 150528, 135168, 236544, 187392, 150528, 64512, 55296, 236544, 184320, 30720, 113664, 21504, 43008, 153600, 168960, 153600, 36864, 202752, 205824, 55296, 168960, 30720, 211968, 27648, 159744, 30720, 181248, 39936, 187392, 150528, 325632]\n",
      "[(18, 768), (49, 768), (44, 768), (77, 768), (61, 768), (49, 768), (21, 768), (18, 768), (77, 768), (60, 768), (10, 768), (37, 768), (7, 768), (14, 768), (50, 768), (55, 768), (50, 768), (12, 768), (66, 768), (67, 768), (18, 768), (55, 768), (10, 768), (69, 768), (9, 768), (52, 768), (10, 768), (59, 768), (13, 768), (61, 768), (49, 768), (106, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.051968 MB\n",
      "sizes:[18432, 30720, 52224, 24576, 138240, 181248, 175104, 113664, 24576, 141312, 362496, 184320, 30720, 125952, 187392, 129024, 162816, 202752, 122880, 193536, 144384, 190464, 147456, 39936, 92160, 132096, 67584, 76800, 301056, 43008, 52224, 162816]\n",
      "[(6, 768), (10, 768), (17, 768), (8, 768), (45, 768), (59, 768), (57, 768), (37, 768), (8, 768), (46, 768), (118, 768), (60, 768), (10, 768), (41, 768), (61, 768), (42, 768), (53, 768), (66, 768), (40, 768), (63, 768), (47, 768), (62, 768), (48, 768), (13, 768), (30, 768), (43, 768), (22, 768), (25, 768), (98, 768), (14, 768), (17, 768), (53, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.744768 MB\n",
      "sizes:[18432, 215040, 221184, 122880, 18432, 156672, 175104, 64512, 251904, 33792, 264192, 30720, 138240, 30720, 147456, 15360, 18432, 193536, 162816, 18432, 43008, 95232, 30720, 337920, 184320, 172032, 168960, 43008, 15360, 61440, 181248, 113664]\n",
      "[(6, 768), (70, 768), (72, 768), (40, 768), (6, 768), (51, 768), (57, 768), (21, 768), (82, 768), (11, 768), (86, 768), (10, 768), (45, 768), (10, 768), (48, 768), (5, 768), (6, 768), (63, 768), (53, 768), (6, 768), (14, 768), (31, 768), (10, 768), (110, 768), (60, 768), (56, 768), (55, 768), (14, 768), (5, 768), (20, 768), (59, 768), (37, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.412992 MB\n",
      "sizes:[159744, 132096, 162816, 104448, 153600, 159744, 15360, 15360, 159744, 125952, 73728, 27648, 147456, 199680, 193536, 172032, 49152, 156672, 18432, 138240, 107520, 221184, 172032, 165888, 27648, 52224, 18432, 55296, 36864, 153600, 18432, 18432]\n",
      "[(52, 768), (43, 768), (53, 768), (34, 768), (50, 768), (52, 768), (5, 768), (5, 768), (52, 768), (41, 768), (24, 768), (9, 768), (48, 768), (65, 768), (63, 768), (56, 768), (16, 768), (51, 768), (6, 768), (45, 768), (35, 768), (72, 768), (56, 768), (54, 768), (9, 768), (17, 768), (6, 768), (18, 768), (12, 768), (50, 768), (6, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.196352 MB\n",
      "sizes:[150528, 156672, 245760, 36864, 70656, 156672, 239616, 150528, 39936, 156672, 181248, 36864, 162816, 162816, 144384, 55296, 159744, 36864, 43008, 30720, 175104, 245760, 316416, 276480, 33792, 30720, 181248, 175104, 43008, 116736, 165888, 18432]\n",
      "[(49, 768), (51, 768), (80, 768), (12, 768), (23, 768), (51, 768), (78, 768), (49, 768), (13, 768), (51, 768), (59, 768), (12, 768), (53, 768), (53, 768), (47, 768), (18, 768), (52, 768), (12, 768), (14, 768), (10, 768), (57, 768), (80, 768), (103, 768), (90, 768), (11, 768), (10, 768), (59, 768), (57, 768), (14, 768), (38, 768), (54, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.740096 MB\n",
      "sizes:[49152, 159744, 202752, 208896, 168960, 101376, 168960, 162816, 24576, 276480, 18432, 297984, 30720, 368640, 172032, 64512, 27648, 104448, 227328, 33792, 304128, 141312, 153600, 150528, 273408, 43008, 181248, 162816, 138240, 138240, 150528, 33792]\n",
      "[(16, 768), (52, 768), (66, 768), (68, 768), (55, 768), (33, 768), (55, 768), (53, 768), (8, 768), (90, 768), (6, 768), (97, 768), (10, 768), (120, 768), (56, 768), (21, 768), (9, 768), (34, 768), (74, 768), (11, 768), (99, 768), (46, 768), (50, 768), (49, 768), (89, 768), (14, 768), (59, 768), (53, 768), (45, 768), (45, 768), (49, 768), (11, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.371456 MB\n",
      "sizes:[328704, 135168, 193536, 153600, 156672, 165888, 61440, 178176, 64512, 36864, 168960, 221184, 178176, 18432, 55296, 36864, 181248, 175104, 162816, 135168, 245760, 162816, 18432, 73728, 181248, 196608, 215040, 39936, 33792, 98304, 144384, 153600]\n",
      "[(107, 768), (44, 768), (63, 768), (50, 768), (51, 768), (54, 768), (20, 768), (58, 768), (21, 768), (12, 768), (55, 768), (72, 768), (58, 768), (6, 768), (18, 768), (12, 768), (59, 768), (57, 768), (53, 768), (44, 768), (80, 768), (53, 768), (6, 768), (24, 768), (59, 768), (64, 768), (70, 768), (13, 768), (11, 768), (32, 768), (47, 768), (50, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.13184 MB\n",
      "sizes:[245760, 21504, 168960, 208896, 43008, 184320, 178176, 196608, 147456, 150528, 27648, 110592, 21504, 168960, 218112, 67584, 135168, 82944, 190464, 150528, 156672, 178176, 46080, 141312, 224256, 95232, 36864, 156672, 92160, 52224, 208896, 24576]\n",
      "[(80, 768), (7, 768), (55, 768), (68, 768), (14, 768), (60, 768), (58, 768), (64, 768), (48, 768), (49, 768), (9, 768), (36, 768), (7, 768), (55, 768), (71, 768), (22, 768), (44, 768), (27, 768), (62, 768), (49, 768), (51, 768), (58, 768), (15, 768), (46, 768), (73, 768), (31, 768), (12, 768), (51, 768), (30, 768), (17, 768), (68, 768), (8, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.866048 MB\n",
      "sizes:[172032, 245760, 168960, 208896, 55296, 15360, 215040, 24576, 135168, 175104, 36864, 58368, 368640, 181248, 95232, 178176, 172032, 156672, 310272, 64512, 199680, 33792, 58368, 181248, 172032, 159744, 193536, 316416, 218112, 110592, 33792, 150528]\n",
      "[(56, 768), (80, 768), (55, 768), (68, 768), (18, 768), (5, 768), (70, 768), (8, 768), (44, 768), (57, 768), (12, 768), (19, 768), (120, 768), (59, 768), (31, 768), (58, 768), (56, 768), (51, 768), (101, 768), (21, 768), (65, 768), (11, 768), (19, 768), (59, 768), (56, 768), (52, 768), (63, 768), (103, 768), (71, 768), (36, 768), (11, 768), (49, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.385344 MB\n",
      "sizes:[162816, 172032, 208896, 113664, 285696, 39936, 30720, 55296, 156672, 178176, 64512, 58368, 21504, 43008, 15360, 184320, 61440, 39936, 21504, 187392, 178176, 73728, 24576, 156672, 49152, 156672, 18432, 159744, 52224, 181248, 58368, 175104]\n",
      "[(53, 768), (56, 768), (68, 768), (37, 768), (93, 768), (13, 768), (10, 768), (18, 768), (51, 768), (58, 768), (21, 768), (19, 768), (7, 768), (14, 768), (5, 768), (60, 768), (20, 768), (13, 768), (7, 768), (61, 768), (58, 768), (24, 768), (8, 768), (51, 768), (16, 768), (51, 768), (6, 768), (52, 768), (17, 768), (59, 768), (19, 768), (57, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.242432 MB\n",
      "sizes:[27648, 107520, 95232, 153600, 135168, 36864, 18432, 70656, 337920, 156672, 162816, 181248, 18432, 76800, 270336, 261120, 159744, 165888, 147456, 236544, 64512, 242688, 172032, 104448, 153600, 113664, 162816, 18432, 79872, 98304, 165888, 46080]\n",
      "[(9, 768), (35, 768), (31, 768), (50, 768), (44, 768), (12, 768), (6, 768), (23, 768), (110, 768), (51, 768), (53, 768), (59, 768), (6, 768), (25, 768), (88, 768), (85, 768), (52, 768), (54, 768), (48, 768), (77, 768), (21, 768), (79, 768), (56, 768), (34, 768), (50, 768), (37, 768), (53, 768), (6, 768), (26, 768), (32, 768), (54, 768), (15, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:5.741568 MB\n",
      "sizes:[147456, 193536, 156672, 264192, 276480, 55296, 205824, 58368, 224256, 190464, 267264, 288768, 156672, 172032, 279552, 165888, 21504, 199680, 156672, 211968, 122880, 175104, 165888, 273408, 184320, 181248, 184320, 258048, 159744, 18432, 175104, 150528]\n",
      "[(48, 768), (63, 768), (51, 768), (86, 768), (90, 768), (18, 768), (67, 768), (19, 768), (73, 768), (62, 768), (87, 768), (94, 768), (51, 768), (56, 768), (91, 768), (54, 768), (7, 768), (65, 768), (51, 768), (69, 768), (40, 768), (57, 768), (54, 768), (89, 768), (60, 768), (59, 768), (60, 768), (84, 768), (52, 768), (6, 768), (57, 768), (49, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.245504 MB\n",
      "sizes:[181248, 58368, 36864, 178176, 242688, 64512, 150528, 15360, 21504, 288768, 27648, 150528, 24576, 184320, 165888, 193536, 165888, 119808, 150528, 150528, 46080, 153600, 110592, 39936, 150528, 178176, 187392, 190464, 153600, 205824, 46080, 211968]\n",
      "[(59, 768), (19, 768), (12, 768), (58, 768), (79, 768), (21, 768), (49, 768), (5, 768), (7, 768), (94, 768), (9, 768), (49, 768), (8, 768), (60, 768), (54, 768), (63, 768), (54, 768), (39, 768), (49, 768), (49, 768), (15, 768), (50, 768), (36, 768), (13, 768), (49, 768), (58, 768), (61, 768), (62, 768), (50, 768), (67, 768), (15, 768), (69, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.02432 MB\n",
      "sizes:[27648, 172032, 86016, 138240, 156672, 86016, 175104, 181248, 55296, 138240, 30720, 162816, 181248, 36864, 33792, 233472, 43008, 39936, 172032, 181248, 215040, 135168, 175104, 168960, 153600, 33792, 162816, 175104, 175104, 30720, 141312, 125952]\n",
      "[(9, 768), (56, 768), (28, 768), (45, 768), (51, 768), (28, 768), (57, 768), (59, 768), (18, 768), (45, 768), (10, 768), (53, 768), (59, 768), (12, 768), (11, 768), (76, 768), (14, 768), (13, 768), (56, 768), (59, 768), (70, 768), (44, 768), (57, 768), (55, 768), (50, 768), (11, 768), (53, 768), (57, 768), (57, 768), (10, 768), (46, 768), (41, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.156416 MB\n",
      "sizes:[144384, 24576, 147456, 261120, 15360, 39936, 175104, 267264, 33792, 43008, 181248, 215040, 178176, 159744, 18432, 18432, 39936, 153600, 175104, 172032, 159744, 21504, 168960, 301056, 193536, 89088, 129024, 159744, 138240, 82944, 33792, 215040]\n",
      "[(47, 768), (8, 768), (48, 768), (85, 768), (5, 768), (13, 768), (57, 768), (87, 768), (11, 768), (14, 768), (59, 768), (70, 768), (58, 768), (52, 768), (6, 768), (6, 768), (13, 768), (50, 768), (57, 768), (56, 768), (52, 768), (7, 768), (55, 768), (98, 768), (63, 768), (29, 768), (42, 768), (52, 768), (45, 768), (27, 768), (11, 768), (70, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.537344 MB\n",
      "sizes:[162816, 218112, 24576, 159744, 153600, 159744, 15360, 39936, 227328, 168960, 30720, 153600, 184320, 27648, 43008, 165888, 294912, 175104, 196608, 153600, 147456, 165888, 138240, 230400, 159744, 162816, 58368, 147456, 33792, 162816, 353280, 21504]\n",
      "[(53, 768), (71, 768), (8, 768), (52, 768), (50, 768), (52, 768), (5, 768), (13, 768), (74, 768), (55, 768), (10, 768), (50, 768), (60, 768), (9, 768), (14, 768), (54, 768), (96, 768), (57, 768), (64, 768), (50, 768), (48, 768), (54, 768), (45, 768), (75, 768), (52, 768), (53, 768), (19, 768), (48, 768), (11, 768), (53, 768), (115, 768), (7, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.40832 MB\n",
      "sizes:[144384, 43008, 55296, 175104, 162816, 18432, 55296, 30720, 202752, 55296, 156672, 110592, 187392, 230400, 279552, 95232, 288768, 172032, 147456, 153600, 24576, 15360, 153600, 172032, 144384, 156672, 122880, 175104, 175104, 168960, 144384, 190464]\n",
      "[(47, 768), (14, 768), (18, 768), (57, 768), (53, 768), (6, 768), (18, 768), (10, 768), (66, 768), (18, 768), (51, 768), (36, 768), (61, 768), (75, 768), (91, 768), (31, 768), (94, 768), (56, 768), (48, 768), (50, 768), (8, 768), (5, 768), (50, 768), (56, 768), (47, 768), (51, 768), (40, 768), (57, 768), (57, 768), (55, 768), (47, 768), (62, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.110336 MB\n",
      "sizes:[202752, 236544, 33792, 159744, 165888, 187392, 141312, 79872, 27648, 224256, 301056, 86016, 156672, 168960, 18432, 24576, 86016, 172032, 261120, 73728, 236544, 153600, 165888, 21504, 70656, 236544, 18432, 46080, 172032, 18432, 135168, 27648]\n",
      "[(66, 768), (77, 768), (11, 768), (52, 768), (54, 768), (61, 768), (46, 768), (26, 768), (9, 768), (73, 768), (98, 768), (28, 768), (51, 768), (55, 768), (6, 768), (8, 768), (28, 768), (56, 768), (85, 768), (24, 768), (77, 768), (50, 768), (54, 768), (7, 768), (23, 768), (77, 768), (6, 768), (15, 768), (56, 768), (6, 768), (44, 768), (9, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:1.152 MB\n",
      "sizes:[162816, 178176, 39936, 141312, 172032, 181248, 70656, 24576, 159744, 21504]\n",
      "[(53, 768), (58, 768), (13, 768), (46, 768), (56, 768), (59, 768), (23, 768), (8, 768), (52, 768), (7, 768)]\n",
      "\n",
      "Epoch: 2 \t Training Loss: 0.413157\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.073472 MB\n",
      "sizes:[144384, 46080, 52224, 187392, 18432, 156672, 18432, 165888, 251904, 101376, 159744, 33792, 101376, 242688, 181248, 46080, 190464, 52224, 135168, 156672, 178176, 172032, 27648, 184320, 144384, 43008, 215040, 135168, 113664, 70656, 181248, 165888]\n",
      "[(47, 768), (15, 768), (17, 768), (61, 768), (6, 768), (51, 768), (6, 768), (54, 768), (82, 768), (33, 768), (52, 768), (11, 768), (33, 768), (79, 768), (59, 768), (15, 768), (62, 768), (17, 768), (44, 768), (51, 768), (58, 768), (56, 768), (9, 768), (60, 768), (47, 768), (14, 768), (70, 768), (44, 768), (37, 768), (23, 768), (59, 768), (54, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.326976 MB\n",
      "sizes:[18432, 150528, 15360, 24576, 18432, 254976, 162816, 236544, 30720, 172032, 67584, 135168, 153600, 49152, 288768, 147456, 15360, 39936, 24576, 156672, 33792, 175104, 184320, 159744, 141312, 92160, 27648, 104448, 24576, 175104, 21504, 24576]\n",
      "[(6, 768), (49, 768), (5, 768), (8, 768), (6, 768), (83, 768), (53, 768), (77, 768), (10, 768), (56, 768), (22, 768), (44, 768), (50, 768), (16, 768), (94, 768), (48, 768), (5, 768), (13, 768), (8, 768), (51, 768), (11, 768), (57, 768), (60, 768), (52, 768), (46, 768), (30, 768), (9, 768), (34, 768), (8, 768), (57, 768), (7, 768), (8, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.796992 MB\n",
      "sizes:[67584, 153600, 178176, 92160, 110592, 30720, 36864, 178176, 242688, 331776, 304128, 104448, 233472, 141312, 15360, 282624, 24576, 193536, 135168, 15360, 162816, 227328, 178176, 162816, 193536]\n",
      "[(22, 768), (50, 768), (58, 768), (30, 768), (36, 768), (10, 768), (12, 768), (58, 768), (79, 768), (108, 768), (99, 768), (34, 768), (76, 768), (46, 768), (5, 768), (92, 768), (8, 768), (63, 768), (44, 768), (5, 768), (53, 768), (74, 768), (58, 768), (53, 768), (63, 768)]\n",
      "\n",
      "y_pred: 0, y_true 7.0\n",
      "y_score\n",
      "tensor([0.1685, 0.1781, 0.1235, 0.1672, 0.1332, 0.1277, 0.1332, 0.1908, 0.1542,\n",
      "        0.1272, 0.1582, 0.1613, 0.1690, 0.1635, 0.1607, 0.1781, 0.1378, 0.1852,\n",
      "        0.1531, 0.2326, 0.1327, 0.1727, 0.1512, 0.1517, 0.1189, 0.1143, 0.1472,\n",
      "        0.1828, 0.1531, 0.1029, 0.1920, 0.1406, 0.1332, 0.1288, 0.1264, 0.0473,\n",
      "        0.0453, 0.1732, 0.1441, 0.1738, 0.0691, 0.1776, 0.1067, 0.1545, 0.1837,\n",
      "        0.1817, 0.1686, 0.1303, 0.1264, 0.1702, 0.0943, 0.1349, 0.0705, 0.1410,\n",
      "        0.1282, 0.1734, 0.1559, 0.1239, 0.1512, 0.1575, 0.1456, 0.1332, 0.1397,\n",
      "        0.1456, 0.1997, 0.1372, 0.0658, 0.0673, 0.1217, 0.1564, 0.0987, 0.1328,\n",
      "        0.1615, 0.1876, 0.1943, 0.1301, 0.1695, 0.1510, 0.1264, 0.1692, 0.1456,\n",
      "        0.1879, 0.1049, 0.1264, 0.1619, 0.2380, 0.1489, 0.1597, 0.1876])\n",
      "y_pred\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "y_true\n",
      "tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
      "\n",
      "Epoch: 2 \t Validation p: 0.00, r:0.00, f: 0.00, roc_auc: 0.84\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.349952 MB\n",
      "sizes:[21504, 36864, 175104, 144384, 199680, 15360, 202752, 18432, 242688, 159744, 193536, 39936, 55296, 251904, 261120, 337920, 144384, 21504, 175104, 159744, 18432, 43008, 138240, 52224, 181248, 162816, 215040, 156672, 184320, 116736, 181248, 43008]\n",
      "[(7, 768), (12, 768), (57, 768), (47, 768), (65, 768), (5, 768), (66, 768), (6, 768), (79, 768), (52, 768), (63, 768), (13, 768), (18, 768), (82, 768), (85, 768), (110, 768), (47, 768), (7, 768), (57, 768), (52, 768), (6, 768), (14, 768), (45, 768), (17, 768), (59, 768), (53, 768), (70, 768), (51, 768), (60, 768), (38, 768), (59, 768), (14, 768)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abot/sw/anaconda3/envs/cs598_dlh/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertCollate tensor sizes\n",
      "cum_size:4.57728 MB\n",
      "sizes:[184320, 168960, 230400, 18432, 172032, 156672, 159744, 55296, 258048, 36864, 162816, 181248, 172032, 18432, 39936, 39936, 184320, 135168, 138240, 288768, 21504, 202752, 33792, 236544, 304128, 95232, 288768, 15360, 368640, 24576, 150528, 33792]\n",
      "[(60, 768), (55, 768), (75, 768), (6, 768), (56, 768), (51, 768), (52, 768), (18, 768), (84, 768), (12, 768), (53, 768), (59, 768), (56, 768), (6, 768), (13, 768), (13, 768), (60, 768), (44, 768), (45, 768), (94, 768), (7, 768), (66, 768), (11, 768), (77, 768), (99, 768), (31, 768), (94, 768), (5, 768), (120, 768), (8, 768), (49, 768), (11, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.893696 MB\n",
      "sizes:[239616, 162816, 125952, 153600, 125952, 129024, 153600, 18432, 153600, 156672, 193536, 150528, 227328, 46080, 215040, 267264, 162816, 273408, 159744, 24576, 144384, 165888, 150528, 24576, 147456, 159744, 208896, 107520, 184320, 49152, 187392, 224256]\n",
      "[(78, 768), (53, 768), (41, 768), (50, 768), (41, 768), (42, 768), (50, 768), (6, 768), (50, 768), (51, 768), (63, 768), (49, 768), (74, 768), (15, 768), (70, 768), (87, 768), (53, 768), (89, 768), (52, 768), (8, 768), (47, 768), (54, 768), (49, 768), (8, 768), (48, 768), (52, 768), (68, 768), (35, 768), (60, 768), (16, 768), (61, 768), (73, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.529728 MB\n",
      "sizes:[159744, 156672, 159744, 36864, 181248, 175104, 55296, 294912, 196608, 187392, 172032, 30720, 15360, 67584, 24576, 70656, 30720, 18432, 27648, 113664, 144384, 18432, 116736, 18432, 150528, 162816, 236544, 113664, 193536, 46080, 39936, 113664]\n",
      "[(52, 768), (51, 768), (52, 768), (12, 768), (59, 768), (57, 768), (18, 768), (96, 768), (64, 768), (61, 768), (56, 768), (10, 768), (5, 768), (22, 768), (8, 768), (23, 768), (10, 768), (6, 768), (9, 768), (37, 768), (47, 768), (6, 768), (38, 768), (6, 768), (49, 768), (53, 768), (77, 768), (37, 768), (63, 768), (15, 768), (13, 768), (37, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.174848 MB\n",
      "sizes:[168960, 135168, 208896, 43008, 162816, 64512, 119808, 39936, 55296, 181248, 187392, 181248, 175104, 184320, 39936, 187392, 162816, 245760, 18432, 165888, 211968, 141312, 86016, 175104, 165888, 147456, 21504, 36864, 227328, 33792, 147456, 52224]\n",
      "[(55, 768), (44, 768), (68, 768), (14, 768), (53, 768), (21, 768), (39, 768), (13, 768), (18, 768), (59, 768), (61, 768), (59, 768), (57, 768), (60, 768), (13, 768), (61, 768), (53, 768), (80, 768), (6, 768), (54, 768), (69, 768), (46, 768), (28, 768), (57, 768), (54, 768), (48, 768), (7, 768), (12, 768), (74, 768), (11, 768), (48, 768), (17, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.451328 MB\n",
      "sizes:[153600, 156672, 138240, 30720, 39936, 168960, 233472, 181248, 153600, 150528, 193536, 73728, 153600, 138240, 159744, 208896, 122880, 43008, 30720, 165888, 39936, 202752, 162816, 61440, 172032, 325632, 89088, 175104, 46080, 150528, 205824, 122880]\n",
      "[(50, 768), (51, 768), (45, 768), (10, 768), (13, 768), (55, 768), (76, 768), (59, 768), (50, 768), (49, 768), (63, 768), (24, 768), (50, 768), (45, 768), (52, 768), (68, 768), (40, 768), (14, 768), (10, 768), (54, 768), (13, 768), (66, 768), (53, 768), (20, 768), (56, 768), (106, 768), (29, 768), (57, 768), (15, 768), (49, 768), (67, 768), (40, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.959808 MB\n",
      "sizes:[202752, 297984, 162816, 165888, 184320, 43008, 36864, 181248, 141312, 33792, 30720, 245760, 159744, 162816, 36864, 181248, 141312, 156672, 129024, 98304, 36864, 58368, 190464, 36864, 147456, 67584, 153600, 36864, 162816, 30720, 110592, 135168]\n",
      "[(66, 768), (97, 768), (53, 768), (54, 768), (60, 768), (14, 768), (12, 768), (59, 768), (46, 768), (11, 768), (10, 768), (80, 768), (52, 768), (53, 768), (12, 768), (59, 768), (46, 768), (51, 768), (42, 768), (32, 768), (12, 768), (19, 768), (62, 768), (12, 768), (48, 768), (22, 768), (50, 768), (12, 768), (53, 768), (10, 768), (36, 768), (44, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.999744 MB\n",
      "sizes:[18432, 92160, 205824, 79872, 162816, 153600, 125952, 162816, 181248, 141312, 175104, 24576, 110592, 43008, 168960, 46080, 172032, 156672, 125952, 279552, 156672, 18432, 64512, 43008, 184320, 61440, 156672, 221184, 104448, 168960, 24576, 168960]\n",
      "[(6, 768), (30, 768), (67, 768), (26, 768), (53, 768), (50, 768), (41, 768), (53, 768), (59, 768), (46, 768), (57, 768), (8, 768), (36, 768), (14, 768), (55, 768), (15, 768), (56, 768), (51, 768), (41, 768), (91, 768), (51, 768), (6, 768), (21, 768), (14, 768), (60, 768), (20, 768), (51, 768), (72, 768), (34, 768), (55, 768), (8, 768), (55, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.40832 MB\n",
      "sizes:[215040, 21504, 30720, 24576, 110592, 190464, 150528, 159744, 168960, 156672, 18432, 264192, 18432, 110592, 270336, 86016, 104448, 236544, 178176, 193536, 58368, 104448, 15360, 172032, 245760, 156672, 135168, 172032, 36864, 52224, 239616, 310272]\n",
      "[(70, 768), (7, 768), (10, 768), (8, 768), (36, 768), (62, 768), (49, 768), (52, 768), (55, 768), (51, 768), (6, 768), (86, 768), (6, 768), (36, 768), (88, 768), (28, 768), (34, 768), (77, 768), (58, 768), (63, 768), (19, 768), (34, 768), (5, 768), (56, 768), (80, 768), (51, 768), (44, 768), (56, 768), (12, 768), (17, 768), (78, 768), (101, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.666368 MB\n",
      "sizes:[138240, 21504, 264192, 221184, 64512, 64512, 21504, 236544, 73728, 156672, 175104, 86016, 141312, 181248, 172032, 150528, 52224, 279552, 21504, 95232, 172032, 175104, 150528, 175104, 82944, 150528, 245760, 156672, 193536, 196608, 162816, 187392]\n",
      "[(45, 768), (7, 768), (86, 768), (72, 768), (21, 768), (21, 768), (7, 768), (77, 768), (24, 768), (51, 768), (57, 768), (28, 768), (46, 768), (59, 768), (56, 768), (49, 768), (17, 768), (91, 768), (7, 768), (31, 768), (56, 768), (57, 768), (49, 768), (57, 768), (27, 768), (49, 768), (80, 768), (51, 768), (63, 768), (64, 768), (53, 768), (61, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.646464 MB\n",
      "sizes:[144384, 162816, 181248, 24576, 156672, 43008, 15360, 199680, 150528, 150528, 125952, 24576, 15360, 95232, 30720, 33792, 132096, 153600, 178176, 15360, 55296, 224256, 27648, 89088, 172032, 43008, 218112, 215040, 135168, 175104, 193536, 64512]\n",
      "[(47, 768), (53, 768), (59, 768), (8, 768), (51, 768), (14, 768), (5, 768), (65, 768), (49, 768), (49, 768), (41, 768), (8, 768), (5, 768), (31, 768), (10, 768), (11, 768), (43, 768), (50, 768), (58, 768), (5, 768), (18, 768), (73, 768), (9, 768), (29, 768), (56, 768), (14, 768), (71, 768), (70, 768), (44, 768), (57, 768), (63, 768), (21, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.036608 MB\n",
      "sizes:[190464, 18432, 15360, 135168, 175104, 86016, 33792, 175104, 18432, 30720, 190464, 165888, 165888, 30720, 181248, 172032, 153600, 18432, 33792, 159744, 159744, 153600, 49152, 156672, 202752, 153600, 288768, 27648, 150528, 159744, 55296, 328704]\n",
      "[(62, 768), (6, 768), (5, 768), (44, 768), (57, 768), (28, 768), (11, 768), (57, 768), (6, 768), (10, 768), (62, 768), (54, 768), (54, 768), (10, 768), (59, 768), (56, 768), (50, 768), (6, 768), (11, 768), (52, 768), (52, 768), (50, 768), (16, 768), (51, 768), (66, 768), (50, 768), (94, 768), (9, 768), (49, 768), (52, 768), (18, 768), (107, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.65568 MB\n",
      "sizes:[27648, 196608, 172032, 15360, 125952, 21504, 162816, 181248, 27648, 18432, 236544, 107520, 184320, 18432, 187392, 156672, 168960, 172032, 21504, 150528, 168960, 55296, 150528, 172032, 18432, 79872, 30720, 156672, 129024, 215040, 82944, 43008]\n",
      "[(9, 768), (64, 768), (56, 768), (5, 768), (41, 768), (7, 768), (53, 768), (59, 768), (9, 768), (6, 768), (77, 768), (35, 768), (60, 768), (6, 768), (61, 768), (51, 768), (55, 768), (56, 768), (7, 768), (49, 768), (55, 768), (18, 768), (49, 768), (56, 768), (6, 768), (26, 768), (10, 768), (51, 768), (42, 768), (70, 768), (27, 768), (14, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.211712 MB\n",
      "sizes:[46080, 55296, 353280, 162816, 21504, 144384, 113664, 218112, 64512, 178176, 156672, 55296, 30720, 55296, 236544, 211968, 288768, 165888, 159744, 276480, 43008, 24576, 168960, 67584, 135168, 110592, 224256, 39936, 18432, 215040, 138240, 30720]\n",
      "[(15, 768), (18, 768), (115, 768), (53, 768), (7, 768), (47, 768), (37, 768), (71, 768), (21, 768), (58, 768), (51, 768), (18, 768), (10, 768), (18, 768), (77, 768), (69, 768), (94, 768), (54, 768), (52, 768), (90, 768), (14, 768), (8, 768), (55, 768), (22, 768), (44, 768), (36, 768), (73, 768), (13, 768), (6, 768), (70, 768), (45, 768), (10, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.86912 MB\n",
      "sizes:[58368, 202752, 76800, 190464, 138240, 196608, 175104, 165888, 181248, 147456, 215040, 178176, 193536, 24576, 24576, 175104, 30720, 110592, 347136, 162816, 175104, 172032, 172032, 39936, 208896, 43008, 264192, 215040, 144384, 211968, 73728, 153600]\n",
      "[(19, 768), (66, 768), (25, 768), (62, 768), (45, 768), (64, 768), (57, 768), (54, 768), (59, 768), (48, 768), (70, 768), (58, 768), (63, 768), (8, 768), (8, 768), (57, 768), (10, 768), (36, 768), (113, 768), (53, 768), (57, 768), (56, 768), (56, 768), (13, 768), (68, 768), (14, 768), (86, 768), (70, 768), (47, 768), (69, 768), (24, 768), (50, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.426752 MB\n",
      "sizes:[30720, 156672, 254976, 172032, 162816, 113664, 30720, 92160, 165888, 92160, 248832, 33792, 33792, 64512, 122880, 181248, 178176, 144384, 261120, 175104, 218112, 98304, 150528, 168960, 82944, 18432, 138240, 236544, 261120, 58368, 18432, 261120]\n",
      "[(10, 768), (51, 768), (83, 768), (56, 768), (53, 768), (37, 768), (10, 768), (30, 768), (54, 768), (30, 768), (81, 768), (11, 768), (11, 768), (21, 768), (40, 768), (59, 768), (58, 768), (47, 768), (85, 768), (57, 768), (71, 768), (32, 768), (49, 768), (55, 768), (27, 768), (6, 768), (45, 768), (77, 768), (85, 768), (19, 768), (6, 768), (85, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.571136 MB\n",
      "sizes:[39936, 337920, 175104, 178176, 242688, 165888, 153600, 153600, 187392, 153600, 147456, 61440, 18432, 58368, 153600, 95232, 153600, 95232, 15360, 224256, 190464, 36864, 39936, 147456, 239616, 27648, 218112, 144384, 168960, 21504, 316416, 208896]\n",
      "[(13, 768), (110, 768), (57, 768), (58, 768), (79, 768), (54, 768), (50, 768), (50, 768), (61, 768), (50, 768), (48, 768), (20, 768), (6, 768), (19, 768), (50, 768), (31, 768), (50, 768), (31, 768), (5, 768), (73, 768), (62, 768), (12, 768), (13, 768), (48, 768), (78, 768), (9, 768), (71, 768), (47, 768), (55, 768), (7, 768), (103, 768), (68, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.466688 MB\n",
      "sizes:[267264, 276480, 144384, 141312, 245760, 150528, 135168, 276480, 21504, 24576, 178176, 187392, 27648, 202752, 24576, 202752, 55296, 184320, 36864, 178176, 36864, 285696, 64512, 162816, 70656, 245760, 156672, 49152, 55296, 70656, 159744, 147456]\n",
      "[(87, 768), (90, 768), (47, 768), (46, 768), (80, 768), (49, 768), (44, 768), (90, 768), (7, 768), (8, 768), (58, 768), (61, 768), (9, 768), (66, 768), (8, 768), (66, 768), (18, 768), (60, 768), (12, 768), (58, 768), (12, 768), (93, 768), (21, 768), (53, 768), (23, 768), (80, 768), (51, 768), (16, 768), (18, 768), (23, 768), (52, 768), (48, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.821568 MB\n",
      "sizes:[215040, 181248, 301056, 147456, 132096, 172032, 175104, 46080, 21504, 79872, 147456, 30720, 150528, 24576, 168960, 181248, 153600, 178176, 122880, 55296, 33792, 21504, 15360, 156672, 27648, 27648, 27648, 178176, 159744, 76800, 178176, 233472]\n",
      "[(70, 768), (59, 768), (98, 768), (48, 768), (43, 768), (56, 768), (57, 768), (15, 768), (7, 768), (26, 768), (48, 768), (10, 768), (49, 768), (8, 768), (55, 768), (59, 768), (50, 768), (58, 768), (40, 768), (18, 768), (11, 768), (7, 768), (5, 768), (51, 768), (9, 768), (9, 768), (9, 768), (58, 768), (52, 768), (25, 768), (58, 768), (76, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.827712 MB\n",
      "sizes:[153600, 86016, 178176, 168960, 159744, 30720, 122880, 153600, 33792, 113664, 156672, 43008, 79872, 138240, 172032, 27648, 193536, 73728, 18432, 18432, 165888, 316416, 27648, 58368, 58368, 153600, 159744, 175104, 175104, 168960, 175104, 70656]\n",
      "[(50, 768), (28, 768), (58, 768), (55, 768), (52, 768), (10, 768), (40, 768), (50, 768), (11, 768), (37, 768), (51, 768), (14, 768), (26, 768), (45, 768), (56, 768), (9, 768), (63, 768), (24, 768), (6, 768), (6, 768), (54, 768), (103, 768), (9, 768), (19, 768), (19, 768), (50, 768), (52, 768), (57, 768), (57, 768), (55, 768), (57, 768), (23, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.73248 MB\n",
      "sizes:[52224, 76800, 221184, 15360, 236544, 39936, 92160, 33792, 18432, 24576, 141312, 362496, 156672, 159744, 187392, 33792, 273408, 162816, 230400, 33792, 46080, 21504, 36864, 181248, 33792, 98304, 64512, 211968, 190464, 61440, 193536, 39936]\n",
      "[(17, 768), (25, 768), (72, 768), (5, 768), (77, 768), (13, 768), (30, 768), (11, 768), (6, 768), (8, 768), (46, 768), (118, 768), (51, 768), (52, 768), (61, 768), (11, 768), (89, 768), (53, 768), (75, 768), (11, 768), (15, 768), (7, 768), (12, 768), (59, 768), (11, 768), (32, 768), (21, 768), (69, 768), (62, 768), (20, 768), (63, 768), (13, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.598784 MB\n",
      "sizes:[199680, 301056, 18432, 205824, 156672, 156672, 27648, 282624, 30720, 301056, 58368, 162816, 43008, 159744, 15360, 153600, 165888, 190464, 27648, 199680, 18432, 24576, 172032, 205824, 138240, 153600, 294912, 147456, 156672, 101376, 159744, 168960]\n",
      "[(65, 768), (98, 768), (6, 768), (67, 768), (51, 768), (51, 768), (9, 768), (92, 768), (10, 768), (98, 768), (19, 768), (53, 768), (14, 768), (52, 768), (5, 768), (50, 768), (54, 768), (62, 768), (9, 768), (65, 768), (6, 768), (8, 768), (56, 768), (67, 768), (45, 768), (50, 768), (96, 768), (48, 768), (51, 768), (33, 768), (52, 768), (55, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:1.560576 MB\n",
      "sizes:[301056, 168960, 168960, 159744, 150528, 33792, 55296, 368640, 138240, 15360]\n",
      "[(98, 768), (55, 768), (55, 768), (52, 768), (49, 768), (11, 768), (18, 768), (120, 768), (45, 768), (5, 768)]\n",
      "\n",
      "Epoch: 3 \t Training Loss: 0.403209\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.750912 MB\n",
      "sizes:[18432, 15360, 49152, 18432, 288768, 24576, 178176, 110592, 46080, 193536, 331776, 233472, 172032, 162816, 30720, 18432, 39936, 184320, 33792, 135168, 67584, 67584, 175104, 282624, 181248, 15360, 147456, 24576, 92160, 215040, 165888, 30720]\n",
      "[(6, 768), (5, 768), (16, 768), (6, 768), (94, 768), (8, 768), (58, 768), (36, 768), (15, 768), (63, 768), (108, 768), (76, 768), (56, 768), (53, 768), (10, 768), (6, 768), (13, 768), (60, 768), (11, 768), (44, 768), (22, 768), (22, 768), (57, 768), (92, 768), (59, 768), (5, 768), (48, 768), (8, 768), (30, 768), (70, 768), (54, 768), (10, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.303872 MB\n",
      "sizes:[18432, 254976, 156672, 227328, 156672, 141312, 135168, 175104, 101376, 101376, 190464, 242688, 251904, 15360, 52224, 27648, 135168, 162816, 144384, 92160, 43008, 178176, 159744, 181248, 141312, 242688, 104448, 135168, 24576, 36864, 113664, 159744]\n",
      "[(6, 768), (83, 768), (51, 768), (74, 768), (51, 768), (46, 768), (44, 768), (57, 768), (33, 768), (33, 768), (62, 768), (79, 768), (82, 768), (5, 768), (17, 768), (9, 768), (44, 768), (53, 768), (47, 768), (30, 768), (14, 768), (58, 768), (52, 768), (59, 768), (46, 768), (79, 768), (34, 768), (44, 768), (8, 768), (12, 768), (37, 768), (52, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.142656 MB\n",
      "sizes:[150528, 162816, 184320, 165888, 104448, 187392, 178176, 178176, 193536, 52224, 46080, 15360, 304128, 236544, 24576, 24576, 70656, 144384, 172032, 21504, 33792, 156672, 27648, 153600, 153600]\n",
      "[(49, 768), (53, 768), (60, 768), (54, 768), (34, 768), (61, 768), (58, 768), (58, 768), (63, 768), (17, 768), (15, 768), (5, 768), (99, 768), (77, 768), (8, 768), (8, 768), (23, 768), (47, 768), (56, 768), (7, 768), (11, 768), (51, 768), (9, 768), (50, 768), (50, 768)]\n",
      "\n",
      "y_pred: 0, y_true 7.0\n",
      "y_score\n",
      "tensor([0.1643, 0.1537, 0.2357, 0.1643, 0.1945, 0.1046, 0.0608, 0.1189, 0.2306,\n",
      "        0.2399, 0.2072, 0.2046, 0.2305, 0.1613, 0.0752, 0.1643, 0.2193, 0.1754,\n",
      "        0.2063, 0.1704, 0.2598, 0.1118, 0.1226, 0.1574, 0.1654, 0.1537, 0.1285,\n",
      "        0.1830, 0.1307, 0.2007, 0.2408, 0.1991, 0.0395, 0.1738, 0.1421, 0.3365,\n",
      "        0.1202, 0.1715, 0.1067, 0.1415, 0.1962, 0.1228, 0.1619, 0.1916, 0.1742,\n",
      "        0.1537, 0.2404, 0.1914, 0.2424, 0.2015, 0.1927, 0.0557, 0.1278, 0.1719,\n",
      "        0.2153, 0.2438, 0.1654, 0.1941, 0.1908, 0.1672, 0.1830, 0.1104, 0.1765,\n",
      "        0.1716, 0.1221, 0.1825, 0.1369, 0.1649, 0.1482, 0.1753, 0.1236, 0.1439,\n",
      "        0.2838, 0.1359, 0.2306, 0.1537, 0.2419, 0.1980, 0.1830, 0.0416, 0.0985,\n",
      "        0.1280, 0.2364, 0.1740, 0.0769, 0.3316, 0.1914, 0.2271, 0.1340])\n",
      "y_pred\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "y_true\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0.,\n",
      "        0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n",
      "Epoch: 3 \t Validation p: 0.00, r:0.00, f: 0.00, roc_auc: 0.73\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.959808 MB\n",
      "sizes:[168960, 297984, 110592, 21504, 86016, 27648, 162816, 15360, 18432, 43008, 162816, 153600, 39936, 138240, 159744, 165888, 156672, 67584, 141312, 215040, 43008, 159744, 208896, 129024, 150528, 190464, 18432, 347136, 27648, 39936, 18432, 273408]\n",
      "[(55, 768), (97, 768), (36, 768), (7, 768), (28, 768), (9, 768), (53, 768), (5, 768), (6, 768), (14, 768), (53, 768), (50, 768), (13, 768), (45, 768), (52, 768), (54, 768), (51, 768), (22, 768), (46, 768), (70, 768), (14, 768), (52, 768), (68, 768), (42, 768), (49, 768), (62, 768), (6, 768), (113, 768), (9, 768), (13, 768), (6, 768), (89, 768)]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abot/sw/anaconda3/envs/cs598_dlh/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertCollate tensor sizes\n",
      "cum_size:3.585024 MB\n",
      "sizes:[18432, 156672, 135168, 113664, 18432, 181248, 245760, 18432, 159744, 30720, 58368, 175104, 168960, 282624, 165888, 73728, 33792, 187392, 18432, 175104, 46080, 208896, 172032, 21504, 21504, 92160, 162816, 43008, 58368, 138240, 36864, 165888]\n",
      "[(6, 768), (51, 768), (44, 768), (37, 768), (6, 768), (59, 768), (80, 768), (6, 768), (52, 768), (10, 768), (19, 768), (57, 768), (55, 768), (92, 768), (54, 768), (24, 768), (11, 768), (61, 768), (6, 768), (57, 768), (15, 768), (68, 768), (56, 768), (7, 768), (7, 768), (30, 768), (53, 768), (14, 768), (19, 768), (45, 768), (12, 768), (54, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.482048 MB\n",
      "sizes:[193536, 150528, 165888, 73728, 261120, 175104, 92160, 125952, 178176, 135168, 168960, 187392, 245760, 55296, 156672, 39936, 230400, 150528, 178176, 181248, 36864, 150528, 15360, 215040, 178176, 18432, 141312, 159744, 190464, 153600, 33792, 43008]\n",
      "[(63, 768), (49, 768), (54, 768), (24, 768), (85, 768), (57, 768), (30, 768), (41, 768), (58, 768), (44, 768), (55, 768), (61, 768), (80, 768), (18, 768), (51, 768), (13, 768), (75, 768), (49, 768), (58, 768), (59, 768), (12, 768), (49, 768), (5, 768), (70, 768), (58, 768), (6, 768), (46, 768), (52, 768), (62, 768), (50, 768), (11, 768), (14, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.87072 MB\n",
      "sizes:[175104, 150528, 27648, 36864, 168960, 205824, 24576, 168960, 52224, 175104, 178176, 15360, 162816, 181248, 39936, 153600, 150528, 270336, 39936, 64512, 153600, 178176, 46080, 21504, 175104, 156672, 18432, 153600, 156672, 144384, 64512, 159744]\n",
      "[(57, 768), (49, 768), (9, 768), (12, 768), (55, 768), (67, 768), (8, 768), (55, 768), (17, 768), (57, 768), (58, 768), (5, 768), (53, 768), (59, 768), (13, 768), (50, 768), (49, 768), (88, 768), (13, 768), (21, 768), (50, 768), (58, 768), (15, 768), (7, 768), (57, 768), (51, 768), (6, 768), (50, 768), (51, 768), (47, 768), (21, 768), (52, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.873792 MB\n",
      "sizes:[125952, 55296, 64512, 159744, 279552, 43008, 236544, 49152, 153600, 159744, 168960, 95232, 153600, 168960, 193536, 70656, 95232, 239616, 18432, 196608, 46080, 162816, 39936, 61440, 224256, 27648, 110592, 18432, 33792, 18432, 156672, 245760]\n",
      "[(41, 768), (18, 768), (21, 768), (52, 768), (91, 768), (14, 768), (77, 768), (16, 768), (50, 768), (52, 768), (55, 768), (31, 768), (50, 768), (55, 768), (63, 768), (23, 768), (31, 768), (78, 768), (6, 768), (64, 768), (15, 768), (53, 768), (13, 768), (20, 768), (73, 768), (9, 768), (36, 768), (6, 768), (11, 768), (6, 768), (51, 768), (80, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.93056 MB\n",
      "sizes:[153600, 33792, 264192, 82944, 122880, 156672, 199680, 202752, 162816, 46080, 184320, 159744, 150528, 30720, 184320, 353280, 227328, 135168, 205824, 181248, 150528, 175104, 267264, 27648, 236544, 98304, 18432, 261120, 21504, 261120, 30720, 144384]\n",
      "[(50, 768), (11, 768), (86, 768), (27, 768), (40, 768), (51, 768), (65, 768), (66, 768), (53, 768), (15, 768), (60, 768), (52, 768), (49, 768), (10, 768), (60, 768), (115, 768), (74, 768), (44, 768), (67, 768), (59, 768), (49, 768), (57, 768), (87, 768), (9, 768), (77, 768), (32, 768), (6, 768), (85, 768), (7, 768), (85, 768), (10, 768), (47, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.411392 MB\n",
      "sizes:[165888, 172032, 64512, 205824, 190464, 27648, 150528, 30720, 264192, 172032, 156672, 30720, 150528, 55296, 181248, 79872, 168960, 187392, 39936, 211968, 144384, 159744, 125952, 147456, 113664, 153600, 175104, 202752, 30720, 116736, 175104, 159744]\n",
      "[(54, 768), (56, 768), (21, 768), (67, 768), (62, 768), (9, 768), (49, 768), (10, 768), (86, 768), (56, 768), (51, 768), (10, 768), (49, 768), (18, 768), (59, 768), (26, 768), (55, 768), (61, 768), (13, 768), (69, 768), (47, 768), (52, 768), (41, 768), (48, 768), (37, 768), (50, 768), (57, 768), (66, 768), (10, 768), (38, 768), (57, 768), (52, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.391488 MB\n",
      "sizes:[181248, 33792, 33792, 27648, 95232, 18432, 24576, 162816, 55296, 236544, 33792, 211968, 43008, 172032, 187392, 144384, 21504, 46080, 43008, 165888, 193536, 30720, 285696, 24576, 36864, 39936, 276480, 199680, 165888, 95232, 73728, 30720]\n",
      "[(59, 768), (11, 768), (11, 768), (9, 768), (31, 768), (6, 768), (8, 768), (53, 768), (18, 768), (77, 768), (11, 768), (69, 768), (14, 768), (56, 768), (61, 768), (47, 768), (7, 768), (15, 768), (14, 768), (54, 768), (63, 768), (10, 768), (93, 768), (8, 768), (12, 768), (13, 768), (90, 768), (65, 768), (54, 768), (31, 768), (24, 768), (10, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.833856 MB\n",
      "sizes:[21504, 168960, 147456, 159744, 190464, 43008, 193536, 190464, 55296, 150528, 156672, 82944, 218112, 39936, 175104, 202752, 24576, 30720, 156672, 110592, 165888, 135168, 187392, 107520, 52224, 141312, 39936, 43008, 43008, 30720, 193536, 175104]\n",
      "[(7, 768), (55, 768), (48, 768), (52, 768), (62, 768), (14, 768), (63, 768), (62, 768), (18, 768), (49, 768), (51, 768), (27, 768), (71, 768), (13, 768), (57, 768), (66, 768), (8, 768), (10, 768), (51, 768), (36, 768), (54, 768), (44, 768), (61, 768), (35, 768), (17, 768), (46, 768), (13, 768), (14, 768), (14, 768), (10, 768), (63, 768), (57, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:5.587968 MB\n",
      "sizes:[168960, 156672, 215040, 328704, 175104, 135168, 175104, 279552, 165888, 184320, 215040, 190464, 104448, 147456, 224256, 153600, 172032, 288768, 150528, 221184, 55296, 193536, 165888, 55296, 132096, 15360, 79872, 181248, 215040, 301056, 162816, 178176]\n",
      "[(55, 768), (51, 768), (70, 768), (107, 768), (57, 768), (44, 768), (57, 768), (91, 768), (54, 768), (60, 768), (70, 768), (62, 768), (34, 768), (48, 768), (73, 768), (50, 768), (56, 768), (94, 768), (49, 768), (72, 768), (18, 768), (63, 768), (54, 768), (18, 768), (43, 768), (5, 768), (26, 768), (59, 768), (70, 768), (98, 768), (53, 768), (58, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.19328 MB\n",
      "sizes:[110592, 187392, 15360, 178176, 153600, 18432, 215040, 181248, 162816, 202752, 175104, 168960, 24576, 147456, 113664, 153600, 138240, 159744, 27648, 218112, 141312, 98304, 162816, 144384, 162816, 175104, 138240, 168960, 21504, 30720, 49152, 147456]\n",
      "[(36, 768), (61, 768), (5, 768), (58, 768), (50, 768), (6, 768), (70, 768), (59, 768), (53, 768), (66, 768), (57, 768), (55, 768), (8, 768), (48, 768), (37, 768), (50, 768), (45, 768), (52, 768), (9, 768), (71, 768), (46, 768), (32, 768), (53, 768), (47, 768), (53, 768), (57, 768), (45, 768), (55, 768), (7, 768), (10, 768), (16, 768), (48, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.574208 MB\n",
      "sizes:[55296, 147456, 153600, 202752, 98304, 162816, 76800, 159744, 337920, 52224, 162816, 24576, 153600, 162816, 273408, 175104, 162816, 79872, 172032, 208896, 162816, 172032, 132096, 150528, 18432, 181248, 67584, 267264, 43008, 18432, 141312, 196608]\n",
      "[(18, 768), (48, 768), (50, 768), (66, 768), (32, 768), (53, 768), (25, 768), (52, 768), (110, 768), (17, 768), (53, 768), (8, 768), (50, 768), (53, 768), (89, 768), (57, 768), (53, 768), (26, 768), (56, 768), (68, 768), (53, 768), (56, 768), (43, 768), (49, 768), (6, 768), (59, 768), (22, 768), (87, 768), (14, 768), (6, 768), (46, 768), (64, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:5.121024 MB\n",
      "sizes:[202752, 172032, 82944, 159744, 218112, 168960, 30720, 64512, 147456, 202752, 215040, 172032, 181248, 138240, 236544, 301056, 184320, 215040, 368640, 172032, 368640, 159744, 58368, 36864, 36864, 172032, 43008, 58368, 245760, 187392, 95232, 24576]\n",
      "[(66, 768), (56, 768), (27, 768), (52, 768), (71, 768), (55, 768), (10, 768), (21, 768), (48, 768), (66, 768), (70, 768), (56, 768), (59, 768), (45, 768), (77, 768), (98, 768), (60, 768), (70, 768), (120, 768), (56, 768), (120, 768), (52, 768), (19, 768), (12, 768), (12, 768), (56, 768), (14, 768), (19, 768), (80, 768), (61, 768), (31, 768), (8, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.033536 MB\n",
      "sizes:[30720, 15360, 147456, 153600, 36864, 138240, 153600, 24576, 227328, 193536, 55296, 61440, 221184, 122880, 208896, 248832, 233472, 156672, 21504, 236544, 245760, 18432, 58368, 156672, 159744, 18432, 224256, 184320, 153600, 73728, 33792, 18432]\n",
      "[(10, 768), (5, 768), (48, 768), (50, 768), (12, 768), (45, 768), (50, 768), (8, 768), (74, 768), (63, 768), (18, 768), (20, 768), (72, 768), (40, 768), (68, 768), (81, 768), (76, 768), (51, 768), (7, 768), (77, 768), (80, 768), (6, 768), (19, 768), (51, 768), (52, 768), (6, 768), (73, 768), (60, 768), (50, 768), (24, 768), (11, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.769344 MB\n",
      "sizes:[156672, 156672, 153600, 165888, 168960, 36864, 236544, 21504, 150528, 27648, 21504, 156672, 156672, 150528, 21504, 113664, 30720, 172032, 89088, 159744, 208896, 15360, 21504, 242688, 33792, 46080, 301056, 39936, 144384, 55296, 294912, 18432]\n",
      "[(51, 768), (51, 768), (50, 768), (54, 768), (55, 768), (12, 768), (77, 768), (7, 768), (49, 768), (9, 768), (7, 768), (51, 768), (51, 768), (49, 768), (7, 768), (37, 768), (10, 768), (56, 768), (29, 768), (52, 768), (68, 768), (5, 768), (7, 768), (79, 768), (11, 768), (15, 768), (98, 768), (13, 768), (47, 768), (18, 768), (96, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.349952 MB\n",
      "sizes:[36864, 181248, 110592, 36864, 92160, 264192, 24576, 172032, 181248, 199680, 33792, 184320, 30720, 153600, 159744, 172032, 107520, 138240, 162816, 52224, 33792, 181248, 159744, 135168, 86016, 122880, 304128, 175104, 86016, 129024, 165888, 276480]\n",
      "[(12, 768), (59, 768), (36, 768), (12, 768), (30, 768), (86, 768), (8, 768), (56, 768), (59, 768), (65, 768), (11, 768), (60, 768), (10, 768), (50, 768), (52, 768), (56, 768), (35, 768), (45, 768), (53, 768), (17, 768), (11, 768), (59, 768), (52, 768), (44, 768), (28, 768), (40, 768), (99, 768), (57, 768), (28, 768), (42, 768), (54, 768), (90, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.005888 MB\n",
      "sizes:[144384, 18432, 33792, 178176, 316416, 43008, 276480, 153600, 122880, 39936, 168960, 294912, 125952, 236544, 70656, 33792, 221184, 141312, 15360, 27648, 181248, 21504, 239616, 156672, 153600, 24576, 211968, 64512, 18432, 181248, 27648, 61440]\n",
      "[(47, 768), (6, 768), (11, 768), (58, 768), (103, 768), (14, 768), (90, 768), (50, 768), (40, 768), (13, 768), (55, 768), (96, 768), (41, 768), (77, 768), (23, 768), (11, 768), (72, 768), (46, 768), (5, 768), (9, 768), (59, 768), (7, 768), (78, 768), (51, 768), (50, 768), (8, 768), (69, 768), (21, 768), (6, 768), (59, 768), (9, 768), (20, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.365312 MB\n",
      "sizes:[233472, 79872, 15360, 138240, 18432, 55296, 184320, 181248, 113664, 153600, 24576, 156672, 125952, 181248, 159744, 156672, 187392, 39936, 36864, 165888, 184320, 141312, 178176, 172032, 86016, 288768, 138240, 199680, 202752, 39936, 156672, 168960]\n",
      "[(76, 768), (26, 768), (5, 768), (45, 768), (6, 768), (18, 768), (60, 768), (59, 768), (37, 768), (50, 768), (8, 768), (51, 768), (41, 768), (59, 768), (52, 768), (51, 768), (61, 768), (13, 768), (12, 768), (54, 768), (60, 768), (46, 768), (58, 768), (56, 768), (28, 768), (94, 768), (45, 768), (65, 768), (66, 768), (13, 768), (51, 768), (55, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.866048 MB\n",
      "sizes:[218112, 122880, 258048, 175104, 43008, 144384, 172032, 288768, 144384, 172032, 116736, 36864, 58368, 144384, 150528, 138240, 178176, 172032, 15360, 175104, 181248, 261120, 150528, 205824, 135168, 162816, 64512, 15360, 325632, 215040, 61440, 162816]\n",
      "[(71, 768), (40, 768), (84, 768), (57, 768), (14, 768), (47, 768), (56, 768), (94, 768), (47, 768), (56, 768), (38, 768), (12, 768), (19, 768), (47, 768), (49, 768), (45, 768), (58, 768), (56, 768), (5, 768), (57, 768), (59, 768), (85, 768), (49, 768), (67, 768), (44, 768), (53, 768), (21, 768), (5, 768), (106, 768), (70, 768), (20, 768), (53, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.990528 MB\n",
      "sizes:[70656, 27648, 147456, 310272, 196608, 113664, 15360, 55296, 86016, 36864, 33792, 162816, 175104, 24576, 254976, 27648, 190464, 15360, 230400, 159744, 21504, 178176, 190464, 211968, 316416, 153600, 193536, 168960, 101376, 21504, 33792, 64512]\n",
      "[(23, 768), (9, 768), (48, 768), (101, 768), (64, 768), (37, 768), (5, 768), (18, 768), (28, 768), (12, 768), (11, 768), (53, 768), (57, 768), (8, 768), (83, 768), (9, 768), (62, 768), (5, 768), (75, 768), (52, 768), (7, 768), (58, 768), (62, 768), (69, 768), (103, 768), (50, 768), (63, 768), (55, 768), (33, 768), (7, 768), (11, 768), (21, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.66944 MB\n",
      "sizes:[104448, 239616, 24576, 46080, 156672, 156672, 172032, 36864, 104448, 52224, 18432, 245760, 150528, 147456, 362496, 58368, 125952, 301056, 175104, 119808, 49152, 89088, 92160, 251904, 181248, 76800, 70656, 147456, 193536, 193536, 337920, 187392]\n",
      "[(34, 768), (78, 768), (8, 768), (15, 768), (51, 768), (51, 768), (56, 768), (12, 768), (34, 768), (17, 768), (6, 768), (80, 768), (49, 768), (48, 768), (118, 768), (19, 768), (41, 768), (98, 768), (57, 768), (39, 768), (16, 768), (29, 768), (30, 768), (82, 768), (59, 768), (25, 768), (23, 768), (48, 768), (63, 768), (63, 768), (110, 768), (61, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.772416 MB\n",
      "sizes:[30720, 67584, 18432, 168960, 162816, 24576, 196608, 236544, 110592, 33792, 156672, 242688, 150528, 175104, 129024, 18432, 162816, 156672, 24576, 30720, 159744, 30720, 135168, 172032, 178176, 153600, 184320, 55296, 224256, 138240, 18432, 24576]\n",
      "[(10, 768), (22, 768), (6, 768), (55, 768), (53, 768), (8, 768), (64, 768), (77, 768), (36, 768), (11, 768), (51, 768), (79, 768), (49, 768), (57, 768), (42, 768), (6, 768), (53, 768), (51, 768), (8, 768), (10, 768), (52, 768), (10, 768), (44, 768), (56, 768), (58, 768), (50, 768), (60, 768), (18, 768), (73, 768), (45, 768), (6, 768), (8, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:0.79872 MB\n",
      "sizes:[55296, 27648, 64512, 76800, 15360, 24576, 58368, 30720, 288768, 156672]\n",
      "[(18, 768), (9, 768), (21, 768), (25, 768), (5, 768), (8, 768), (19, 768), (10, 768), (94, 768), (51, 768)]\n",
      "\n",
      "Epoch: 4 \t Training Loss: 0.392536\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.119552 MB\n",
      "sizes:[15360, 36864, 233472, 135168, 178176, 52224, 159744, 67584, 147456, 101376, 33792, 254976, 46080, 24576, 184320, 43008, 15360, 178176, 175104, 24576, 331776, 39936, 181248, 135168, 193536, 153600, 113664, 33792, 141312, 282624, 162816, 242688]\n",
      "[(5, 768), (12, 768), (76, 768), (44, 768), (58, 768), (17, 768), (52, 768), (22, 768), (48, 768), (33, 768), (11, 768), (83, 768), (15, 768), (8, 768), (60, 768), (14, 768), (5, 768), (58, 768), (57, 768), (8, 768), (108, 768), (13, 768), (59, 768), (44, 768), (63, 768), (50, 768), (37, 768), (11, 768), (46, 768), (92, 768), (53, 768), (79, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.442112 MB\n",
      "sizes:[156672, 193536, 24576, 52224, 236544, 251904, 153600, 18432, 92160, 67584, 159744, 288768, 172032, 144384, 187392, 18432, 304128, 178176, 172032, 141312, 162816, 70656, 184320, 27648, 104448, 181248, 215040, 104448, 101376, 15360, 18432, 242688]\n",
      "[(51, 768), (63, 768), (8, 768), (17, 768), (77, 768), (82, 768), (50, 768), (6, 768), (30, 768), (22, 768), (52, 768), (94, 768), (56, 768), (47, 768), (61, 768), (6, 768), (99, 768), (58, 768), (56, 768), (46, 768), (53, 768), (23, 768), (60, 768), (9, 768), (34, 768), (59, 768), (70, 768), (34, 768), (33, 768), (5, 768), (6, 768), (79, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:2.635776 MB\n",
      "sizes:[24576, 150528, 21504, 178176, 24576, 135168, 227328, 162816, 27648, 49152, 144384, 135168, 190464, 156672, 165888, 15360, 165888, 92160, 30720, 110592, 156672, 18432, 30720, 175104, 46080]\n",
      "[(8, 768), (49, 768), (7, 768), (58, 768), (8, 768), (44, 768), (74, 768), (53, 768), (9, 768), (16, 768), (47, 768), (44, 768), (62, 768), (51, 768), (54, 768), (5, 768), (54, 768), (30, 768), (10, 768), (36, 768), (51, 768), (6, 768), (10, 768), (57, 768), (15, 768)]\n",
      "\n",
      "y_pred: 0, y_true 7.0\n",
      "y_score\n",
      "tensor([0.0749, 0.0563, 0.1545, 0.2106, 0.0872, 0.1251, 0.1479, 0.1407, 0.0763,\n",
      "        0.1562, 0.1024, 0.1172, 0.1180, 0.0892, 0.1191, 0.0770, 0.0749, 0.1146,\n",
      "        0.0862, 0.0230, 0.1541, 0.1105, 0.1070, 0.0564, 0.2010, 0.1888, 0.1058,\n",
      "        0.0341, 0.1111, 0.1039, 0.1140, 0.1378, 0.0916, 0.1927, 0.0892, 0.0868,\n",
      "        0.1473, 0.1257, 0.0946, 0.0797, 0.0740, 0.0632, 0.1183, 0.1405, 0.1904,\n",
      "        0.1451, 0.1182, 0.0220, 0.1963, 0.0907, 0.1623, 0.1178, 0.1543, 0.0443,\n",
      "        0.0880, 0.0937, 0.1402, 0.1929, 0.1391, 0.0969, 0.0757, 0.0749, 0.0797,\n",
      "        0.1377, 0.0547, 0.0822, 0.0845, 0.0350, 0.0892, 0.1311, 0.3146, 0.1645,\n",
      "        0.0937, 0.1216, 0.0744, 0.1230, 0.0941, 0.0754, 0.1091, 0.0749, 0.2057,\n",
      "        0.0363, 0.0981, 0.0691, 0.2874, 0.0797, 0.0334, 0.0774, 0.1180])\n",
      "y_pred\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "y_true\n",
      "tensor([0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 1., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abot/sw/anaconda3/envs/cs598_dlh/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4 \t Validation p: 0.00, r:0.00, f: 0.00, roc_auc: 0.74\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.941376 MB\n",
      "sizes:[43008, 39936, 101376, 39936, 21504, 135168, 172032, 24576, 175104, 168960, 172032, 168960, 30720, 184320, 159744, 178176, 178176, 178176, 147456, 150528, 49152, 316416, 144384, 159744, 21504, 110592, 33792, 156672, 33792, 156672, 18432, 270336]\n",
      "[(14, 768), (13, 768), (33, 768), (13, 768), (7, 768), (44, 768), (56, 768), (8, 768), (57, 768), (55, 768), (56, 768), (55, 768), (10, 768), (60, 768), (52, 768), (58, 768), (58, 768), (58, 768), (48, 768), (49, 768), (16, 768), (103, 768), (47, 768), (52, 768), (7, 768), (36, 768), (11, 768), (51, 768), (11, 768), (51, 768), (6, 768), (88, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.941376 MB\n",
      "sizes:[153600, 156672, 52224, 24576, 27648, 156672, 147456, 30720, 27648, 159744, 159744, 156672, 218112, 193536, 110592, 181248, 21504, 168960, 61440, 150528, 208896, 55296, 153600, 175104, 36864, 190464, 67584, 175104, 181248, 147456, 15360, 175104]\n",
      "[(50, 768), (51, 768), (17, 768), (8, 768), (9, 768), (51, 768), (48, 768), (10, 768), (9, 768), (52, 768), (52, 768), (51, 768), (71, 768), (63, 768), (36, 768), (59, 768), (7, 768), (55, 768), (20, 768), (49, 768), (68, 768), (18, 768), (50, 768), (57, 768), (12, 768), (62, 768), (22, 768), (57, 768), (59, 768), (48, 768), (5, 768), (57, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.574208 MB\n",
      "sizes:[98304, 125952, 172032, 122880, 156672, 73728, 245760, 162816, 156672, 135168, 276480, 36864, 150528, 33792, 104448, 245760, 138240, 162816, 135168, 82944, 43008, 211968, 279552, 304128, 58368, 144384, 178176, 153600, 159744, 33792, 39936, 150528]\n",
      "[(32, 768), (41, 768), (56, 768), (40, 768), (51, 768), (24, 768), (80, 768), (53, 768), (51, 768), (44, 768), (90, 768), (12, 768), (49, 768), (11, 768), (34, 768), (80, 768), (45, 768), (53, 768), (44, 768), (27, 768), (14, 768), (69, 768), (91, 768), (99, 768), (19, 768), (47, 768), (58, 768), (50, 768), (52, 768), (11, 768), (13, 768), (49, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.926016 MB\n",
      "sizes:[316416, 264192, 153600, 15360, 55296, 89088, 24576, 144384, 215040, 218112, 15360, 175104, 15360, 150528, 18432, 162816, 55296, 39936, 187392, 153600, 43008, 36864, 181248, 153600, 172032, 24576, 236544, 98304, 141312, 147456, 86016, 135168]\n",
      "[(103, 768), (86, 768), (50, 768), (5, 768), (18, 768), (29, 768), (8, 768), (47, 768), (70, 768), (71, 768), (5, 768), (57, 768), (5, 768), (49, 768), (6, 768), (53, 768), (18, 768), (13, 768), (61, 768), (50, 768), (14, 768), (12, 768), (59, 768), (50, 768), (56, 768), (8, 768), (77, 768), (32, 768), (46, 768), (48, 768), (28, 768), (44, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.076544 MB\n",
      "sizes:[27648, 162816, 242688, 156672, 21504, 30720, 15360, 168960, 301056, 168960, 64512, 61440, 196608, 49152, 39936, 43008, 27648, 138240, 58368, 30720, 276480, 165888, 21504, 190464, 138240, 168960, 215040, 58368, 282624, 205824, 159744, 187392]\n",
      "[(9, 768), (53, 768), (79, 768), (51, 768), (7, 768), (10, 768), (5, 768), (55, 768), (98, 768), (55, 768), (21, 768), (20, 768), (64, 768), (16, 768), (13, 768), (14, 768), (9, 768), (45, 768), (19, 768), (10, 768), (90, 768), (54, 768), (7, 768), (62, 768), (45, 768), (55, 768), (70, 768), (19, 768), (92, 768), (67, 768), (52, 768), (61, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.380672 MB\n",
      "sizes:[24576, 162816, 55296, 24576, 162816, 279552, 156672, 184320, 264192, 156672, 184320, 64512, 125952, 147456, 24576, 162816, 147456, 175104, 168960, 153600, 202752, 233472, 224256, 27648, 205824, 147456, 162816, 55296, 58368, 162816, 36864, 36864]\n",
      "[(8, 768), (53, 768), (18, 768), (8, 768), (53, 768), (91, 768), (51, 768), (60, 768), (86, 768), (51, 768), (60, 768), (21, 768), (41, 768), (48, 768), (8, 768), (53, 768), (48, 768), (57, 768), (55, 768), (50, 768), (66, 768), (76, 768), (73, 768), (9, 768), (67, 768), (48, 768), (53, 768), (18, 768), (19, 768), (53, 768), (12, 768), (12, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.314688 MB\n",
      "sizes:[33792, 181248, 15360, 33792, 141312, 55296, 15360, 43008, 15360, 49152, 184320, 79872, 175104, 33792, 113664, 125952, 156672, 132096, 159744, 58368, 30720, 288768, 205824, 33792, 181248, 43008, 18432, 43008, 132096, 221184, 248832, 64512]\n",
      "[(11, 768), (59, 768), (5, 768), (11, 768), (46, 768), (18, 768), (5, 768), (14, 768), (5, 768), (16, 768), (60, 768), (26, 768), (57, 768), (11, 768), (37, 768), (41, 768), (51, 768), (43, 768), (52, 768), (19, 768), (10, 768), (94, 768), (67, 768), (11, 768), (59, 768), (14, 768), (6, 768), (14, 768), (43, 768), (72, 768), (81, 768), (21, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:5.013504 MB\n",
      "sizes:[33792, 190464, 144384, 159744, 107520, 95232, 337920, 325632, 162816, 104448, 159744, 193536, 267264, 18432, 162816, 273408, 301056, 46080, 36864, 116736, 92160, 301056, 202752, 211968, 138240, 18432, 144384, 27648, 98304, 82944, 233472, 224256]\n",
      "[(11, 768), (62, 768), (47, 768), (52, 768), (35, 768), (31, 768), (110, 768), (106, 768), (53, 768), (34, 768), (52, 768), (63, 768), (87, 768), (6, 768), (53, 768), (89, 768), (98, 768), (15, 768), (12, 768), (38, 768), (30, 768), (98, 768), (66, 768), (69, 768), (45, 768), (6, 768), (47, 768), (9, 768), (32, 768), (27, 768), (76, 768), (73, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.17792 MB\n",
      "sizes:[208896, 70656, 159744, 39936, 33792, 27648, 175104, 215040, 43008, 193536, 73728, 18432, 230400, 24576, 175104, 46080, 175104, 227328, 150528, 33792, 27648, 46080, 215040, 184320, 211968, 165888, 159744, 288768, 24576, 168960, 310272, 52224]\n",
      "[(68, 768), (23, 768), (52, 768), (13, 768), (11, 768), (9, 768), (57, 768), (70, 768), (14, 768), (63, 768), (24, 768), (6, 768), (75, 768), (8, 768), (57, 768), (15, 768), (57, 768), (74, 768), (49, 768), (11, 768), (9, 768), (15, 768), (70, 768), (60, 768), (69, 768), (54, 768), (52, 768), (94, 768), (8, 768), (55, 768), (101, 768), (17, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.803136 MB\n",
      "sizes:[21504, 116736, 86016, 129024, 261120, 18432, 239616, 18432, 178176, 175104, 156672, 33792, 141312, 138240, 24576, 153600, 18432, 55296, 36864, 190464, 82944, 187392, 33792, 95232, 153600, 159744, 187392, 236544, 55296, 159744, 92160, 165888]\n",
      "[(7, 768), (38, 768), (28, 768), (42, 768), (85, 768), (6, 768), (78, 768), (6, 768), (58, 768), (57, 768), (51, 768), (11, 768), (46, 768), (45, 768), (8, 768), (50, 768), (6, 768), (18, 768), (12, 768), (62, 768), (27, 768), (61, 768), (11, 768), (31, 768), (50, 768), (52, 768), (61, 768), (77, 768), (18, 768), (52, 768), (30, 768), (54, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.88608 MB\n",
      "sizes:[175104, 61440, 172032, 165888, 181248, 39936, 46080, 245760, 168960, 150528, 208896, 89088, 153600, 36864, 27648, 18432, 76800, 156672, 245760, 153600, 39936, 21504, 30720, 153600, 39936, 178176, 159744, 147456, 153600, 138240, 30720, 218112]\n",
      "[(57, 768), (20, 768), (56, 768), (54, 768), (59, 768), (13, 768), (15, 768), (80, 768), (55, 768), (49, 768), (68, 768), (29, 768), (50, 768), (12, 768), (9, 768), (6, 768), (25, 768), (51, 768), (80, 768), (50, 768), (13, 768), (7, 768), (10, 768), (50, 768), (13, 768), (58, 768), (52, 768), (48, 768), (50, 768), (45, 768), (10, 768), (71, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.11648 MB\n",
      "sizes:[165888, 153600, 30720, 199680, 190464, 18432, 175104, 125952, 159744, 202752, 46080, 181248, 165888, 58368, 39936, 156672, 172032, 196608, 27648, 135168, 73728, 52224, 30720, 18432, 125952, 150528, 239616, 73728, 175104, 227328, 86016, 261120]\n",
      "[(54, 768), (50, 768), (10, 768), (65, 768), (62, 768), (6, 768), (57, 768), (41, 768), (52, 768), (66, 768), (15, 768), (59, 768), (54, 768), (19, 768), (13, 768), (51, 768), (56, 768), (64, 768), (9, 768), (44, 768), (24, 768), (17, 768), (10, 768), (6, 768), (41, 768), (49, 768), (78, 768), (24, 768), (57, 768), (74, 768), (28, 768), (85, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.420608 MB\n",
      "sizes:[43008, 18432, 190464, 125952, 150528, 21504, 215040, 175104, 196608, 159744, 61440, 181248, 55296, 144384, 165888, 162816, 162816, 144384, 273408, 18432, 172032, 224256, 64512, 110592, 221184, 24576, 221184, 178176, 43008, 193536, 55296, 245760]\n",
      "[(14, 768), (6, 768), (62, 768), (41, 768), (49, 768), (7, 768), (70, 768), (57, 768), (64, 768), (52, 768), (20, 768), (59, 768), (18, 768), (47, 768), (54, 768), (53, 768), (53, 768), (47, 768), (89, 768), (6, 768), (56, 768), (73, 768), (21, 768), (36, 768), (72, 768), (8, 768), (72, 768), (58, 768), (14, 768), (63, 768), (18, 768), (80, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.780032 MB\n",
      "sizes:[135168, 156672, 205824, 236544, 236544, 30720, 168960, 153600, 18432, 150528, 172032, 129024, 175104, 337920, 18432, 208896, 172032, 15360, 193536, 165888, 147456, 122880, 27648, 178176, 153600, 165888, 156672, 362496, 24576, 215040, 36864, 107520]\n",
      "[(44, 768), (51, 768), (67, 768), (77, 768), (77, 768), (10, 768), (55, 768), (50, 768), (6, 768), (49, 768), (56, 768), (42, 768), (57, 768), (110, 768), (6, 768), (68, 768), (56, 768), (5, 768), (63, 768), (54, 768), (48, 768), (40, 768), (9, 768), (58, 768), (50, 768), (54, 768), (51, 768), (118, 768), (8, 768), (70, 768), (12, 768), (35, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.494336 MB\n",
      "sizes:[135168, 175104, 24576, 175104, 181248, 202752, 162816, 153600, 187392, 156672, 43008, 30720, 150528, 261120, 297984, 187392, 193536, 181248, 64512, 70656, 18432, 202752, 178176, 27648, 95232, 181248, 55296, 328704, 36864, 165888, 150528, 18432]\n",
      "[(44, 768), (57, 768), (8, 768), (57, 768), (59, 768), (66, 768), (53, 768), (50, 768), (61, 768), (51, 768), (14, 768), (10, 768), (49, 768), (85, 768), (97, 768), (61, 768), (63, 768), (59, 768), (21, 768), (23, 768), (6, 768), (66, 768), (58, 768), (9, 768), (31, 768), (59, 768), (18, 768), (107, 768), (12, 768), (54, 768), (49, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.987456 MB\n",
      "sizes:[181248, 21504, 46080, 193536, 79872, 43008, 168960, 113664, 239616, 129024, 70656, 30720, 156672, 159744, 172032, 168960, 18432, 86016, 104448, 276480, 141312, 153600, 236544, 138240, 21504, 168960, 224256, 199680, 147456, 52224, 24576, 18432]\n",
      "[(59, 768), (7, 768), (15, 768), (63, 768), (26, 768), (14, 768), (55, 768), (37, 768), (78, 768), (42, 768), (23, 768), (10, 768), (51, 768), (52, 768), (56, 768), (55, 768), (6, 768), (28, 768), (34, 768), (90, 768), (46, 768), (50, 768), (77, 768), (45, 768), (7, 768), (55, 768), (73, 768), (65, 768), (48, 768), (17, 768), (8, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.411392 MB\n",
      "sizes:[55296, 67584, 199680, 264192, 79872, 113664, 202752, 162816, 294912, 39936, 30720, 193536, 215040, 162816, 285696, 21504, 178176, 150528, 301056, 156672, 21504, 138240, 67584, 58368, 181248, 187392, 24576, 36864, 135168, 150528, 196608, 36864]\n",
      "[(18, 768), (22, 768), (65, 768), (86, 768), (26, 768), (37, 768), (66, 768), (53, 768), (96, 768), (13, 768), (10, 768), (63, 768), (70, 768), (53, 768), (93, 768), (7, 768), (58, 768), (49, 768), (98, 768), (51, 768), (7, 768), (45, 768), (22, 768), (19, 768), (59, 768), (61, 768), (8, 768), (12, 768), (44, 768), (49, 768), (64, 768), (12, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.337664 MB\n",
      "sizes:[15360, 181248, 159744, 110592, 15360, 21504, 242688, 184320, 110592, 150528, 156672, 64512, 199680, 64512, 24576, 187392, 30720, 150528, 156672, 184320, 172032, 141312, 58368, 215040, 193536, 92160, 18432, 215040, 175104, 261120, 353280, 30720]\n",
      "[(5, 768), (59, 768), (52, 768), (36, 768), (5, 768), (7, 768), (79, 768), (60, 768), (36, 768), (49, 768), (51, 768), (21, 768), (65, 768), (21, 768), (8, 768), (61, 768), (10, 768), (49, 768), (51, 768), (60, 768), (56, 768), (46, 768), (19, 768), (70, 768), (63, 768), (30, 768), (6, 768), (70, 768), (57, 768), (85, 768), (115, 768), (10, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.66944 MB\n",
      "sizes:[181248, 18432, 144384, 95232, 153600, 52224, 159744, 172032, 193536, 236544, 55296, 211968, 43008, 288768, 172032, 168960, 181248, 122880, 21504, 165888, 178176, 150528, 76800, 113664, 30720, 92160, 236544, 202752, 162816, 33792, 368640, 184320]\n",
      "[(59, 768), (6, 768), (47, 768), (31, 768), (50, 768), (17, 768), (52, 768), (56, 768), (63, 768), (77, 768), (18, 768), (69, 768), (14, 768), (94, 768), (56, 768), (55, 768), (59, 768), (40, 768), (7, 768), (54, 768), (58, 768), (49, 768), (25, 768), (37, 768), (10, 768), (30, 768), (77, 768), (66, 768), (53, 768), (11, 768), (120, 768), (60, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.334592 MB\n",
      "sizes:[18432, 55296, 30720, 95232, 208896, 202752, 86016, 144384, 156672, 251904, 172032, 113664, 245760, 36864, 113664, 27648, 267264, 33792, 39936, 150528, 181248, 254976, 141312, 156672, 172032, 168960, 236544, 153600, 294912, 64512, 39936, 18432]\n",
      "[(6, 768), (18, 768), (10, 768), (31, 768), (68, 768), (66, 768), (28, 768), (47, 768), (51, 768), (82, 768), (56, 768), (37, 768), (80, 768), (12, 768), (37, 768), (9, 768), (87, 768), (11, 768), (13, 768), (49, 768), (59, 768), (83, 768), (46, 768), (51, 768), (56, 768), (55, 768), (77, 768), (50, 768), (96, 768), (21, 768), (13, 768), (6, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.687872 MB\n",
      "sizes:[79872, 162816, 153600, 18432, 33792, 258048, 190464, 122880, 156672, 165888, 162816, 122880, 288768, 168960, 172032, 175104, 172032, 178176, 184320, 141312, 218112, 156672, 230400, 162816, 43008, 175104, 76800, 18432, 165888, 24576, 168960, 138240]\n",
      "[(26, 768), (53, 768), (50, 768), (6, 768), (11, 768), (84, 768), (62, 768), (40, 768), (51, 768), (54, 768), (53, 768), (40, 768), (94, 768), (55, 768), (56, 768), (57, 768), (56, 768), (58, 768), (60, 768), (46, 768), (71, 768), (51, 768), (75, 768), (53, 768), (14, 768), (57, 768), (25, 768), (6, 768), (54, 768), (8, 768), (55, 768), (45, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.867648 MB\n",
      "sizes:[153600, 172032, 347136, 150528, 172032, 39936, 21504, 119808, 110592, 30720, 175104, 18432, 18432, 30720, 168960, 162816, 70656, 159744, 144384, 172032, 156672, 15360, 147456, 181248, 36864, 138240, 153600, 27648, 187392, 159744, 159744, 64512]\n",
      "[(50, 768), (56, 768), (113, 768), (49, 768), (56, 768), (13, 768), (7, 768), (39, 768), (36, 768), (10, 768), (57, 768), (6, 768), (6, 768), (10, 768), (55, 768), (53, 768), (23, 768), (52, 768), (47, 768), (56, 768), (51, 768), (5, 768), (48, 768), (59, 768), (12, 768), (45, 768), (50, 768), (9, 768), (61, 768), (52, 768), (52, 768), (21, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:1.293312 MB\n",
      "sizes:[368640, 181248, 190464, 21504, 46080, 162816, 15360, 15360, 138240, 153600]\n",
      "[(120, 768), (59, 768), (62, 768), (7, 768), (15, 768), (53, 768), (5, 768), (5, 768), (45, 768), (50, 768)]\n",
      "\n",
      "Epoch: 5 \t Training Loss: 0.396266\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.125696 MB\n",
      "sizes:[52224, 36864, 251904, 70656, 159744, 15360, 46080, 24576, 18432, 165888, 18432, 18432, 67584, 227328, 178176, 172032, 178176, 153600, 24576, 178176, 147456, 187392, 156672, 24576, 175104, 184320, 156672, 304128, 254976, 27648, 165888, 282624]\n",
      "[(17, 768), (12, 768), (82, 768), (23, 768), (52, 768), (5, 768), (15, 768), (8, 768), (6, 768), (54, 768), (6, 768), (6, 768), (22, 768), (74, 768), (58, 768), (56, 768), (58, 768), (50, 768), (8, 768), (58, 768), (48, 768), (61, 768), (51, 768), (8, 768), (57, 768), (60, 768), (51, 768), (99, 768), (83, 768), (9, 768), (54, 768), (92, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.861504 MB\n",
      "sizes:[162816, 110592, 33792, 193536, 141312, 175104, 24576, 153600, 33792, 46080, 30720, 15360, 30720, 135168, 18432, 144384, 178176, 141312, 242688, 190464, 172032, 331776, 15360, 113664, 144384, 150528, 15360, 215040, 101376, 52224, 104448, 242688]\n",
      "[(53, 768), (36, 768), (11, 768), (63, 768), (46, 768), (57, 768), (8, 768), (50, 768), (11, 768), (15, 768), (10, 768), (5, 768), (10, 768), (44, 768), (6, 768), (47, 768), (58, 768), (46, 768), (79, 768), (62, 768), (56, 768), (108, 768), (5, 768), (37, 768), (47, 768), (49, 768), (5, 768), (70, 768), (33, 768), (17, 768), (34, 768), (79, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.21024 MB\n",
      "sizes:[39936, 233472, 236544, 288768, 92160, 92160, 49152, 156672, 43008, 181248, 135168, 135168, 67584, 21504, 193536, 135168, 162816, 24576, 159744, 184320, 181248, 104448, 162816, 101376, 27648]\n",
      "[(13, 768), (76, 768), (77, 768), (94, 768), (30, 768), (30, 768), (16, 768), (51, 768), (14, 768), (59, 768), (44, 768), (44, 768), (22, 768), (7, 768), (63, 768), (44, 768), (53, 768), (8, 768), (52, 768), (60, 768), (59, 768), (34, 768), (53, 768), (33, 768), (9, 768)]\n",
      "\n",
      "y_pred: 0, y_true 7.0\n",
      "y_score\n",
      "tensor([0.1858, 0.0479, 0.0723, 0.0247, 0.0789, 0.1054, 0.1756, 0.0439, 0.1138,\n",
      "        0.0646, 0.0181, 0.1138, 0.2070, 0.2382, 0.0305, 0.0978, 0.0573, 0.0651,\n",
      "        0.1298, 0.0613, 0.0599, 0.0758, 0.1854, 0.1298, 0.0546, 0.0733, 0.0702,\n",
      "        0.1644, 0.0706, 0.1373, 0.1481, 0.0693, 0.0793, 0.0386, 0.1514, 0.1276,\n",
      "        0.0719, 0.0475, 0.1298, 0.1200, 0.0249, 0.1756, 0.0245, 0.1054, 0.1445,\n",
      "        0.0364, 0.1138, 0.0473, 0.0698, 0.0729, 0.0825, 0.0806, 0.1262, 0.0905,\n",
      "        0.1054, 0.0654, 0.1102, 0.0573, 0.1054, 0.0856, 0.0547, 0.0638, 0.0598,\n",
      "        0.0828, 0.1641, 0.0954, 0.0967, 0.0879, 0.0298, 0.0447, 0.1808, 0.0478,\n",
      "        0.0608, 0.1350, 0.1464, 0.0924, 0.0329, 0.1219, 0.1603, 0.0880, 0.1130,\n",
      "        0.0185, 0.1197, 0.0745, 0.0705, 0.0908, 0.1091, 0.1031, 0.1373])\n",
      "y_pred\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "y_true\n",
      "tensor([1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 1., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0.])\n",
      "\n",
      "Epoch: 5 \t Validation p: 0.00, r:0.00, f: 0.00, roc_auc: 0.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abot/sw/anaconda3/envs/cs598_dlh/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "train(mort_desc_naive_rnn, mort_demb_train_loader, mort_demb_val_loader,\n",
    "      n_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "41ce9a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertCollate tensor sizes\n",
      "cum_size:3.999744 MB\n",
      "sizes:[101376, 193536, 242688, 39936, 150528, 36864, 162816, 52224, 104448, 156672, 254976, 24576, 159744, 18432, 15360, 92160, 162816, 43008, 101376, 18432, 187392, 288768, 184320, 135168, 27648, 33792, 144384, 175104, 15360, 233472, 227328, 215040]\n",
      "[(33, 768), (63, 768), (79, 768), (13, 768), (49, 768), (12, 768), (53, 768), (17, 768), (34, 768), (51, 768), (83, 768), (8, 768), (52, 768), (6, 768), (5, 768), (30, 768), (53, 768), (14, 768), (33, 768), (6, 768), (61, 768), (94, 768), (60, 768), (44, 768), (9, 768), (11, 768), (47, 768), (57, 768), (5, 768), (76, 768), (74, 768), (70, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.00896 MB\n",
      "sizes:[113664, 190464, 175104, 162816, 27648, 181248, 30720, 135168, 141312, 18432, 165888, 110592, 15360, 46080, 153600, 331776, 236544, 52224, 135168, 92160, 24576, 172032, 178176, 67584, 181248, 24576, 242688, 67584, 46080, 178176, 156672, 153600]\n",
      "[(37, 768), (62, 768), (57, 768), (53, 768), (9, 768), (59, 768), (10, 768), (44, 768), (46, 768), (6, 768), (54, 768), (36, 768), (5, 768), (15, 768), (50, 768), (108, 768), (77, 768), (17, 768), (44, 768), (30, 768), (8, 768), (56, 768), (58, 768), (22, 768), (59, 768), (8, 768), (79, 768), (22, 768), (15, 768), (58, 768), (51, 768), (50, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.188736 MB\n",
      "sizes:[33792, 165888, 24576, 49152, 147456, 184320, 135168, 18432, 172032, 304128, 251904, 282624, 156672, 15360, 141312, 144384, 70656, 21504, 24576, 104448, 178176, 193536, 30720, 178176, 159744]\n",
      "[(11, 768), (54, 768), (8, 768), (16, 768), (48, 768), (60, 768), (44, 768), (6, 768), (56, 768), (99, 768), (82, 768), (92, 768), (51, 768), (5, 768), (46, 768), (47, 768), (23, 768), (7, 768), (8, 768), (34, 768), (58, 768), (63, 768), (10, 768), (58, 768), (52, 768)]\n",
      "\n",
      "y_pred: 0, y_true 7.0\n",
      "y_score\n",
      "tensor([0.1031, 0.1276, 0.0825, 0.1641, 0.0573, 0.0479, 0.0793, 0.1858, 0.0908,\n",
      "        0.0702, 0.0706, 0.0439, 0.1197, 0.1138, 0.1054, 0.0298, 0.1091, 0.0608,\n",
      "        0.0547, 0.0181, 0.0758, 0.0879, 0.0745, 0.0364, 0.1373, 0.1514, 0.1102,\n",
      "        0.0546, 0.1054, 0.0954, 0.2382, 0.0856, 0.0654, 0.0806, 0.0475, 0.1130,\n",
      "        0.1373, 0.1350, 0.0245, 0.0924, 0.0719, 0.1138, 0.0646, 0.0386, 0.1054,\n",
      "        0.1756, 0.0651, 0.0905, 0.0967, 0.0638, 0.0880, 0.0447, 0.1298, 0.0978,\n",
      "        0.0613, 0.0329, 0.0705, 0.0185, 0.0828, 0.2070, 0.1756, 0.0573, 0.1854,\n",
      "        0.1200, 0.0249, 0.1481, 0.1298, 0.1808, 0.0599, 0.0733, 0.1464, 0.1138,\n",
      "        0.1262, 0.1644, 0.0723, 0.0693, 0.0478, 0.1054, 0.0729, 0.0473, 0.0247,\n",
      "        0.1219, 0.1298, 0.0598, 0.0698, 0.1603, 0.1445, 0.0305, 0.0789])\n",
      "y_pred\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "y_true\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 1., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abot/sw/anaconda3/envs/cs598_dlh/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p, r, f, roc_auc, rcurve = eval_model(mort_desc_naive_rnn, mort_demb_val_loader)\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "8341a4ae-e258-43ad-a0b1-ff9af60b18b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertCollate tensor sizes\n",
      "cum_size:4.509696 MB\n",
      "sizes:[76800, 36864, 18432, 156672, 43008, 147456, 190464, 36864, 181248, 245760, 168960, 208896, 273408, 245760, 156672, 156672, 159744, 325632, 159744, 159744, 36864, 43008, 18432, 30720, 27648, 55296, 331776, 153600, 221184, 187392, 98304, 156672]\n",
      "[(25, 768), (12, 768), (6, 768), (51, 768), (14, 768), (48, 768), (62, 768), (12, 768), (59, 768), (80, 768), (55, 768), (68, 768), (89, 768), (80, 768), (51, 768), (51, 768), (52, 768), (106, 768), (52, 768), (52, 768), (12, 768), (14, 768), (6, 768), (10, 768), (9, 768), (18, 768), (108, 768), (50, 768), (72, 768), (61, 768), (32, 768), (51, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:4.273152 MB\n",
      "sizes:[49152, 273408, 190464, 125952, 46080, 208896, 193536, 15360, 95232, 175104, 27648, 30720, 193536, 294912, 73728, 49152, 30720, 36864, 208896, 116736, 33792, 147456, 156672, 153600, 294912, 291840, 52224, 248832, 175104, 27648, 39936, 215040]\n",
      "[(16, 768), (89, 768), (62, 768), (41, 768), (15, 768), (68, 768), (63, 768), (5, 768), (31, 768), (57, 768), (9, 768), (10, 768), (63, 768), (96, 768), (24, 768), (16, 768), (10, 768), (12, 768), (68, 768), (38, 768), (11, 768), (48, 768), (51, 768), (50, 768), (96, 768), (95, 768), (17, 768), (81, 768), (57, 768), (9, 768), (13, 768), (70, 768)]\n",
      "\n",
      "BertCollate tensor sizes\n",
      "cum_size:3.376128 MB\n",
      "sizes:[211968, 67584, 55296, 282624, 150528, 15360, 199680, 95232, 165888, 49152, 254976, 39936, 181248, 135168, 168960, 147456, 239616, 70656, 30720, 21504, 165888, 46080, 199680, 147456, 95232, 138240]\n",
      "[(69, 768), (22, 768), (18, 768), (92, 768), (49, 768), (5, 768), (65, 768), (31, 768), (54, 768), (16, 768), (83, 768), (13, 768), (59, 768), (44, 768), (55, 768), (48, 768), (78, 768), (23, 768), (10, 768), (7, 768), (54, 768), (15, 768), (65, 768), (48, 768), (31, 768), (45, 768)]\n",
      "\n",
      "y_pred: 0, y_true 8.0\n",
      "y_score\n",
      "tensor([0.0618, 0.1579, 0.1138, 0.0429, 0.0261, 0.0510, 0.2315, 0.1579, 0.1547,\n",
      "        0.0779, 0.0595, 0.0837, 0.0910, 0.0869, 0.0325, 0.0960, 0.0869, 0.0694,\n",
      "        0.0546, 0.0966, 0.1579, 0.1700, 0.0234, 0.1445, 0.0209, 0.1906, 0.1399,\n",
      "        0.0571, 0.1798, 0.3390, 0.0821, 0.1675, 0.1808, 0.0807, 0.1113, 0.0678,\n",
      "        0.0266, 0.1241, 0.0930, 0.1054, 0.0561, 0.1381, 0.1373, 0.1445, 0.0313,\n",
      "        0.1196, 0.0782, 0.1808, 0.0467, 0.0469, 0.0861, 0.0497, 0.0249, 0.0747,\n",
      "        0.0882, 0.0724, 0.1242, 0.2198, 0.1858, 0.1218, 0.0994, 0.1373, 0.0447,\n",
      "        0.0562, 0.0647, 0.0964, 0.1906, 0.1319, 0.0612, 0.1054, 0.1106, 0.0370,\n",
      "        0.0754, 0.1808, 0.0786, 0.1641, 0.0682, 0.0664, 0.0628, 0.1062, 0.2724,\n",
      "        0.0747, 0.1445, 0.0183, 0.2495, 0.1756, 0.1448, 0.0756, 0.0444, 0.0538])\n",
      "y_pred\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "y_true\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 1.])\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/abot/sw/anaconda3/envs/cs598_dlh/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p, r, f, roc_auc, rcurve = eval_model(mort_desc_naive_rnn, mort_demb_test_loader)\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2c2d9-c161-4b9d-8182-86384faea929",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0edec-1570-4640-9926-d908b456f957",
   "metadata": {},
   "source": [
    "See instructions here: \n",
    "- https://pypi.org/project/pytorch-pretrained-bert/#examples\n",
    "- https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n",
    "- Could also get it from pytorch transformers library: https://pytorch.org/hub/huggingface_pytorch-transformers/\n",
    "- https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/\n",
    "- https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html - general text embedding.\n",
    "- https://machinelearningmastery.com/handle-long-sequences-long-short-term-memory-recurrent-neural-networks/ handling long seq\n",
    "- Encoder/Decoder training for embedding vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d58de-bf9a-4b57-977f-6aa3ac75e84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n",
    "class BERTClassification(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(BERTClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.bert_drop = nn.Dropout(0.4)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, pooledOut = self.bert(ids, attention_mask = mask,\n",
    "                                token_type_ids=token_type_ids)\n",
    "        bertOut = self.bert_drop(pooledOut)\n",
    "        output = self.out(bertOut)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40665f4-db92-4567-b0f9-001bd35e9284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# TODO(botelho3)`bert-base-uncased` is big. Load `bert-tiny` instead from the filesystem?\n",
    "# Model available at https://huggingface.co/prajjwal1/bert-tiny.\n",
    "# model = BERT_CLASS.from_pretrained(PRE_TRAINED_MODEL_NAME_OR_PATH, cache_dir=None)\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7539b72-a871-4d52-947b-075b5c4be2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de132943-2c3e-405b-b2f8-5d348472672c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if USE_GPU_:\n",
    "    tokens_tensor = tokens_tensor.to(GPU_STRING_)\n",
    "    segments_tensors = segments_tensors.to(GPU_STRING_)\n",
    "    model.to(GPU_STRING_)\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efb18a-626d-42d4-b19d-ba4b8f752a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Dataset Transforms to overwrite the old Dataset with a new transformed dataset.\n",
    "# Can either construct a new class BertDataset(Dataset): __init__(self, old_dataset)\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "# Or can leverage transfomrs\n",
    "# class Rescale(object): __init__(self) -> class BertTransform() __init__(bert_model), __call__(self, sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccba6af-1867-491d-b9d0-d99965738c64",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5da368cd-3045-4ec0-86fb-2ce15b5d1b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 4 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# General includes.\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "#from termcolor import colored, cprint\n",
    "import colored\n",
    "from datetime import datetime, timedelta\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Typing includes.\n",
    "from typing import Dict, List, Optional, Any\n",
    "\n",
    "# Numerical includes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# pyHealth includes.\n",
    "from pyhealth.datasets import BaseDataset, MIMIC3Dataset, SampleDataset , split_by_patient\n",
    "from pyhealth.data import Patient, Visit, Event\n",
    "from pyhealth.data import Event, Visit, Patient\n",
    "from pyhealth.datasets.utils import strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf9239f-3735-4b98-bad9-bc2f5415fd8c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73747987-b662-43cb-8374-1aa511477205",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "95e714b1-ca5d-4d7b-9ace-6b4372adfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertTokenizerFast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83fa50-9dd0-467b-bd36-634795e4a09c",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ebef3528-0396-459a-8112-b272089f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_GPU_ = False\n",
    "GPU_STRING_ = 'cuda'\n",
    "DATA_DIR_ = '../data_input_path/mimic'\n",
    "BATCH_SIZE_ = 32\n",
    "EMBEDDING_DIM_ = 264  # BERT requires a multiple of 12\n",
    "SHUFFLE_ = True\n",
    "SAMPLE_MULTIPLIER_ = 3\n",
    "\n",
    "# TBD\n",
    "CACHE_DIR_ 'cache'\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "seed = 90210\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f8271-44b2-44a2-b7cc-cd7339a70e87",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02624f53-6cee-4db0-8b15-e08b755a04f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8f0d5fa9-226e-4612-b28f-ed24de16399a",
   "metadata": {},
   "source": [
    "### Load MIMIC III Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f55b3bf1-cb66-4d55-9b56-b314e958664b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "607ef1d3-caa8-4db8-b664-86172bea936d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Malignant ascites\n",
      "78951 ancestors ['789.5', '789', '780-789.99', '780-799.99', '001-999.99']\n",
      "Ascites\n",
      "7895 ancestors ['789', '780-789.99', '780-799.99', '001-999.99']\n",
      "Abdominal rigidity\n",
      "7894 ancestors ['789', '780-789.99', '780-799.99', '001-999.99']\n",
      "Abdominal rigidity, right upper quadrant\n",
      "78941 ancestors ['789.4', '789', '780-789.99', '780-799.99', '001-999.99']\n",
      "789.51\n",
      "789.5\n"
     ]
    }
   ],
   "source": [
    "from pyhealth.medcode import InnerMap, ICD9CM\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd9cm.lookup(\"428.0\") # get detailed info\n",
    "icd9cm.get_ancestors(\"428.0\") # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"78951\")) # get detailed info\n",
    "print(f'78951 ancestors {icd9cm.get_ancestors(\"78951\")}') # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"7895\")) # get detailed info\n",
    "print(f'7895 ancestors {icd9cm.get_ancestors(\"7895\")}') # get parents\n",
    "\n",
    "\n",
    "print(icd9cm.lookup(\"7894\")) # get detailed info\n",
    "print(f'7894 ancestors {icd9cm.get_ancestors(\"7894\")}') # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"78941\")) # get detailed info\n",
    "print(f'78941 ancestors {icd9cm.get_ancestors(\"78941\")}') # get parents\n",
    "\n",
    "print(ICD9CM.standardize('78951'))\n",
    "print(ICD9CM.standardize('7895'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a56f2f-8f6d-4ca9-8c46-5dd80e564d4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9e11ff-3f94-4339-ae4c-86d8bc1675ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b10ca4-58f4-436f-addf-7a103dfcec67",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706834a9-029c-4036-a3ed-954208fc0f32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06839339-48f1-42dc-bb0d-5da9b03e491e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641e77b2-ded0-4df9-811a-93e9f7448f0f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f4ea87-e0ee-402c-a8d7-0d31be602e44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6adb784d-4169-4325-9b9a-64ed8e741d5f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23b5287-6771-4f6b-943b-29576fc8704f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf00442-936a-4620-9ce5-b93f1c800f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "dcfb3443-8dfa-4149-904b-04fc24d7a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_duration_minutes(start_datetime: str, end_datetime: str) -> float:\n",
    "    '''Return duration in minutes as a float.\n",
    "    '''\n",
    "    # MIMIC-III uses the following format: 2146-07-22 00:00:00\n",
    "    start = datetime.strptime(start_datetime, '%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strptime(start_datetime,   '%Y-%m-%d %H:%M:%S')\n",
    "    return float((end - start).seconds)\n",
    "\n",
    "class MIMIC3DatasetWrapper(MIMIC3Dataset):\n",
    "    ''' Add extra tables to the MIMIC III dataset.\n",
    "    \n",
    "      Some of the tables we need like \"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\"\n",
    "      are not supported out of the box. \n",
    "      \n",
    "      This class defines parsing methods to extract text data from these extra tables.\n",
    "      The text data is generally joined on the PATIENTID, HADMID, ITEMID to match the\n",
    "      pyHealth Vists class representation.\n",
    "    '''\n",
    "   \n",
    "    # We need to add storage for text-based lookup tables here.\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._valid_text_tables = [\"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\", \"D_LABITEMS\"]\n",
    "        self._text_descriptions = {x: {} for x in self._valid_text_tables}\n",
    "        self._text_luts = {x: {} for x in self._valid_text_tables}\n",
    "        super().__init__(*args, **kwargs)\n",
    "    \n",
    "    def get_all_tables(self) -> List[str]: \n",
    "        return list(self._text_descriptions.keys())\n",
    "        \n",
    "    def get_text_dict(self, table_name: str) -> Dict[str, Dict[Any, Any]]:\n",
    "        return self._text_descriptions.get(table_name)\n",
    "    \n",
    "    def set_text_lut(self, table_name: str, lut: Dict[Any, Any]) -> None:\n",
    "        self._text_luts[table_name] = lut\n",
    "    \n",
    "    def get_text_lut(self, table_name: str) -> Dict[Any, Any]:\n",
    "        return self._text_luts[table_name]\n",
    "    \n",
    "#     def _validate(self) -> Dict:\n",
    "#         \"\"\"Helper function which validates the samples.\n",
    "#         Will be called in `self.__init__()`.\n",
    "#         Returns:\n",
    "#             input_info: Dict, a dict whose keys are the same as the keys in the\n",
    "#                 samples, and values are the corresponding input information:\n",
    "#                 - \"type\": the element type of each key attribute, one of float,\n",
    "#                     int, str.\n",
    "#                 - \"dim\": the list dimension of each key attribute, one of 0, 1, 2, 3.\n",
    "#                 - \"len\": the length of the vector, only valid for vector-based\n",
    "#                     attributes.\n",
    "#         \"\"\"\n",
    "#         \"\"\" 1. Check if all samples are of type dict. \"\"\"\n",
    "#         assert all(\n",
    "#             [isinstance(s, dict) for s in self.samples],\n",
    "#         ), \"Each sample should be a dict\"\n",
    "#         keys = self.samples[0].keys()\n",
    "\n",
    "#         \"\"\" 2. Check if all samples have the same keys. \"\"\"\n",
    "#         assert all(\n",
    "#             [set(s.keys()) == set(keys) for s in self.samples]\n",
    "#         ), \"All samples should have the same keys\"\n",
    "\n",
    "#         \"\"\" 3. Check if \"patient_id\" and \"visit_id\" are in the keys.\"\"\"\n",
    "#         assert \"patient_id\" in keys, \"patient_id should be in the keys\"\n",
    "#         assert \"visit_id\" in keys, \"visit_id should be in the keys\"\n",
    "#         return {}\n",
    "\n",
    "    \n",
    "    def _add_events_to_patient_dict(\n",
    "        self,\n",
    "        patient_dict: Dict[str, Patient],\n",
    "        group_df: pd.DataFrame,\n",
    "    ) -> Dict[str, Patient]:\n",
    "        #TODO(botelho3) Imported from PyHealth Base dataset githubf to\n",
    "        #support parse_prescription\n",
    "        \"\"\"Helper function which adds the events column of a df.groupby object to the patient dict.\n",
    "        \n",
    "        Will be called at the end of each `self.parse_[table_name]()` function.\n",
    "        Args:\n",
    "            patient_dict: a dict mapping patient_id to `Patient` object.\n",
    "            group_df: a df.groupby object, having two columns: patient_id and events.\n",
    "                - the patient_id column is the index of the patient\n",
    "                - the events column is a list of <Event> objects\n",
    "        Returns:\n",
    "            The updated patient dict.\n",
    "        \"\"\"\n",
    "        for _, events in group_df.items():\n",
    "            for event in events:\n",
    "                patient_dict = self._add_event_to_patient_dict(patient_dict, event)\n",
    "        return patient_dict\n",
    "\n",
    "    \n",
    "    def parse_prescriptions(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:\n",
    "        \"\"\"Helper function which parses PRESCRIPTIONS table.\n",
    "        \n",
    "        TODO(botelho3) - we have to override this to include the text fields. The\n",
    "        prescriptions table does not link to a separate D_ICD_* table in MIMIC-III\n",
    "        thtat contains text descriptions of the prescription. The text descriptions\n",
    "        are in the columns of this table. Regular pyHealth ignores these columns. We\n",
    "        override this method to appent pyHealth Event objects containing the text\n",
    "        columns to each patient.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - PRESCRIPTIONS: https://mimic.mit.edu/docs/iii/tables/prescriptions/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The updated patients dict.\n",
    "        \"\"\"\n",
    "        table = \"PRESCRIPTIONS\"\n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            low_memory=False,\n",
    "            dtype={\"SUBJECT_ID\": str, \"HADM_ID\": str, \"NDC\": str,\n",
    "                   \"DRUG_TYPE\": str, \"DRUG\": str,\n",
    "                   \"PROD_STRENGTH\": str, \"ROUTE\": str, \"ENDDATE\": str},\n",
    "        )\n",
    "        # drop records of the other patients\n",
    "        df = df[df[\"SUBJECT_ID\"].isin(patients.keys())]\n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"SUBJECT_ID\", \"HADM_ID\", \"NDC\", \"DRUG_TYPE\", \"DRUG\"])\n",
    "        # sort by start date and end date\n",
    "        df = df.sort_values(\n",
    "            [\"SUBJECT_ID\", \"HADM_ID\", \"STARTDATE\", \"ENDDATE\"], ascending=True\n",
    "        )\n",
    "        # group by patient and visit\n",
    "        group_df = df.groupby(\"SUBJECT_ID\")\n",
    "        \n",
    "        # parallel unit for prescription (per patient)\n",
    "        def prescription_unit(p_id, p_info):\n",
    "            events = []\n",
    "            for v_id, v_info in p_info.groupby(\"HADM_ID\"):\n",
    "                zipped = zip(v_info[\"STARTDATE\"], v_info[\"NDC\"], v_info[\"DRUG_TYPE\"],\n",
    "                             v_info[\"DRUG\"], v_info[\"PROD_STRENGTH\"], v_info[\"ROUTE\"],\n",
    "                             v_info[\"ENDDATE\"])\n",
    "                for timestamp, code, dtype, dname, dose, route, endtime in zipped:\n",
    "                    assert(type(dname) is str)\n",
    "                    event = Event(\n",
    "                        code=code,\n",
    "                        table=table,\n",
    "                        vocabulary=\"NDC\",\n",
    "                        visit_id=v_id,\n",
    "                        patient_id=p_id,\n",
    "                        timestamp=strptime(timestamp),\n",
    "                        dtype=dtype,\n",
    "                        dname=dname,\n",
    "                        dose=dose,\n",
    "                        route=route,\n",
    "                        duration=_compute_duration_minutes(timestamp, endtime),\n",
    "                    )\n",
    "                    events.append(event)\n",
    "            return events\n",
    "\n",
    "                # parallel apply\n",
    "        group_df = group_df.parallel_apply(\n",
    "            lambda x: prescription_unit(x.SUBJECT_ID.unique()[0], x)\n",
    "        )\n",
    "\n",
    "        # summarize the results\n",
    "        # print(f'BASE {dir(BaseDataset)}')\n",
    "        # print(f'MIMIC {dir(MIMIC3Dataset)}')\n",
    "        # print(f'WRAPPER {dir(self)}')\n",
    "        # # patients = BaseDataset._add_events_to_patient_dict(patients, group_df)\n",
    "        # # patients = self._add_events_to_patient_dict(patients, group_df)\n",
    "        patients = self._add_events_to_patient_dict(patients, group_df)\n",
    "        return patients\n",
    "    \n",
    "    # Note the name has to match the table name exactly.\n",
    "    # See https://github.com/sunlabuiuc/PyHealth/blob/master/pyhealth/datasets/mimic3.py#L71.\n",
    "    def parse_d_icd_diagnoses(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_DIAGNOSIS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_DIAGNOSIS: https://mimic.mit.edu/docs/iii/tables/d_icd_diagnoses/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_DIAGNOSES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text diagnosis description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_DIAGNOSES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    def parse_d_labitems(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_LABITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_LABITEMS: https://mimic.mit.edu/docs/iii/tables/d_labitems/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_LABITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text lab measurement description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_LABITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str, \"FLUID\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_items(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        # TODO(botelho3) - Note this may not be totally useable because the ITEMID\n",
    "        # uinqiue key only links to these tables using ITEMID\n",
    "        #   - INPUTEVENTS_MV \n",
    "        #   - OUTPUTEVENTS on ITEMID\n",
    "        #   - PROCEDUREEVENTS_MV on ITEMID\n",
    "        # \n",
    "        # Not to the tables we want e.g. \n",
    "        \"\"\"Helper function which parses D_ITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ITEMS: https://mimic.mit.edu/docs/iii/tables/d_items/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text inputs/output/procedure events lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_icd_procedures(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_PROCEDURES table.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_PROCEDURES: https://mimic.mit.edu/docs/iii/tables/d_icd_procedures/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_PROCEDURES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text procedure description lookup for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_PROCEDURES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "427a42a6-441e-438c-96f4-b38ba82fd192",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading data from: `/home/abot/cs598dlh/github_mirror_cs598dlh/cs598_desc_emb_project/src/../data_input_path/mimic`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing PATIENTS and ADMISSIONS: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 776.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing D_ICD_DIAGNOSES\n",
      "Parsing D_ICD_PROCEDURES\n",
      "Parsing D_ITEMS\n",
      "Parsing D_LABITEMS\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Parsing DIAGNOSES_ICD: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 129/129 [00:00<00:00, 10003.24it/s]\n",
      "Parsing PROCEDURES_ICD: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 113/113 [00:00<00:00, 11509.38it/s]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6d3af8526fe404b9d0052f9d7c1de89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=24), Label(value='0 / 24'))), HBox…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Mapping codes: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 346.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Statistics of base dataset (dev=False):\n",
      "\t- Dataset: MIMIC3DatasetWrapper\n",
      "\t- Number of patients: 100\n",
      "\t- Number of visits: 129\n",
      "\t- Number of visits per patient: 1.2900\n",
      "\t- Number of events per visit in D_ICD_DIAGNOSES: 0.0000\n",
      "\t- Number of events per visit in D_ICD_PROCEDURES: 0.0000\n",
      "\t- Number of events per visit in D_ITEMS: 0.0000\n",
      "\t- Number of events per visit in D_LABITEMS: 0.0000\n",
      "\t- Number of events per visit in DIAGNOSES_ICD: 13.6512\n",
      "\t- Number of events per visit in PROCEDURES_ICD: 3.9225\n",
      "\t- Number of events per visit in PRESCRIPTIONS: 115.6667\n",
      "\n",
      "\n",
      "dataset.patients: patient_id -> <Patient>\n",
      "\n",
      "<Patient>\n",
      "    - visits: visit_id -> <Visit> \n",
      "    - other patient-level info\n",
      "    \n",
      "    <Visit>\n",
      "        - event_list_dict: table_name -> List[Event]\n",
      "        - other visit-level info\n",
      "    \n",
      "        <Event>\n",
      "            - code: str\n",
      "            - other event-level info\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f'Reading data from: `{os.path.join(os.getcwd(), DATA_DIR_)}`')\n",
    "\n",
    "mimic3base = MIMIC3DatasetWrapper(\n",
    "    # root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
    "    root=os.path.join(os.getcwd(), DATA_DIR_),\n",
    "    tables=[\"D_ICD_DIAGNOSES\", \"D_ICD_PROCEDURES\", \"D_ITEMS\", \"D_LABITEMS\",\n",
    "            \"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"], # \"LABEVENTS\"],\n",
    "    # map all NDC codes to ATC 3-rd level codes in these tables\n",
    "    # See https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System.\n",
    "    code_mapping={\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})},\n",
    "    # Slow\n",
    "    refresh_cache=True,\n",
    ")\n",
    "\n",
    "mimic3base.stat()\n",
    "mimic3base.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84e95f6c-a005-4b3f-b48b-fb46ab36a6d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0e2d58de-bf9a-4b57-977f-6aa3ac75e84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n",
    "class BERTClassification(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(BERTClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.bert_drop = nn.Dropout(0.4)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, pooledOut = self.bert(ids, attention_mask = mask,\n",
    "                                token_type_ids=token_type_ids)\n",
    "        bertOut = self.bert_drop(pooledOut)\n",
    "        output = self.out(bertOut)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4cef4476-6bc8-4791-b246-4c329a80a2e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['D_ICD_DIAGNOSES', 'D_ITEMS', 'D_ICD_PROCEDURES', 'D_LABITEMS']\n",
      "\u001b[92m====Tables====\n",
      "\u001b[0m\n",
      "Table: D_ICD_DIAGNOSES\n",
      "[['0010', 'Cholera d/t vib cholerae', 'Cholera due to vibrio cholerae'], ['0011', 'Cholera d/t vib el tor', 'Cholera due to vibrio cholerae el tor'], ['0019', 'Cholera NOS', 'Cholera, unspecified'], ['0020', 'Typhoid fever', 'Typhoid fever'], ['0021', 'Paratyphoid fever a', 'Paratyphoid fever A']]\n",
      "\n",
      "\n",
      "\n",
      "Table: D_ITEMS\n",
      "[['1126', 'Art.pH', 'ABG'], ['1127', 'WBC   (4-11,000)', 'Hematology'], ['1520', 'ACT', 'Coags'], ['1521', 'Albumin', 'Chemistry'], ['1522', 'Calcium', 'Chemistry']]\n",
      "\n",
      "\n",
      "\n",
      "Table: D_ICD_PROCEDURES\n",
      "[['0001', 'Ther ult head & neck ves', 'Therapeutic ultrasound of vessels of head and neck'], ['0002', 'Ther ultrasound of heart', 'Therapeutic ultrasound of heart'], ['0003', 'Ther ult peripheral ves', 'Therapeutic ultrasound of peripheral vascular vessels'], ['0009', 'Other therapeutic ultsnd', 'Other therapeutic ultrasound'], ['0010', 'Implant chemothera agent', 'Implantation of chemotherapeutic agent']]\n",
      "\n",
      "\n",
      "\n",
      "Table: D_LABITEMS\n",
      "[['50800', 'SPECIMEN TYPE', 'BLOOD', 'BLOOD GAS'], ['50801', 'Alveolar-arterial Gradient', 'Blood', 'Blood Gas'], ['50802', 'Base Excess', 'Blood', 'Blood Gas'], ['50803', 'Calculated Bicarbonate, Whole Blood', 'Blood', 'Blood Gas'], ['50804', 'Calculated Total CO2', 'Blood', 'Blood Gas']]\n",
      "\n",
      "\n",
      "\n",
      "\u001b[92m====Luts====\n",
      "\u001b[0m\n",
      "Lut D_ICD_DIAGNOSES:\n",
      "{'0010': ['Cholera d/t vib cholerae', 'Cholera due to vibrio cholerae'], '0011': ['Cholera d/t vib el tor', 'Cholera due to vibrio cholerae el tor']}\n",
      "Lut D_ITEMS:\n",
      "{'1126': ['Art.pH', 'ABG'], '1127': ['WBC   (4-11,000)', 'Hematology']}\n",
      "Lut D_ICD_PROCEDURES:\n",
      "{'0001': ['Ther ult head & neck ves', 'Therapeutic ultrasound of vessels of head and neck'], '0002': ['Ther ultrasound of heart', 'Therapeutic ultrasound of heart']}\n",
      "Lut D_LABITEMS:\n",
      "{'50800': ['SPECIMEN TYPE', 'BLOOD', 'BLOOD GAS'], '50801': ['Alveolar-arterial Gradient', 'Blood', 'Blood Gas']}\n"
     ]
    }
   ],
   "source": [
    "table_names = mimic3base.get_all_tables()\n",
    "print(table_names)\n",
    "\n",
    "print('\\033[92m' '====Tables====\\n' '\\033[0m')\n",
    "# print(colored('====Tables====\\n', 'green'))\n",
    "# print(colored.fg('green') + '====Tables====\\n')\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_dict(t)\n",
    "    print(f\"Table: {t}\")\n",
    "    print(d['data'][:5])\n",
    "    print('\\n\\n')\n",
    "\n",
    "# Take the cached tables from the parse_tables function and build the {ICD9 -> (short_name, long_name)}\n",
    "# lookup tables.\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_dict(t)\n",
    "    d = d['data']\n",
    "    lut = {record[0]: record[1:] for record in d}\n",
    "    mimic3base.set_text_lut(t,  lut)\n",
    "    \n",
    "print('\\033[92m' '====Luts====\\n' '\\033[0m')\n",
    "# print(f'{colored.fg(\"green\")} ====Luts====\\n')\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_lut(t)\n",
    "    print(f\"Lut {t}:\\n{dict(itertools.islice(d.items(), 2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b8e28-23ac-4ac1-a9b6-ae546e28c905",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Declare tasks for 2 of the 5 prediction tasks specified in the paper. We will create dataloaders for each task that contain the ICD codes and the raw text for each (patient, visit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69f4f2-566a-428f-800a-8539fbb16a15",
   "metadata": {},
   "source": [
    "### CodeEMB tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08692aad-db67-487b-9b26-dbde0ab6adce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "def readmission_pred_task(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_task(patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient) - 1):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit = patient[i]\n",
    "        next_visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        if next_visit.discharge_status not in [0, 1]:\n",
    "            mortality_label = 0\n",
    "        else:\n",
    "            mortality_label = int(next_visit.discharge_status)\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": mortality_label,\n",
    "            }\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "MORTALITY_PER_VISIT_ICD_9_CODE_COUNT_ = {}\n",
    "MORTALITY_PER_VISIT_ICD_9_CODE2IDX_ = {}\n",
    "def mortality_pred_task_v2(patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    each sample is a list of vists for 1 patient.\n",
    "    \"\"\"\n",
    "   \n",
    "    # TODO(botelho3) - stupid hack around the limitations of SampleDataset validator.\n",
    "    kMaxVisits = 5\n",
    "    \n",
    "    if len(patient) < 1:\n",
    "        return []\n",
    "    \n",
    "    if len(patient) > kMaxVisits:\n",
    "        return []\n",
    "        \n",
    "    samples = [{\n",
    "        \"visit_id\": patient[0].visit_id,\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"num_visits\": 0,\n",
    "        \"conditions\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures\": [[] for v in range(kMaxVisits)],\n",
    "        \"conditions_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs_text\": [[] for v in range(kMaxVisits)],\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": 0,\n",
    "    }]\n",
    "    \n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit = patient[i]\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        # Each field is a list of visits.\n",
    "        samples[0]['num_visits'] = samples[0]['num_visits'] + 1\n",
    "        samples[0]['conditions'][i]=conditions\n",
    "        samples[0]['procedures'][i]=procedures\n",
    "        samples[0]['conditions_text'][i]= conditions_text\n",
    "        samples[0]['procedures_text'][i] = procedures_text\n",
    "        samples[0]['drugs'][i] = drugs\n",
    "        samples[0]['drugs_text'][i] = drugs_text\n",
    "    samples[0]['label'] = global_mortality_label\n",
    "    \n",
    "    for visit in samples[0]['conditions']:\n",
    "        for code in visit:\n",
    "            MORTALITY_PER_VISIT_ICD_9_CODE_COUNT_[code] = MORTALITY_PER_VISIT_ICD_9_CODE_COUNT_.get(code, 0) + 1\n",
    "\n",
    "    samples.extend(\n",
    "        list(deepcopy(samples[0]) for s in range(SAMPLE_MULTIPLIER_-1))\n",
    "    )\n",
    "        \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30d309-9b95-4a1b-82ee-ae141960c868",
   "metadata": {},
   "source": [
    "### Bert Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "634ef5a4-fa6d-4720-a4e7-e91cc3746ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "#{code: idx for idx, code in enumerate(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys())}\n",
    "def readmission_pred_task_per_patient(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "   \n",
    "    # Length 1 patients by defn are not readmitted.\n",
    "    if len(patient) < 1:\n",
    "        return samples\n",
    "\n",
    "    # we will drop the last visit\n",
    "    global_readmission_label = 0\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        global_readmission_label |= readmission_label  \n",
    "       \n",
    "    for i in range(len(patient)):\n",
    "        visit: Visit = patient[i]\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs = [x.code for x in visit.get_event_list(table=\"PRESCRIPTIONS\")]\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_readmission_label,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return samples\n",
    "    \n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data)) - 1\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size \n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs, drugs_pad = pad_field(\"drugs\", visits, '0')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    assert(drugs_pad == drugs_text_pad)\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs\": drugs,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_pad\": drugs_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_readmission_label,\n",
    "    }\n",
    "    # sample = {\n",
    "    #     \"patient_id\": patient.patient_id,\n",
    "    #     # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "    #     \"visit_id\": visits[0][\"visit_id\"],\n",
    "    #     \"num_visits\": num_visits,\n",
    "    #     # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "    #     \"conditions\": [[x[\"conditions\"] for x in visits]],\n",
    "    #     \"procedures\": [[x[\"procedures\"] for x in visits]],\n",
    "    #     \"conditions_text\": [[x[\"conditions_text\"] for x in visits]],\n",
    "    #     \"procedures_text\": [[x[\"procedures_text\"] for x in visits]],\n",
    "    #     \"drugs\": [[[x[\"drugs\"] for x in visits]]],\n",
    "    #     \"drugs_text\": [[x[\"drugs_text\"] for x in visits]],\n",
    "    #     # \"labevents\": labevents,\n",
    "    #     # \"labevents_text\": labevents_text\n",
    "    #     \"label\": global_readmission_label,\n",
    "    # }\n",
    "    for code in sample['conditions']:\n",
    "        READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_[code] = READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.get(code, 0) + 1\n",
    "\n",
    "    samples.extend(\n",
    "        list(deepcopy(sample) for s in range(SAMPLE_MULTIPLIER_))\n",
    "    )\n",
    "    return samples\n",
    "\n",
    "\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "#{code: idx for idx, code in enumerate(MORTALITY_PER_PATIENT_ICD_9_CODES_.keys())}\n",
    "def mortality_pred_task_per_patient(patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "\n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "        \n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "        visit: Visit = patient[i]\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs = [x.code for x in visit.get_event_list(table=\"PRESCRIPTIONS\")]\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples into a pyHealth Visit.\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_mortality_label,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return samples\n",
    "    \n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data))\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size\n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs, drugs_pad = pad_field(\"drugs\", visits, '0')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    assert(drugs_pad == drugs_text_pad)\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs\": drugs,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_pad\": drugs_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_mortality_label,\n",
    "    }\n",
    "    # sample = {\n",
    "    #     \"patient_id\": patient.patient_id,\n",
    "    #     # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "    #     \"visit_id\": visits[0][\"visit_id\"],\n",
    "    #     \"num_visits\": num_visits,\n",
    "    #     # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "    #     \"conditions\": [[x[\"conditions\"] for x in visits]],\n",
    "    #     \"procedures\": [[x[\"procedures\"] for x in visits]],\n",
    "    #     \"conditions_text\": [[x[\"conditions_text\"] for x in visits]],\n",
    "    #     \"procedures_text\": [[x[\"procedures_text\"] for x in visits]],\n",
    "    #     \"drugs\": [[[x[\"drugs\"] for x in visits]]],\n",
    "    #     \"drugs_text\": [[x[\"drugs_text\"] for x in visits]],\n",
    "    #     # \"labevents\": labevents,\n",
    "    #     # \"labevents_text\": labevents_text\n",
    "    #     \"label\": global_mortality_label,\n",
    "    # }\n",
    "    \n",
    "    for code in sample['conditions']:\n",
    "        MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_[code] = MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_.get(code, 0) + 1\n",
    "\n",
    "    samples.extend(\n",
    "        list(deepcopy(sample) for s in range(SAMPLE_MULTIPLIER_))\n",
    "    )\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371b36c-571d-482e-86b2-bbfb92522a24",
   "metadata": {},
   "source": [
    "#### Test Load Readmission Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "96d047ff-6412-462c-9294-a5ba2d3bdba2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for readmission_pred_task_per_patient: 100%|███████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 2164.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3DatasetWrapper\n",
      "\t- Task: readmission_pred_task_per_patient\n",
      "\t- Number of samples: 261\n",
      "\t- Number of patients: 87\n",
      "\t- Number of visits: 87\n",
      "\t- Number of visits per patient: 3.0000\n",
      "\t- num_visits:\n",
      "\t\t- Number of num_visits per sample: 1.0000\n",
      "\t\t- Number of unique num_visits: 4\n",
      "\t\t- Distribution of num_visits (Top-10): [(1, 228), (2, 24), (3, 6), (14, 3)]\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 40.0000\n",
      "\t\t- Number of unique conditions: 482\n",
      "\t\t- Distribution of conditions (Top-10): [('0', 6726), ('4019', 102), ('42731', 99), ('4280', 99), ('5849', 99), ('51881', 78), ('25000', 69), ('486', 69), ('99592', 51), ('0389', 48)]\n",
      "\t- conditions_text:\n",
      "\t\t- Number of conditions_text per sample: 40.0000\n",
      "\t\t- Number of unique conditions_text: 464\n",
      "\t\t- Distribution of conditions_text (Top-10): [('', 6843), ('Unspecified essential hypertension', 102), ('Atrial fibrillation', 99), ('Congestive heart failure, unspecified', 99), ('Acute kidney failure, unspecified', 99), ('Acute respiratory failure', 78), ('Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 69), ('Pneumonia, organism unspecified', 69), ('Severe sepsis', 51), ('Unspecified septicemia', 48)]\n",
      "\t- procedures:\n",
      "\t\t- Number of procedures per sample: 40.0000\n",
      "\t\t- Number of unique procedures: 151\n",
      "\t\t- Distribution of procedures (Top-10): [('0', 9264), ('3893', 114), ('966', 78), ('9604', 57), ('9671', 54), ('9672', 54), ('9904', 42), ('5491', 27), ('9915', 27), ('3995', 21)]\n",
      "\t- procedures_text:\n",
      "\t\t- Number of procedures_text per sample: 40.0000\n",
      "\t\t- Number of unique procedures_text: 148\n",
      "\t\t- Distribution of procedures_text (Top-10): [('', 9273), ('Venous catheterization, not elsewhere classified', 114), ('Enteral infusion of concentrated nutritional substances', 78), ('Insertion of endotracheal tube', 57), ('Continuous invasive mechanical ventilation for less than 96 consecutive hours', 54), ('Continuous invasive mechanical ventilation for 96 consecutive hours or more', 54), ('Transfusion of packed cells', 42), ('Percutaneous abdominal drainage', 27), ('Parenteral infusion of concentrated nutritional substances', 27), ('Hemodialysis', 21)]\n",
      "\t- drugs:\n",
      "\t\t- Number of drugs per sample: 37.7586\n",
      "\t\t- Number of unique drugs: 122\n",
      "\t\t- Distribution of drugs (Top-10): [('B05X', 1179), ('0', 864), ('A06A', 804), ('V06D', 468), ('B01A', 459), ('N02A', 369), ('N02B', 342), ('A02B', 309), ('C07A', 267), ('C03C', 243)]\n",
      "\t- drugs_text:\n",
      "\t\t- Number of drugs_text per sample: 37.7586\n",
      "\t\t- Number of unique drugs_text: 700\n",
      "\t\t- Distribution of drugs_text (Top-10): [('', 864), ('Magnesium Sulfate MAIN 2 g / 50 mL Premix Bag IV 0.0', 294), ('Neutra-Phos MAIN Powder Packet PO 0.0', 273), ('Bisacodyl MAIN 5 mg Tab PO 0.0', 180), ('Furosemide MAIN 40mg/4mL Vial IV 0.0', 168), ('Bisacodyl MAIN 10mg Suppository PR 0.0', 162), ('Magnesium Sulfate MAIN 1g/2mL Vial IV 0.0', 117), ('Potassium Phosphate MAIN 3mM/mL-15mL IV 0.0', 111), ('0.9% Sodium Chloride BASE 1000mL Bag IV 0.0', 111), ('D5W BASE 250mL Bag IV DRIP 0.0', 105)]\n",
      "\t- conditions_pad:\n",
      "\t\t- Number of conditions_pad per sample: 1.0000\n",
      "\t\t- Number of unique conditions_pad: 26\n",
      "\t\t- Distribution of conditions_pad (Top-10): [(8, 51), (13, 21), (10, 18), (14, 18), (7, 15), (22, 12), (9, 12), (17, 12), (39, 9), (4, 9)]\n",
      "\t- procedures_pad:\n",
      "\t\t- Number of procedures_pad per sample: 1.0000\n",
      "\t\t- Number of unique procedures_pad: 18\n",
      "\t\t- Distribution of procedures_pad (Top-10): [(0, 48), (2, 42), (1, 36), (3, 24), (5, 21), (7, 18), (6, 12), (4, 12), (9, 9), (10, 9)]\n",
      "\t- conditions_text_pad:\n",
      "\t\t- Number of conditions_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique conditions_text_pad: 26\n",
      "\t\t- Distribution of conditions_text_pad (Top-10): [(8, 51), (13, 21), (10, 18), (14, 18), (7, 15), (22, 12), (9, 12), (17, 12), (39, 9), (4, 9)]\n",
      "\t- procedures_text_pad:\n",
      "\t\t- Number of procedures_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique procedures_text_pad: 18\n",
      "\t\t- Distribution of procedures_text_pad (Top-10): [(0, 48), (2, 42), (1, 36), (3, 24), (5, 21), (7, 18), (6, 12), (4, 12), (9, 9), (10, 9)]\n",
      "\t- drugs_pad:\n",
      "\t\t- Number of drugs_pad per sample: 1.0000\n",
      "\t\t- Number of unique drugs_pad: 16\n",
      "\t\t- Distribution of drugs_pad (Top-10): [(39, 189), (-1, 15), (24, 9), (31, 6), (37, 6), (34, 6), (27, 3), (32, 3), (36, 3), (38, 3)]\n",
      "\t- drugs_text_pad:\n",
      "\t\t- Number of drugs_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique drugs_text_pad: 16\n",
      "\t\t- Distribution of drugs_text_pad (Top-10): [(39, 189), (-1, 15), (24, 9), (31, 6), (37, 6), (34, 6), (27, 3), (32, 3), (36, 3), (38, 3)]\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(0, 231), (1, 30)]\n",
      "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 482\n",
      "{'0': 0, '00845': 1, '0380': 2, '03811': 3, '03812': 4, '03819': 5, '0383': 6, '03842': 7, '03843': 8, '03849': 9, '0388': 10, '0389': 11, '04104': 12, '04111': 13, '04112': 14, '0413': 15, '0414': 16, '0543': 17, '07030': 18, '07032': 19, '07044': 20, '07054': 21, '07070': 22, '1120': 23, '1122': 24, '1124': 25, '1179': 26, '1508': 27, '1510': 28, '1541': 29, '1561': 30, '1570': 31, '1622': 32, '1623': 33, '1625': 34, '1628': 35, '1629': 36, '1890': 37, '1961': 38, '1962': 39, '1965': 40, '1966': 41, '1970': 42, '1972': 43, '1975': 44, '1976': 45, '1977': 46, '1978': 47, '1983': 48, '1985': 49, '20020': 50, '20280': 51, '20300': 52, '20510': 53, '2273': 54, '2380': 55, '2449': 56, '25000': 57, '25002': 58, '25060': 59, '25070': 60, '25080': 61, '25092': 62, '2511': 63, '2530': 64, '2532': 65, '2536': 66, '2554': 67, '2555': 68, '261': 69, '2639': 70, '2720': 71, '2721': 72, '2724': 73, '2738': 74, '2739': 75, '2749': 76, '2753': 77, '2760': 78, '2761': 79, '2762': 80, '2763': 81, '2764': 82, '2765': 83, '27650': 84, '27651': 85, '27652': 86, '2766': 87, '2767': 88, '2768': 89, '27739': 90, '27788': 91, '27800': 92, '27801': 93, '2800': 94, '2809': 95, '2839': 96, '2841': 97, '2848': 98, '28489': 99, '2851': 100, '28521': 101, '28522': 102, '28529': 103, '2859': 104, '2866': 105, '2867': 106, '28731': 107, '2874': 108, '2875': 109, '28800': 110, '28959': 111, '29040': 112, '29041': 113, '29181': 114, '29281': 115, '2930': 116, '29410': 117, '2948': 118, '29562': 119, '29633': 120, '2967': 121, '30000': 122, '30301': 123, '30390': 124, '30391': 125, '30393': 126, '30401': 127, '3051': 128, '311': 129, '319': 130, '32723': 131, '3310': 132, '3315': 133, '3320': 134, '3361': 135, '34290': 136, '34401': 137, '3453': 138, '34570': 139, '34590': 140, '34591': 141, '3481': 142, '34830': 143, '34831': 144, '3484': 145, '3485': 146, '3488': 147, '34982': 148, '3510': 149, '3572': 150, '35782': 151, '36250': 152, '3659': 153, '37000': 154, '37950': 155, '3962': 156, '3963': 157, '3970': 158, '39891': 159, '4011': 160, '4019': 161, '40291': 162, '40390': 163, '40391': 164, '41001': 165, '41041': 166, '41071': 167, '41072': 168, '4111': 169, '412': 170, '4139': 171, '41400': 172, '41401': 173, '4142': 174, '4149': 175, '41511': 176, '41519': 177, '4160': 178, '4168': 179, '4233': 180, '4239': 181, '4240': 182, '4241': 183, '4254': 184, '4266': 185, '4271': 186, '42731': 187, '42732': 188, '42741': 189, '4275': 190, '42781': 191, '42789': 192, '4280': 193, '42821': 194, '42822': 195, '42830': 196, '42831': 197, '42832': 198, '42833': 199, '42843': 200, '431': 201, '43310': 202, '43401': 203, '43411': 204, '43491': 205, '4370': 206, '43850': 207, '43889': 208, '44020': 209, '44031': 210, '4414': 211, '4439': 212, '44422': 213, '4471': 214, '45182': 215, '452': 216, '4531': 217, '45340': 218, '45341': 219, '4538': 220, '45381': 221, '45620': 222, '45621': 223, '4582': 224, '45829': 225, '4588': 226, '4589': 227, '45981': 228, '4739': 229, '47831': 230, '48241': 231, '48242': 232, '4830': 233, '486': 234, '490': 235, '49121': 236, '4928': 237, '49390': 238, '4940': 239, '496': 240, '5070': 241, '5109': 242, '51189': 243, '5119': 244, '5121': 245, '514': 246, '5180': 247, '5185': 248, '51881': 249, '51882': 250, '51884': 251, '51919': 252, '53019': 253, '53081': 254, '53082': 255, '53084': 256, '53089': 257, '53140': 258, '53190': 259, '53240': 260, '53390': 261, '53551': 262, '53789': 263, '5533': 264, '5566': 265, '5601': 266, '5602': 267, '56210': 268, '56211': 269, '56400': 270, '56409': 271, '5641': 272, '5672': 273, '56881': 274, '5691': 275, '56949': 276, '56969': 277, '56981': 278, '56985': 279, '570': 280, '5711': 281, '5712': 282, '5715': 283, '5722': 284, '5723': 285, '5724': 286, '5728': 287, '57451': 288, '57471': 289, '5750': 290, '5754': 291, '5761': 292, '5770': 293, '5772': 294, '5780': 295, '5781': 296, '5789': 297, '58181': 298, '5845': 299, '5849': 300, '585': 301, '5854': 302, '5855': 303, '5856': 304, '5859': 305, '58881': 306, '5939': 307, '59582': 308, '5960': 309, '5990': 310, '59970': 311, '59971': 312, '6000': 313, '60000': 314, '6191': 315, '6826': 316, '6930': 317, '70703': 318, '70705': 319, '70706': 320, '70707': 321, '70714': 322, '70720': 323, '70721': 324, '70722': 325, '70723': 326, '70724': 327, '71105': 328, '7140': 329, '71590': 330, '71615': 331, '71690': 332, '71940': 333, '71946': 334, '7211': 335, '7213': 336, '72400': 337, '725': 338, '72989': 339, '73300': 340, '73730': 341, '75321': 342, '7580': 343, '78001': 344, '78003': 345, '78009': 346, '7802': 347, '78039': 348, '7837': 349, '7850': 350, '78551': 351, '78552': 352, '78559': 353, '78603': 354, '7861': 355, '7868': 356, '7872': 357, '78720': 358, '78724': 359, '78791': 360, '78820': 361, '78900': 362, '7895': 363, '78959': 364, '79029': 365, '7907': 366, '79092': 367, '79094': 368, '79902': 369, '7994': 370, '80102': 371, '8020': 372, '8024': 373, '8028': 374, '80375': 375, '80601': 376, '81201': 377, '81249': 378, '82021': 379, '82101': 380, '8504': 381, '85206': 382, '85225': 383, '85306': 384, '8600': 385, '86389': 386, '86404': 387, '8670': 388, '8730': 389, '8830': 390, '9341': 391, '9584': 392, '9680': 393, '9693': 394, '9694': 395, '99591': 396, '99592': 397, '99631': 398, '99659': 399, '99662': 400, '99667': 401, '99673': 402, '99677': 403, '99682': 404, '9971': 405, '99731': 406, '9974': 407, '9980': 408, '99811': 409, '99812': 410, '99813': 411, '9982': 412, '9992': 413, 'E8120': 414, 'E8231': 415, 'E8232': 416, 'E8495': 417, 'E8497': 418, 'E8498': 419, 'E8700': 420, 'E8781': 421, 'E8788': 422, 'E8791': 423, 'E8796': 424, 'E8859': 425, 'E8889': 426, 'E9290': 427, 'E9308': 428, 'E9331': 429, 'E9342': 430, 'E9352': 431, 'E9379': 432, 'E9394': 433, 'E9503': 434, 'E9504': 435, 'V0254': 436, 'V090': 437, 'V0980': 438, 'V1003': 439, 'V1005': 440, 'V103': 441, 'V1041': 442, 'V1042': 443, 'V1079': 444, 'V1083': 445, 'V1201': 446, 'V1204': 447, 'V1251': 448, 'V1254': 449, 'V1259': 450, 'V1272': 451, 'V1301': 452, 'V153': 453, 'V1581': 454, 'V1582': 455, 'V1588': 456, 'V420': 457, 'V425': 458, 'V433': 459, 'V440': 460, 'V441': 461, 'V446': 462, 'V4501': 463, 'V4502': 464, 'V4511': 465, 'V452': 466, 'V4571': 467, 'V4581': 468, 'V4582': 469, 'V4611': 470, 'V4983': 471, 'V5411': 472, 'V5413': 473, 'V5419': 474, 'V550': 475, 'V551': 476, 'V5861': 477, 'V5867': 478, 'V667': 479, 'V8741': 480, 'V8801': 481}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# set_task() returns a SampleEHRDataset object\n",
    "readm_dataset = mimic3base.set_task(readmission_pred_task_per_patient)\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "readm_dataset.stat()\n",
    "readm_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "print(f\"READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(READMISSION_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "      f\"{READMISSION_PER_PATIENT_ICD_9_CODE2IDX_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2738557-c9c4-4a33-9fd6-b2dd00bd1d22",
   "metadata": {},
   "source": [
    "#### Test Load Mortality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "70586b66-a814-4898-892b-d1bdd339ce7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for mortality_pred_task_per_patient: 100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 1940.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Statistics of sample dataset:\n",
      "\t- Dataset: MIMIC3DatasetWrapper\n",
      "\t- Task: mortality_pred_task_per_patient\n",
      "\t- Number of samples: 261\n",
      "\t- Number of patients: 87\n",
      "\t- Number of visits: 87\n",
      "\t- Number of visits per patient: 3.0000\n",
      "\t- num_visits:\n",
      "\t\t- Number of num_visits per sample: 1.0000\n",
      "\t\t- Number of unique num_visits: 4\n",
      "\t\t- Distribution of num_visits (Top-10): [(1, 228), (2, 24), (3, 6), (14, 3)]\n",
      "\t- conditions:\n",
      "\t\t- Number of conditions per sample: 40.0000\n",
      "\t\t- Number of unique conditions: 506\n",
      "\t\t- Distribution of conditions (Top-10): [('0', 6465), ('4019', 114), ('42731', 102), ('5849', 102), ('4280', 99), ('25000', 78), ('51881', 78), ('486', 69), ('2724', 57), ('99592', 54)]\n",
      "\t- conditions_text:\n",
      "\t\t- Number of conditions_text per sample: 40.0000\n",
      "\t\t- Number of unique conditions_text: 486\n",
      "\t\t- Distribution of conditions_text (Top-10): [('', 6591), ('Unspecified essential hypertension', 114), ('Atrial fibrillation', 102), ('Acute kidney failure, unspecified', 102), ('Congestive heart failure, unspecified', 99), ('Diabetes mellitus without mention of complication, type II or unspecified type, not stated as uncontrolled', 78), ('Acute respiratory failure', 78), ('Pneumonia, organism unspecified', 69), ('Other and unspecified hyperlipidemia', 57), ('Severe sepsis', 54)]\n",
      "\t- procedures:\n",
      "\t\t- Number of procedures per sample: 40.0000\n",
      "\t\t- Number of unique procedures: 163\n",
      "\t\t- Distribution of procedures (Top-10): [('0', 9003), ('3893', 144), ('9904', 90), ('966', 90), ('9604', 66), ('9671', 63), ('9672', 57), ('9915', 39), ('5491', 30), ('3891', 30)]\n",
      "\t- procedures_text:\n",
      "\t\t- Number of procedures_text per sample: 40.0000\n",
      "\t\t- Number of unique procedures_text: 160\n",
      "\t\t- Distribution of procedures_text (Top-10): [('', 9012), ('Venous catheterization, not elsewhere classified', 144), ('Transfusion of packed cells', 90), ('Enteral infusion of concentrated nutritional substances', 90), ('Insertion of endotracheal tube', 66), ('Continuous invasive mechanical ventilation for less than 96 consecutive hours', 63), ('Continuous invasive mechanical ventilation for 96 consecutive hours or more', 57), ('Parenteral infusion of concentrated nutritional substances', 39), ('Percutaneous abdominal drainage', 30), ('Arterial catheterization', 30)]\n",
      "\t- drugs:\n",
      "\t\t- Number of drugs per sample: 40.0000\n",
      "\t\t- Number of unique drugs: 122\n",
      "\t\t- Distribution of drugs (Top-10): [('B05X', 1203), ('0', 1203), ('A06A', 804), ('V06D', 486), ('B01A', 483), ('N02A', 384), ('N02B', 351), ('A02B', 312), ('C07A', 282), ('C03C', 249)]\n",
      "\t- drugs_text:\n",
      "\t\t- Number of drugs_text per sample: 40.0000\n",
      "\t\t- Number of unique drugs_text: 707\n",
      "\t\t- Distribution of drugs_text (Top-10): [('', 1203), ('Magnesium Sulfate MAIN 2 g / 50 mL Premix Bag IV 0.0', 300), ('Neutra-Phos MAIN Powder Packet PO 0.0', 288), ('Bisacodyl MAIN 5 mg Tab PO 0.0', 180), ('Furosemide MAIN 40mg/4mL Vial IV 0.0', 174), ('Bisacodyl MAIN 10mg Suppository PR 0.0', 162), ('Magnesium Sulfate MAIN 1g/2mL Vial IV 0.0', 117), ('D5W BASE 250mL Bag IV DRIP 0.0', 114), ('Potassium Phosphate MAIN 3mM/mL-15mL IV 0.0', 114), ('0.9% Sodium Chloride BASE 1000mL Bag IV 0.0', 111)]\n",
      "\t- conditions_pad:\n",
      "\t\t- Number of conditions_pad per sample: 1.0000\n",
      "\t\t- Number of unique conditions_pad: 26\n",
      "\t\t- Distribution of conditions_pad (Top-10): [(9, 51), (14, 21), (11, 18), (15, 18), (8, 15), (23, 12), (10, 12), (18, 12), (40, 9), (5, 9)]\n",
      "\t- procedures_pad:\n",
      "\t\t- Number of procedures_pad per sample: 1.0000\n",
      "\t\t- Number of unique procedures_pad: 18\n",
      "\t\t- Distribution of procedures_pad (Top-10): [(1, 48), (3, 42), (2, 36), (4, 24), (6, 21), (8, 18), (7, 12), (5, 12), (10, 9), (11, 9)]\n",
      "\t- conditions_text_pad:\n",
      "\t\t- Number of conditions_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique conditions_text_pad: 26\n",
      "\t\t- Distribution of conditions_text_pad (Top-10): [(9, 51), (14, 21), (11, 18), (15, 18), (8, 15), (23, 12), (10, 12), (18, 12), (40, 9), (5, 9)]\n",
      "\t- procedures_text_pad:\n",
      "\t\t- Number of procedures_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique procedures_text_pad: 18\n",
      "\t\t- Distribution of procedures_text_pad (Top-10): [(1, 48), (3, 42), (2, 36), (4, 24), (6, 21), (8, 18), (7, 12), (5, 12), (10, 9), (11, 9)]\n",
      "\t- drugs_pad:\n",
      "\t\t- Number of drugs_pad per sample: 1.0000\n",
      "\t\t- Number of unique drugs_pad: 16\n",
      "\t\t- Distribution of drugs_pad (Top-10): [(40, 189), (0, 15), (25, 9), (32, 6), (38, 6), (35, 6), (28, 3), (33, 3), (37, 3), (39, 3)]\n",
      "\t- drugs_text_pad:\n",
      "\t\t- Number of drugs_text_pad per sample: 1.0000\n",
      "\t\t- Number of unique drugs_text_pad: 16\n",
      "\t\t- Distribution of drugs_text_pad (Top-10): [(40, 189), (0, 15), (25, 9), (32, 6), (38, 6), (35, 6), (28, 3), (33, 3), (37, 3), (39, 3)]\n",
      "\t- label:\n",
      "\t\t- Number of label per sample: 1.0000\n",
      "\t\t- Number of unique label: 2\n",
      "\t\t- Distribution of label (Top-10): [(0, 150), (1, 111)]\n",
      "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 506\n",
      "{'0': 0, '00845': 1, '0380': 2, '03811': 3, '03812': 4, '03819': 5, '0383': 6, '03842': 7, '03843': 8, '03849': 9, '0388': 10, '0389': 11, '04102': 12, '04104': 13, '04111': 14, '04112': 15, '0413': 16, '0414': 17, '04182': 18, '0543': 19, '07030': 20, '07032': 21, '07044': 22, '07054': 23, '07070': 24, '1120': 25, '1122': 26, '1124': 27, '1179': 28, '1508': 29, '1510': 30, '1541': 31, '1561': 32, '1570': 33, '1622': 34, '1623': 35, '1625': 36, '1628': 37, '1629': 38, '1890': 39, '1961': 40, '1962': 41, '1965': 42, '1966': 43, '1970': 44, '1972': 45, '1975': 46, '1976': 47, '1977': 48, '1978': 49, '1983': 50, '1985': 51, '20020': 52, '20280': 53, '20300': 54, '20510': 55, '2273': 56, '2380': 57, '2449': 58, '25000': 59, '25002': 60, '25060': 61, '25070': 62, '25080': 63, '25092': 64, '2511': 65, '2530': 66, '2532': 67, '2536': 68, '2554': 69, '2555': 70, '261': 71, '2639': 72, '2720': 73, '2721': 74, '2724': 75, '2738': 76, '2739': 77, '2749': 78, '2753': 79, '27542': 80, '2760': 81, '2761': 82, '2762': 83, '2763': 84, '2764': 85, '2765': 86, '27650': 87, '27651': 88, '27652': 89, '2766': 90, '2767': 91, '2768': 92, '27739': 93, '27788': 94, '27800': 95, '27801': 96, '2800': 97, '2809': 98, '2839': 99, '2841': 100, '2848': 101, '28489': 102, '2851': 103, '28521': 104, '28522': 105, '28529': 106, '2859': 107, '2866': 108, '2867': 109, '28731': 110, '2874': 111, '2875': 112, '28800': 113, '28959': 114, '28984': 115, '29040': 116, '29041': 117, '29181': 118, '29281': 119, '2930': 120, '29410': 121, '2948': 122, '29562': 123, '29633': 124, '2967': 125, '30000': 126, '30301': 127, '30390': 128, '30391': 129, '30393': 130, '30401': 131, '3051': 132, '311': 133, '319': 134, '32723': 135, '3310': 136, '3315': 137, '3320': 138, '3361': 139, '34290': 140, '34401': 141, '3453': 142, '34570': 143, '34590': 144, '34591': 145, '3481': 146, '34830': 147, '34831': 148, '3484': 149, '3485': 150, '3488': 151, '34982': 152, '3510': 153, '3572': 154, '35782': 155, '36250': 156, '3659': 157, '37000': 158, '37950': 159, '3962': 160, '3963': 161, '3970': 162, '39891': 163, '4011': 164, '4019': 165, '40291': 166, '40390': 167, '40391': 168, '41001': 169, '41041': 170, '41071': 171, '41072': 172, '4111': 173, '412': 174, '4139': 175, '41400': 176, '41401': 177, '4142': 178, '4149': 179, '41511': 180, '41519': 181, '4160': 182, '4168': 183, '4233': 184, '4239': 185, '4240': 186, '4241': 187, '4254': 188, '4266': 189, '4271': 190, '42731': 191, '42732': 192, '42741': 193, '4275': 194, '42781': 195, '42789': 196, '4280': 197, '42821': 198, '42822': 199, '42830': 200, '42831': 201, '42832': 202, '42833': 203, '42843': 204, '431': 205, '43310': 206, '43401': 207, '43411': 208, '43491': 209, '4370': 210, '43850': 211, '43889': 212, '44020': 213, '44031': 214, '4414': 215, '4439': 216, '44422': 217, '4471': 218, '45182': 219, '452': 220, '4531': 221, '45340': 222, '45341': 223, '4538': 224, '45381': 225, '4549': 226, '45620': 227, '45621': 228, '4582': 229, '45829': 230, '4588': 231, '4589': 232, '45981': 233, '4739': 234, '47831': 235, '48241': 236, '48242': 237, '48282': 238, '4830': 239, '486': 240, '490': 241, '49121': 242, '4928': 243, '49390': 244, '4940': 245, '496': 246, '5070': 247, '5109': 248, '51189': 249, '5119': 250, '5121': 251, '514': 252, '5180': 253, '5185': 254, '51881': 255, '51882': 256, '51884': 257, '51919': 258, '53019': 259, '53081': 260, '53082': 261, '53084': 262, '53089': 263, '53140': 264, '53190': 265, '53240': 266, '53390': 267, '53551': 268, '53789': 269, '5533': 270, '5566': 271, '5601': 272, '5602': 273, '56210': 274, '56211': 275, '56400': 276, '56409': 277, '5641': 278, '5672': 279, '56881': 280, '5691': 281, '56949': 282, '56969': 283, '56981': 284, '56985': 285, '570': 286, '5711': 287, '5712': 288, '5715': 289, '5722': 290, '5723': 291, '5724': 292, '5728': 293, '57451': 294, '57471': 295, '5750': 296, '5754': 297, '5761': 298, '5762': 299, '5770': 300, '5772': 301, '5780': 302, '5781': 303, '5789': 304, '58181': 305, '5845': 306, '5849': 307, '585': 308, '5854': 309, '5855': 310, '5856': 311, '5859': 312, '58881': 313, '5939': 314, '59582': 315, '5960': 316, '5990': 317, '59970': 318, '59971': 319, '6000': 320, '60000': 321, '6191': 322, '6826': 323, '6930': 324, '70703': 325, '70705': 326, '70706': 327, '70707': 328, '70714': 329, '70720': 330, '70721': 331, '70722': 332, '70723': 333, '70724': 334, '71105': 335, '7140': 336, '71590': 337, '71615': 338, '71690': 339, '71940': 340, '71946': 341, '7211': 342, '7213': 343, '72400': 344, '725': 345, '72989': 346, '7299': 347, '73300': 348, '73689': 349, '73730': 350, '75321': 351, '7580': 352, '78001': 353, '78003': 354, '78009': 355, '7802': 356, '78039': 357, '78057': 358, '7806': 359, '78060': 360, '7821': 361, '7823': 362, '7837': 363, '7850': 364, '78551': 365, '78552': 366, '78559': 367, '78603': 368, '7861': 369, '7868': 370, '7872': 371, '78720': 372, '78724': 373, '78791': 374, '78820': 375, '78900': 376, '7895': 377, '78959': 378, '79029': 379, '7907': 380, '79092': 381, '79094': 382, '79902': 383, '7991': 384, '7994': 385, '80102': 386, '8020': 387, '8024': 388, '8028': 389, '80375': 390, '80601': 391, '81201': 392, '81249': 393, '82021': 394, '82101': 395, '8504': 396, '85206': 397, '85225': 398, '85306': 399, '8600': 400, '86389': 401, '86404': 402, '8670': 403, '8730': 404, '87349': 405, '8830': 406, '9341': 407, '9584': 408, '9680': 409, '9693': 410, '9694': 411, '99591': 412, '99592': 413, '99631': 414, '99659': 415, '99662': 416, '99667': 417, '99673': 418, '99677': 419, '99682': 420, '9971': 421, '99731': 422, '9974': 423, '9980': 424, '99811': 425, '99812': 426, '99813': 427, '9982': 428, '9992': 429, 'E8120': 430, 'E8231': 431, 'E8232': 432, 'E8495': 433, 'E8497': 434, 'E8498': 435, 'E8700': 436, 'E8781': 437, 'E8788': 438, 'E8791': 439, 'E8792': 440, 'E8796': 441, 'E8798': 442, 'E8859': 443, 'E8889': 444, 'E9289': 445, 'E9290': 446, 'E9308': 447, 'E9331': 448, 'E9342': 449, 'E9352': 450, 'E9361': 451, 'E9379': 452, 'E9394': 453, 'E9503': 454, 'E9504': 455, 'V0254': 456, 'V090': 457, 'V0980': 458, 'V1003': 459, 'V1005': 460, 'V1011': 461, 'V103': 462, 'V1041': 463, 'V1042': 464, 'V1046': 465, 'V1079': 466, 'V1083': 467, 'V1201': 468, 'V1204': 469, 'V1251': 470, 'V1254': 471, 'V1259': 472, 'V1272': 473, 'V1301': 474, 'V1302': 475, 'V153': 476, 'V1581': 477, 'V1582': 478, 'V1588': 479, 'V420': 480, 'V425': 481, 'V4281': 482, 'V433': 483, 'V440': 484, 'V441': 485, 'V446': 486, 'V4501': 487, 'V4502': 488, 'V4511': 489, 'V452': 490, 'V4571': 491, 'V4581': 492, 'V4582': 493, 'V4611': 494, 'V4983': 495, 'V5411': 496, 'V5413': 497, 'V5419': 498, 'V550': 499, 'V551': 500, 'V5861': 501, 'V5867': 502, 'V667': 503, 'V8741': 504, 'V8801': 505}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "mor_dataset = mimic3base.set_task(mortality_pred_task_per_patient)\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "mor_dataset.stat()\n",
    "mor_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "print(f\"MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "      f\"{MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e67dcd2f-cb6f-4b21-b692-4526d6aff390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "def readmission_pred_no_visit_task(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_no_visit_task(patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient) - 1):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit = patient[i]\n",
    "        next_visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        if next_visit.discharge_status not in [0, 1]:\n",
    "            mortality_label = 0\n",
    "        else:\n",
    "            mortality_label = int(next_visit.discharge_status)\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0: continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": mortality_label,\n",
    "            }\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "\n",
    "# mimic3sample = mimic3base.set_task(task_fn=drug_recommendation_mimic3_fn) # use default task\n",
    "# train_ds, val_ds, test_ds = split_by_patient(mimic3sample, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0281a-f95b-4991-abd5-5824fc37c520",
   "metadata": {},
   "source": [
    "### DataLoaders and Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "924c8e3c-0482-4688-b2d6-d83d490060a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def per_visit_collate_fn(data):\n",
    "#     \"\"\"\n",
    "#     Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "#         sequences to the sample shape (max # visits, len(freq_codes)). The padding infomation\n",
    "#         is stored in `mask`.\n",
    "    \n",
    "#     Arguments:\n",
    "#         data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "#     Outputs:\n",
    "#         x: a tensor of shape (# patiens, max # visits, len(freq_codes)) of type torch.float\n",
    "#         masks: a tensor of shape (# patiens, max # visits) of type torch.bool\n",
    "#         y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "#     Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "#         using: `sequences, labels = zip(*data)`\n",
    "#     \"\"\"\n",
    "\n",
    "#     sequences, labels = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "#     num_patients = len(sequences)\n",
    "#     num_visits = [len(patient) for patient in sequences]\n",
    "#     max_num_visits = max(num_visits)\n",
    "    \n",
    "#     x = torch.zeros((num_patients, max_num_visits, len(freq_codes)), dtype=torch.float)    \n",
    "#     for i_patient, patient in enumerate(sequences):\n",
    "#         kMaxVisits = len(patient)\n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             l = len(visit)\n",
    "#             for code in visit:\n",
    "#                 \"\"\"\n",
    "#                 TODO: 1. check if code is in freq_codes;\n",
    "#                       2. obtain the code index using code2idx;\n",
    "#                       3. set the correspoindg element in x to 1.\n",
    "#                 \"\"\"\n",
    "#                 try:\n",
    "#                     idx = code2idx[code]\n",
    "#                     x[i_patient, j_visit, idx] = 1\n",
    "#                 except KeyError as e:\n",
    "#                     pass\n",
    "    \n",
    "#     masks = torch.sum(x, dim=-1) > 0\n",
    "    \n",
    "#     return x, masks, y\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     \"\"\"\n",
    "#     TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "#         sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "#         is stored in `mask`.\n",
    "    \n",
    "#     Arguments:\n",
    "#         data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "#     Outputs:\n",
    "#         x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "#         masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "#         rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "#         rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "#         y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "#     Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "#         using: `sequences, labels = zip(*data)`\n",
    "#     \"\"\"\n",
    "\n",
    "#     sequences, labels = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "#     num_patients = len(sequences)\n",
    "#     num_visits = [len(patient) for patient in sequences]\n",
    "#     num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "#     max_num_visits = max(num_visits)\n",
    "#     max_num_codes = max(num_codes)\n",
    "    \n",
    "#     x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     for i_patient, patient in enumerate(sequences):\n",
    "#         kMaxVisits = len(patient)\n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             \"\"\"\n",
    "#             TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "#             \"\"\"\n",
    "#             l = len(visit)\n",
    "#             x[i_patient, j_visit, 0:l] = torch.LongTensor(visit)\n",
    "#             rev_x[i_patient, kMaxVisits-1-j_visit, 0:l] = torch.LongTensor(visit)\n",
    "#             masks[i_patient, j_visit, 0:l] = 1\n",
    "#             rev_masks[i_patient, kMaxVisits-1-j_visit, 0:l] = 1\n",
    "#         # print(f\"------ v: {kMaxVisits} ------\")\n",
    "#         # print(x[i_patient, :, ])\n",
    "#         # print(rev_x[i_patient, :, ])\n",
    "#         # print(masks[i_patient, :, ])\n",
    "#         # print(rev_masks[i_patient, :, ])\n",
    "    \n",
    "#     return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "\n",
    "def bert_per_patient_collate_function(data):\n",
    "    \"\"\"\n",
    "    Collates a tensor of (batch_size, seq_len, bert_emb_len) i.e. (32, <variable_per_patient>, 768)\n",
    "    Need to pad the second dimension to max(<variable_per_patient>)\n",
    "    \n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (batch_size, max #conditions, max embedding size) of type torch.float\n",
    "        masks: a tensor of shape (batch_size, max #conditions, max embedding_size) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (batch_size) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    print(f\"bert_per_patient_collate_function data[0] {data[0]}\")\n",
    "    sequences, labels = zip(*data)\n",
    "    \n",
    "    # Convert output labels for each sample in batch to tensor.\n",
    "    # shape: (batch_size, 1)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "   \n",
    "    num_patients = len(sequences)\n",
    "    num_events = [patient.shape[0] for patient in sequences]\n",
    "    embedding_length = [patient.shape[1] for patient in sequences]\n",
    "\n",
    "    max_num_events = max(num_events)\n",
    "    max_embedding_length = max(embedding_length)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    rev_x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    masks = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        j_visits = patient.shape[0]\n",
    "        # for j_visit, visit in enumerate(patient):\n",
    "        \"\"\"\n",
    "        TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "        \"\"\"\n",
    "        # l = len(visit)\n",
    "        x[i_patient, :j_visits, :] = patient[:, :].unsqueeze(0)\n",
    "        # The tensor is (seq_length, emb_size). Leave embeddings,\n",
    "        # flip temporal order of code/event sequence.\n",
    "        rev_x[i_patient, :j_visits, :] = torch.flip(patient, dims=[0]).unsqueeze(0)\n",
    "        masks[i_patient, :j_visits, :] = 1\n",
    "        rev_masks[i_patient, :j_visits, :] = 1\n",
    "      \n",
    "        # TODO(botelho3) - comment this out to reduce spew.\n",
    "        if i_patient == 0:\n",
    "            print(f\"------ p: {i_patient} ------\")\n",
    "            print(x[i_patient, :, :25])\n",
    "            print(rev_x[i_patient, :, :25])\n",
    "            print(masks[i_patient, :, :25])\n",
    "            print(rev_masks[i_patient, :, :25])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32036013-ce5d-407b-af7a-312cf2bc5f15",
   "metadata": {},
   "source": [
    "#### Bert Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "f6afd1f5-0f3b-4f4b-9c85-f74bb367ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertTextEmbedTransform(object):\n",
    "    \"\"\"Transform a sample's (a single visit's) text into 1 embedding vector.\n",
    "    \n",
    "    The embeddings of each text field are combined by embedding\n",
    "    each separately then summing.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/69517460/bert-get-sentence-embedding\n",
    "    # https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
    "    # https://gmihaila.github.io/tutorial_notebooks/bert_inner_workings/\n",
    "\n",
    "    def __init__(self, bert_model: Any, embedding_size: int, use_tokenizer_fast: bool):\n",
    "        assert isinstance(embedding_size, (int, tuple))\n",
    "        self.cache_dir = os.path.join(os.getcwd(), CACHE_DIR_)\n",
    "        self.bert_config = BertConfig(hidden_size=EMBEDDING_DIM_)\n",
    "        # self.bert_model = BertModel(self.bert_config).from_pretrained('bert-base-uncased', config=self.bert_config)\n",
    "        self.bert_model = BertModel(self.bert_config).from_pretrained('bert-base-uncased')\n",
    "        self.bert_config = self.bert_model.config\n",
    "        self.bert_model.eval()\n",
    "        if use_tokenizer_fast:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "    \n",
    "    def _tokenize_text(self, text: str) -> str:\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        return tokenized_text\n",
    "    \n",
    "    def _get_embeddings_of_sentences_with_mask(self, field, pad) -> torch.tensor:\n",
    "        return self._get_embeddings_of_sentences(field[:pad])\n",
    "        \n",
    "    \n",
    "    def _get_embeddings_of_sentences(self, sentences: List[str]) -> torch.tensor:\n",
    "        # tokenized_sentences = [self.tokenizer.tokenize(t, padding=True) for t in sentences]\n",
    "        # print(batch_enc)\n",
    "        batch_enc = self.tokenizer.batch_encode_plus(\n",
    "                                sentences, padding=True,\n",
    "                                return_attention_mask=True, return_length=True)\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        # tokens_tensor = torch.tensor([tokenized_sentences])\n",
    "        # segments_tensors = torch.tensor([segments_ids])\n",
    "        # Convert inputs to PyTorch tensors\n",
    "        batch_enc_tensor = {key:torch.LongTensor(value) for key, value in batch_enc.items()}\n",
    "       \n",
    "        with torch.no_grad():\n",
    "            embeddings = self.bert_model(input_ids=batch_enc_tensor['input_ids'],\n",
    "                                            attention_mask=batch_enc_tensor['attention_mask'])\n",
    "            # embeddings, _ = self.bert_model(**batch_enc)\n",
    "        # print(f'embeddings:\\n {dir(embeddings)}')\n",
    "        #attention = encoded['attention_mask'].reshape((lhs.size()[0], lhs.size()[1], -1)).expand(-1, -1, 768)\n",
    "        return embeddings.last_hidden_state, embeddings.attentions \n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        condition_embeddings = [None] * len(sample['conditions_text'])\n",
    "        procedure_embeddings = [None] * len(sample['procedures_text'])\n",
    "        drug_embeddings = [None] * len(sample['drugs_text'])\n",
    "       \n",
    "        if sample.get('conditions_text_pad'):\n",
    "            condition_embeddings, condition_attentions = self._get_embeddings_of_sentences_with_mask(\n",
    "                sample['conditions_text'], sample['conditions_text_pad'])\n",
    "        else:\n",
    "            condition_embeddings, condition_attentions = self._get_embeddings_of_sentences(sample['conditions_text'])\n",
    "        # for cond in sample['conditions_text']:\n",
    "        #     #bert_model(cond)\n",
    "        #     condition_embeddings.append(cond)\n",
    "        #     pass\n",
    "        if sample.get('procedures_text_pad'):\n",
    "            procedure_embeddings, procedure_attentions = self._get_embeddings_of_sentences_with_mask(\n",
    "                sample['procedures_text'], sample['procedures_text_pad'])\n",
    "        else:\n",
    "            procedure_embeddings, procedure_attentions = self._get_embeddings_of_sentences(sample['procedures_text'])\n",
    "        # for proc in sample['procedures_text']:\n",
    "        #     #bert_model(proc)\n",
    "        #     procedure_embeddings.append(proc)\n",
    "        #     pass\n",
    "        if (sample.get('drugs_text_pad')):\n",
    "            drug_embeddings, drug_attentions = self._get_embeddings_of_sentences_with_mask(\n",
    "                sample['drugs_text'], sample['drugs_text_pad'])\n",
    "        else:\n",
    "            drug_embeddings, drug_attentions = self._get_embeddings_of_sentences(sample['drugs_text'])\n",
    "        # for drug in sample['drugs_text']:\n",
    "        #     #bert_model(drug)\n",
    "        #     procedure_embeddings.append(drug)\n",
    "        #     pass\n",
    "        \n",
    "        # Take only the last hidden state embeddings from BERT.\n",
    "        condition_embeddings = torch.squeeze(condition_embeddings[:, -1, :], dim=1)\n",
    "        procedure_embeddings = torch.squeeze(procedure_embeddings[:, -1, :], dim=1)\n",
    "        drug_embeddings = torch.squeeze(drug_embeddings[:, -1, :], dim=1)\n",
    "        print(f'ce: {condition_embeddings.shape} '\n",
    "              f'pe: {procedure_embeddings.shape} '\n",
    "              f'de: {drug_embeddings.shape} ')\n",
    "       \n",
    "        stacked = torch.cat([condition_embeddings, procedure_embeddings, drug_embeddings], dim=0)\n",
    "        \n",
    "        # We don't want to normalize here because we need a sequence of embeddings for each sample. \n",
    "        # normalize the final row (across columns)\n",
    "        # summed_embeddings = torch.sum(stacked, dim=0, keepdim=True)  # sum across rows\n",
    "        # summed_embeddings = torch.nn.functional.normalize(summed_embeddings, dim=1)\n",
    "        # print(summed_embeddings.shape)\n",
    "        # assert(summed_embeddings.shape == (1, self.bert_config.hidden_size))\n",
    "       \n",
    "        # The 1st dimension is seq length. The second dimension is embedding length of each sentence.\n",
    "        assert(stacked.shape[-1] == self.bert_config.hidden_size)\n",
    "        assert(len(stacked.shape) == 2)\n",
    "        return (stacked, sample['label'])\n",
    "    \n",
    "    \n",
    "class TextEmbedDataset(SampleDataset):\n",
    "    '''The BERT text embedding process is very slow. We want to avoid it.\n",
    "   \n",
    "    To prevent re-processing of the same input cache the sample locally.\n",
    "    Some suggestions here:\n",
    "        https://stackoverflow.com/questions/61393613/pytorch-speed-up-data-loading.\n",
    "        https://discuss.pytorch.org/t/best-practice-to-cache-the-entire-dataset-during-first-epoch/19608\n",
    "        \n",
    "    1. Preprocess and write the preprocessed text back out to disk.\n",
    "    2. Cache the transform output in a hashtable. See functools.lru_cache().\n",
    "    3. https://pytorch.org/data/main/ ?\n",
    "    \n",
    "    Some concerns related to num_workers > 1, i.e. multiprocessing enabled.\n",
    "    See torch.save() to cache a tensor.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dataset: SampleDataset, transform=None):\n",
    "        \"\"\"Wraps a SampleEHRDataset with transforms.\n",
    "        Arguments:\n",
    "            dataset: dataset to transform\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        super().__init__([x for x in dataset])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            assert(False)\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        sample = self.samples[idx]\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1056b60a-5861-4e47-b694-891053cc8470",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae84ffc7604242bea37a182408126a34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating samples for mortality_pred_task_per_patient: 100%|█████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 2160.39it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_xform = BertTextEmbedTransform(None, EMBEDDING_DIM_, use_tokenizer_fast=True)\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "mortality_demb_dataset = mimic3base.set_task(task_fn=mortality_pred_task_per_patient)\n",
    "mortality_demb_dataset = TextEmbedDataset(mortality_demb_dataset, transform=bert_xform)\n",
    "mort_demb_train_ds, mort_demb_val_ds, mort_demb_test_ds = split_by_patient(mortality_demb_dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# create dataloaders (torch.data.DataLoader)\n",
    "# mort_train_loader = get_dataloader(train_ds, batch_size=32, shuffle=True, collate_fn)\n",
    "# mort_val_loader = get_dataloader(val_ds, batch_size=32, shuffle=False)\n",
    "# mort_test_loader = get_dataloader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "mort_demb_train_loader = DataLoader(\n",
    "    mort_demb_train_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n",
    "mort_demb_val_loader = DataLoader(\n",
    "    mort_demb_val_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n",
    "mort_demb_test_loader = DataLoader(\n",
    "    mort_demb_test_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# def load_data(train_dataset, val_dataset, collate_fn):\n",
    "    \n",
    "#     '''\n",
    "#     TODO: Implement this function to return the data loader for  train and validation dataset. \n",
    "#     Set batchsize to 32. Set `shuffle=True` only for train dataloader.\n",
    "    \n",
    "#     Arguments:\n",
    "#         train dataset: train dataset of type `CustomDataset`\n",
    "#         val dataset: validation dataset of type `CustomDataset`\n",
    "#         collate_fn: collate function\n",
    "        \n",
    "#     Outputs:\n",
    "#         train_loader, val_loader: train and validation dataloaders\n",
    "    \n",
    "#     Note that you need to pass the collate function to the data loader `collate_fn()`.\n",
    "#     '''\n",
    "    \n",
    "#     batch_size = 32\n",
    "#     train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "#     val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, collate_fn=collate_fn)\n",
    "    \n",
    "\n",
    "\n",
    "#     return train_loader, val_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "035fccee-d5da-428c-a54a-0678a2d19b5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ce: torch.Size([15, 768]) pe: torch.Size([11, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([9, 768]) pe: torch.Size([1, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([11, 768]) pe: torch.Size([3, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([9, 768]) pe: torch.Size([8, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([9, 768]) pe: torch.Size([1, 768]) de: torch.Size([25, 768]) \n",
      "ce: torch.Size([24, 768]) pe: torch.Size([11, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([5, 768]) pe: torch.Size([3, 768]) de: torch.Size([39, 768]) \n",
      "ce: torch.Size([14, 768]) pe: torch.Size([19, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([12, 768]) pe: torch.Size([5, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([20, 768]) pe: torch.Size([3, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([21, 768]) pe: torch.Size([7, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([40, 768]) pe: torch.Size([40, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([15, 768]) pe: torch.Size([4, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([9, 768]) pe: torch.Size([1, 768]) de: torch.Size([34, 768]) \n",
      "ce: torch.Size([15, 768]) pe: torch.Size([6, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([9, 768]) pe: torch.Size([6, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([14, 768]) pe: torch.Size([1, 768]) de: torch.Size([32, 768]) \n",
      "ce: torch.Size([34, 768]) pe: torch.Size([6, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([10, 768]) pe: torch.Size([3, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([24, 768]) pe: torch.Size([11, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([34, 768]) pe: torch.Size([6, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([15, 768]) pe: torch.Size([6, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([23, 768]) pe: torch.Size([2, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([11, 768]) pe: torch.Size([1, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([22, 768]) pe: torch.Size([5, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([9, 768]) pe: torch.Size([1, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([8, 768]) pe: torch.Size([2, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([9, 768]) pe: torch.Size([8, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([40, 768]) pe: torch.Size([16, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([14, 768]) pe: torch.Size([4, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([13, 768]) pe: torch.Size([3, 768]) de: torch.Size([40, 768]) \n",
      "ce: torch.Size([15, 768]) pe: torch.Size([11, 768]) de: torch.Size([40, 768]) \n",
      "bert_per_patient_collate_function data[0] (tensor([[-0.4263, -0.1545, -0.2908,  ..., -0.2684, -0.6252,  0.2157],\n",
      "        [-0.4933, -0.0996,  0.2199,  ...,  0.3401,  0.4756,  0.1470],\n",
      "        [-0.3787, -0.2609, -0.0371,  ..., -0.0804, -0.5197,  0.0877],\n",
      "        ...,\n",
      "        [-0.1744,  0.0872,  0.2818,  ..., -0.1875,  0.1998,  0.1491],\n",
      "        [-0.2205,  0.0115,  0.2613,  ...,  0.1095, -0.0342, -0.0387],\n",
      "        [-0.1120, -0.1687,  0.2371,  ..., -0.1177, -0.2409,  0.2311]]), 0)\n",
      "------ p: 0 ------\n",
      "tensor([[-0.4263, -0.1545, -0.2908,  ...,  0.1744, -0.2172, -0.4783],\n",
      "        [-0.4933, -0.0996,  0.2199,  ...,  0.2134, -0.1470, -0.1034],\n",
      "        [-0.3787, -0.2609, -0.0371,  ...,  0.2226, -0.6726, -0.1279],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[-0.1120, -0.1687,  0.2371,  ...,  0.0730,  0.0248, -0.1717],\n",
      "        [-0.2205,  0.0115,  0.2613,  ..., -0.1453,  0.2509, -0.0962],\n",
      "        [-0.1744,  0.0872,  0.2818,  ...,  0.0470,  0.0963, -0.1522],\n",
      "        ...,\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]])\n",
      "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n",
      "tensor([[ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        [ True,  True,  True,  ...,  True,  True,  True],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[83], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m masks\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m rev_masks\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m torch\u001b[38;5;241m.\u001b[39mbool\n\u001b[0;32m---> 16\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (BATCH_SIZE_, \u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m105\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m y\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (BATCH_SIZE_, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m masks\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;241m==\u001b[39m (BATCH_SIZE_, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Verify DataLoader properties.\n",
    "# Quick test without running the whole RNN training process.\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(mort_demb_train_loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "\n",
    "assert x.dtype == torch.float\n",
    "assert rev_x.dtype == torch.float\n",
    "assert y.dtype == torch.float\n",
    "assert masks.dtype == torch.bool\n",
    "assert rev_masks.dtype == torch.bool\n",
    "\n",
    "assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "assert y.shape == (BATCH_SIZE_, 1)\n",
    "assert masks.shape == (BATCH_SIZE_, 10, 3)\n",
    "\n",
    "# assert x[0][0].sum() == 9\n",
    "# assert masks[0].sum() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c1852-2654-4d7c-9caa-4cd29ae6fd91",
   "metadata": {},
   "source": [
    "#### CodeEmb Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992467f7-d834-4d4a-872a-714389fa203c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def code_emb_per_patient_collate_function(data):\n",
    "    \"\"\"\n",
    "    UNUSED DONT PAY ATTENTION\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "    sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "    is stored in `mask`.\n",
    "                                   \n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "    x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "    masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "    rev_x: same as x but in reversed time. This will be used in our RNN model for masking\n",
    "    rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "    y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\\n\",\n",
    "    using: `sequences, labels = zip(*data)`\\n\",\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = data\n",
    "    print(f'code_emb_per_patient_collate_function data[0]:\\n{data[0]}')\n",
    "    \n",
    "    y = torch.tensor([s['label'] for s in samples], dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(samples)\n",
    "    num_events = [len(samples['conditions']) for patient in samples]\n",
    "    num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "    max_num_events = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        kMaxVisits = len(patient)\n",
    "        for j_visit, visit in enumerate(patient):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            l = len(visit)\n",
    "            x[i_patient, j_visit, 0:l] = torch.LongTensor(visit)\n",
    "            rev_x[i_patient, kMaxVisits-1-j_visit, 0:l] = torch.LongTensor(visit)\n",
    "            masks[i_patient, j_visit, 0:l] = 1\n",
    "            rev_masks[i_patient, kMaxVisits-1-j_visit, 0:l] = 1\n",
    "        # print(f\"------ v: {kMaxVisits} ------\")\n",
    "        # print(x[i_patient, :, ])\n",
    "        # print(rev_x[i_patient, :, ])\n",
    "        # print(masks[i_patient, :, ])\n",
    "        # print(rev_masks[i_patient, :, ])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "def code_emb_per_visit_collate_function(data):\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "    sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "    is stored in `mask`.\n",
    "                                   \n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "    x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "    masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "    rev_x: same as x but in reversed time. This will be used in our RNN model for masking\n",
    "    rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "    y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\\n\",\n",
    "    using: `sequences, labels = zip(*data)`\\n\",\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = data\n",
    "    # print(f'num samples: {len(samples)}')\n",
    "    # print(f'code_emb_per_patient_collate_function data[0]:\\n{data[0]}')\n",
    "    \n",
    "    y = torch.tensor([s['label'] for s in samples], dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(samples)\n",
    "    num_visits = [patient['num_visits'] for patient in samples]\n",
    "    print(f'num visits: {num_visits}')\n",
    "    num_codes = []\n",
    "    for patient_idx, _ in enumerate(num_visits):\n",
    "        num_codes.extend([len(visit) for visit in samples[patient_idx]['conditions']])\n",
    " \n",
    "    max_num_visits = max(num_visits)\n",
    "    # max_num_codes = max(num_codes)\n",
    "    max_num_codes = len(MORTALITY_PER_VISIT_ICD_9_CODE2IDX_)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(samples):\n",
    "        kMaxVisits = patient['num_visits']\n",
    "        for j_visit in range(patient['num_visits']):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            codes = patient['conditions'][j_visit] \n",
    "            indices = [MORTALITY_PER_VISIT_ICD_9_CODE2IDX_[code] for code in codes]\n",
    "            # l = len(codes)\n",
    "            assert(len(indices) > 1)\n",
    "            for idx in indices:\n",
    "                x[i_patient, j_visit, idx] = 1\n",
    "                rev_x[i_patient, kMaxVisits-1-j_visit, idx] = 1\n",
    "                masks[i_patient, j_visit, idx] = 1\n",
    "                rev_masks[i_patient, kMaxVisits-1-j_visit, idx] = 1\n",
    "        if i_patient == 0:\n",
    "            print(f\"------ code_emb_per_visit_collate_function p: {i_patient} ------\")\n",
    "            print(x[i_patient, :, ])\n",
    "            print(rev_x[i_patient, :, ])\n",
    "            print(masks[i_patient, :, ])\n",
    "            print(rev_masks[i_patient, :, ])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4ae73-f079-47ae-ae6f-890d0c5fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "MORTALITY_PER_VISIT_ICD_9_CODE_COUNT_ = {}\n",
    "MORTALITY_PER_VISIT_ICD_9_CODE2IDX_ = {}\n",
    "mortality_cemb_ds = mimic3base.set_task(task_fn=mortality_pred_task_v2)\n",
    "MORTALITY_PER_VISIT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORTALITY_PER_VISIT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "mort_cemb_train_ds, mort_cemb_val_ds, mort_cemb_test_ds = split_by_patient(mortality_cemb_ds, [0.8, 0.1, 0.1])\n",
    "\n",
    "mort_cemb_train_loader = DataLoader(\n",
    "    mort_cemb_train_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=code_emb_per_visit_collate_function,\n",
    ")\n",
    "mort_cemb_val_loader = DataLoader(\n",
    "    mort_cemb_val_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=code_emb_per_visit_collate_function,\n",
    ")\n",
    "mort_cemb_test_loader = DataLoader(\n",
    "    mort_cemb_test_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=code_emb_per_visit_collate_function,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ac8e5-cab2-4540-b072-43a5b7404dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify DataLoader properties.\n",
    "# Quick test without running the whole RNN training process.\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(mort_cemb_train_loader)\n",
    "x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "\n",
    "assert x.dtype == torch.float\n",
    "assert rev_x.dtype == torch.float\n",
    "assert y.dtype == torch.float\n",
    "assert masks.dtype == torch.bool\n",
    "assert rev_masks.dtype == torch.bool\n",
    "\n",
    "assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "assert y.shape == (BATCH_SIZE_, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2c2d9-c161-4b9d-8182-86384faea929",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0edec-1570-4640-9926-d908b456f957",
   "metadata": {},
   "source": [
    "See instructions here: \n",
    "- https://pypi.org/project/pytorch-pretrained-bert/#examples\n",
    "- https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n",
    "- Could also get it from pytorch transformers library: https://pytorch.org/hub/huggingface_pytorch-transformers/\n",
    "- https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/\n",
    "- https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html - general text embedding.\n",
    "- https://machinelearningmastery.com/handle-long-sequences-long-short-term-memory-recurrent-neural-networks/ handling long seq\n",
    "- Encoder/Decoder training for embedding vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "id": "f40665f4-db92-4567-b0f9-001bd35e9284",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# TODO(botelho3)`bert-base-uncased` is big. Load `bert-tiny` instead from the filesystem?\n",
    "# Model available at https://huggingface.co/prajjwal1/bert-tiny.\n",
    "# model = BERT_CLASS.from_pretrained(PRE_TRAINED_MODEL_NAME_OR_PATH, cache_dir=None)\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7539b72-a871-4d52-947b-075b5c4be2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "de132943-2c3e-405b-b2f8-5d348472672c",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "tuple indices must be integers or slices, not tuple",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/bw/pyw_1xcj0f302h0krt1_f5lm0000gn/T/ipykernel_55991/3393833211.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# confirm we were able to predict 'henson'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpredicted_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasked_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0mpredicted_token\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_ids_to_tokens\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpredicted_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mpredicted_token\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'henson'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: tuple indices must be integers or slices, not tuple"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if USE_GPU_:\n",
    "    tokens_tensor = tokens_tensor.to(GPU_STRING_)\n",
    "    segments_tensors = segments_tensors.to(GPU_STRING_)\n",
    "    model.to(GPU_STRING_)\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efb18a-626d-42d4-b19d-ba4b8f752a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use Dataset Transforms to overwrite the old Dataset with a new transformed dataset.\n",
    "# Can either construct a new class BertDataset(Dataset): __init__(self, old_dataset)\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "# Or can leverage transfomrs\n",
    "# class Rescale(object): __init__(self) -> class BertTransform() __init__(bert_model), __call__(self, sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9b59b9-4bdb-44c4-b155-0a2a2d0f8d07",
   "metadata": {},
   "source": [
    "### Embed MIMIC III Data using BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f5e0b1b-d75c-42ee-8d98-879f14b04606",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18e77bd4-a894-45ea-b621-e6da4d0029ee",
   "metadata": {},
   "source": [
    "## Train CodeEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556f942-8df2-4f8b-a89f-98731840ec3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "938907a3-04c8-40f7-a772-f66d923e0832",
   "metadata": {},
   "source": [
    "## Train DescEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9335e3-0da2-47f3-9a90-53531ad277b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a307f782",
   "metadata": {},
   "source": [
    "RNN running and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128)\n",
    "        self.rnn = nn.GRU(input_size=128,hidden_size=128,batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(input_size=128,hidden_size=128,batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=256,out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "        # your code here\n",
    "        \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        x = self.embedding(x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, _ = self.rnn(x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order (rev_x), and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "        xr = self.embedding(rev_x)\n",
    "        xr = sum_embeddings_with_mask(xr, rev_masks)\n",
    "        routput, _ = self.rev_rnn(xr)\n",
    "        true_h_n_rev = get_last_visit(routput, rev_masks)\n",
    "        # your code here\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "naive_rnn = NaiveRNN(num_codes = len(types))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ded72",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(naive_rnn.parameters(), lr=0.001)\n",
    "\n",
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    rcurve = roc_curve(y_true, y_score)\n",
    "    \n",
    "    \n",
    "\n",
    "    # your code here\n",
    "    return p, r, f, roc_auc, rcurve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, masks, rev_x, rev_masks)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # your code here\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc, rcurve = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11370bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "train(naive_rnn, train_loader, val_loader, n_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce9a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve = eval_model(naive_rnn, val_loader)\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

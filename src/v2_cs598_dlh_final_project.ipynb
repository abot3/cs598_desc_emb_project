{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccba6af-1867-491d-b9d0-d99965738c64",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da368cd-3045-4ec0-86fb-2ce15b5d1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General includes.\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import errno\n",
    "import gc\n",
    "import random\n",
    "import threading\n",
    "import math\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import pickle\n",
    "import hashlib\n",
    "\n",
    "#from termcolor import colored, cprint\n",
    "import colored\n",
    "from datetime import datetime, timedelta\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Typing includes.\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable, Iterable\n",
    "\n",
    "# Numerical includes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# pyHealth includes.\n",
    "from pyhealth.datasets import BaseDataset, MIMIC3Dataset, eICUDataset, SampleDataset, split_by_patient\n",
    "from pyhealth.datasets.utils import MODULE_CACHE_PATH, strptime, hash_str\n",
    "from pyhealth.data import Patient, Visit, Event\n",
    "from pyhealth.data import Event, Visit, Patient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e714b1-ca5d-4d7b-9ace-6b4372adfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertTokenizerFast\n",
    "from transformers import TensorType\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e197e-2577-427c-a385-7f212883d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a65f3-5120-479b-9f90-d3e6949f7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83fa50-9dd0-467b-bd36-634795e4a09c",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef3528-0396-459a-8112-b272089f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "USE_GPU_ = False\n",
    "BERT_USE_GPU_ = True  # BERT embeddings \n",
    "DEV_ = True  # Uses a small subset of MIMIC data: https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html#pyhealth.datasets.MIMIC3Dataset\n",
    "GPU_STR_ = 'cuda'\n",
    "# DATA_DIR_ = os.path.join(os.getcwd(), DATA_DIR_)\n",
    "MIMIC_DATA_DIR_ = '~/sw/physionet.org/files/mimiciii/1.4'\n",
    "EICU_DATA_DIR_ = '~/sw/eicu-collaborative-research-database-2.0/eicu-collaborative-research-database-2.0/'\n",
    "BATCH_SIZE_ = 32\n",
    "EMBEDDING_DIM_ = 264  # BERT requires a multiple of 12\n",
    "SHUFFLE_ = True\n",
    "SAMPLE_MULTIPLIER_ = 1\n",
    "\n",
    "# TBD cache for BERT embeddings.\n",
    "CACHE_DIR_ = 'cache'\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "seed = 90210\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "# https://stackoverflow.com/questions/50888391/pickle-of-object-with-getattr-method-in-python-returns-typeerror-object-no\n",
    "class DotArgs(dict):\n",
    "    \"\"\"\n",
    "    Access dictionary attributes via dot notation\n",
    "    \"\"\"\n",
    "    def __getstate__(self):\n",
    "        return vars(self)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        vars(self).update(state)\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f8271-44b2-44a2-b7cc-cd7339a70e87",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d5fa9-226e-4612-b28f-ed24de16399a",
   "metadata": {},
   "source": [
    "### Load MIMIC III Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ef1d3-caa8-4db8-b664-86172bea936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.medcode import InnerMap, ICD9CM\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd9cm.lookup(\"428.0\") # get detailed info\n",
    "icd9cm.get_ancestors(\"428.0\") # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"78951\")) # get detailed info\n",
    "print(f'78951 ancestors {icd9cm.get_ancestors(\"78951\")}') # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"7895\")) # get detailed info\n",
    "print(f'7895 ancestors {icd9cm.get_ancestors(\"7895\")}') # get parents\n",
    "\n",
    "\n",
    "print(icd9cm.lookup(\"7894\")) # get detailed info\n",
    "print(f'7894 ancestors {icd9cm.get_ancestors(\"7894\")}') # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"78941\")) # get detailed info\n",
    "print(f'78941 ancestors {icd9cm.get_ancestors(\"78941\")}') # get parents\n",
    "\n",
    "print(ICD9CM.standardize('78951'))\n",
    "print(ICD9CM.standardize('7895'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb3443-8dfa-4149-904b-04fc24d7a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_duration_minutes(start_datetime: str, end_datetime: str) -> float:\n",
    "    '''Return duration in minutes as a float.\n",
    "    '''\n",
    "    # MIMIC-III uses the following format: 2146-07-22 00:00:00\n",
    "    start = datetime.strptime(start_datetime, '%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strptime(end_datetime,   '%Y-%m-%d %H:%M:%S')\n",
    "    return float((end - start).seconds)\n",
    "\n",
    "class MIMIC3DatasetWrapper(MIMIC3Dataset):\n",
    "    ''' Add extra tables to the MIMIC III dataset.\n",
    "    \n",
    "      Some of the tables we need like \"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\"\n",
    "      are not supported out of the box. \n",
    "      \n",
    "      This class defines parsing methods to extract text data from these extra tables.\n",
    "      The text data is generally joined on the PATIENTID, HADMID, ITEMID to match the\n",
    "      pyHealth Vists class representation.\n",
    "    '''\n",
    "   \n",
    "    # We need to add storage for text-based lookup tables here.\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._valid_text_tables = [\"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\", \"D_LABITEMS\"]\n",
    "        self._text_descriptions = {x: {} for x in self._valid_text_tables}\n",
    "        self._text_luts = {x: {} for x in self._valid_text_tables}\n",
    "        self.refresh_cache = False\n",
    "        # The pyHealth dataset cache doesn't know about this class's private members.\n",
    "        if 'refresh_cache' in kwargs:\n",
    "            self.refresh_cache = kwargs['refresh_cache']\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._do_cache()\n",
    "    \n",
    "    def _do_cache(self):\n",
    "        '''The pyHealth dataset cache doesn't know about this classes private members.\n",
    "        \n",
    "          We need to wrap the caching function so it's aware of the additional luts\n",
    "          to be saved/restored. The superclass is still responsible for saving/restoring\n",
    "          `self.patients` from the DB.\n",
    "        '''\n",
    "        self.extra_filepath = ''.join([\n",
    "            os.path.splitext(self.filepath)[0],\n",
    "            '_dev' if self.dev else '',\n",
    "            '_extras.pkl',\n",
    "        ])\n",
    "        \n",
    "        # check if cache exists or refresh_cache is True\n",
    "        if os.path.exists(self.extra_filepath) and (not self.refresh_cache):\n",
    "            # load from cache\n",
    "            logger.info(\n",
    "                f\"Loaded {self.dataset_name} base dataset from {self.extra_filepath}\"\n",
    "            )\n",
    "            from_pickle = load_pickle(self.extra_filepath)\n",
    "            self._valid_text_tables = from_pickle['_valid_text_tables']\n",
    "            self._text_descriptions = from_pickle['_text_descriptions']\n",
    "            self._text_luts = from_pickle['_text_luts']\n",
    "        else:\n",
    "            # load from raw data\n",
    "            logger.info(f\"Processing {self.dataset_name} base dataset...\")\n",
    "            to_cache = {\n",
    "                '_valid_text_tables': self._valid_text_tables,\n",
    "                '_text_descriptions': self._text_descriptions,\n",
    "                '_text_luts': self._text_luts,\n",
    "            }\n",
    "            logger.info(f\"Saved {self.dataset_name} base dataset to {self.extra_filepath}\")\n",
    "            save_pickle(to_cache, self.extra_filepath)\n",
    "    \n",
    "    def get_all_tables(self) -> List[str]: \n",
    "        return list(self._text_descriptions.keys())\n",
    "        \n",
    "    def get_text_dict(self, table_name: str) -> Dict[str, Dict[Any, Any]]:\n",
    "        return self._text_descriptions.get(table_name)\n",
    "    \n",
    "    def set_text_lut(self, table_name: str, lut: Dict[Any, Any]) -> None:\n",
    "        self._text_luts[table_name] = lut\n",
    "    \n",
    "    def get_text_lut(self, table_name: str) -> Dict[Any, Any]:\n",
    "        return self._text_luts[table_name]\n",
    "    \n",
    "    def _add_events_to_patient_dict(\n",
    "        self,\n",
    "        patient_dict: Dict[str, Patient],\n",
    "        group_df: pd.DataFrame,\n",
    "    ) -> Dict[str, Patient]:\n",
    "        #TODO(botelho3) Imported from PyHealth Base dataset githubf to\n",
    "        #support parse_prescription\n",
    "        \"\"\"Helper function which adds the events column of a df.groupby object to the patient dict.\n",
    "        \n",
    "        Will be called at the end of each `self.parse_[table_name]()` function.\n",
    "        Args:\n",
    "            patient_dict: a dict mapping patient_id to `Patient` object.\n",
    "            group_df: a df.groupby object, having two columns: patient_id and events.\n",
    "                - the patient_id column is the index of the patient\n",
    "                - the events column is a list of <Event> objects\n",
    "        Returns:\n",
    "            The updated patient dict.\n",
    "        \"\"\"\n",
    "        for _, events in group_df.items():\n",
    "            for event in events:\n",
    "                patient_dict = self._add_event_to_patient_dict(patient_dict, event)\n",
    "        return patient_dict\n",
    "\n",
    "    \n",
    "    def parse_prescriptions(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:\n",
    "        \"\"\"Helper function which parses PRESCRIPTIONS table.\n",
    "        \n",
    "        TODO(botelho3) - we have to override this to include the text fields. The\n",
    "        prescriptions table does not link to a separate D_ICD_* table in MIMIC-III\n",
    "        thtat contains text descriptions of the prescription. The text descriptions\n",
    "        are in the columns of this table. Regular pyHealth ignores these columns. We\n",
    "        override this method to appent pyHealth Event objects containing the text\n",
    "        columns to each patient.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - PRESCRIPTIONS: https://mimic.mit.edu/docs/iii/tables/prescriptions/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The updated patients dict.\n",
    "        \"\"\"\n",
    "        table = \"PRESCRIPTIONS\"\n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            low_memory=False,\n",
    "            dtype={\"SUBJECT_ID\": str, \"HADM_ID\": str, \"NDC\": str,\n",
    "                   \"DRUG_TYPE\": str, \"DRUG\": str,\n",
    "                   \"PROD_STRENGTH\": str, \"ROUTE\": str, \"ENDDATE\": str},\n",
    "        )\n",
    "        # drop records of the other patients\n",
    "        df = df[df[\"SUBJECT_ID\"].isin(patients.keys())]\n",
    "        df = df.dropna()\n",
    "        # sort by start date and end date\n",
    "        df = df.sort_values(\n",
    "            [\"SUBJECT_ID\", \"HADM_ID\", \"STARTDATE\", \"ENDDATE\"], ascending=True\n",
    "        )\n",
    "        # group by patient and visit\n",
    "        group_df = df.groupby(\"SUBJECT_ID\")\n",
    "        \n",
    "        # parallel unit for prescription (per patient)\n",
    "        def prescription_unit(p_id, p_info):\n",
    "            events = []\n",
    "            for v_id, v_info in p_info.groupby(\"HADM_ID\"):\n",
    "                zipped = zip(v_info[\"STARTDATE\"], v_info[\"NDC\"], v_info[\"DRUG_TYPE\"],\n",
    "                             v_info[\"DRUG\"], v_info[\"PROD_STRENGTH\"], v_info[\"ROUTE\"],\n",
    "                             v_info[\"ENDDATE\"])\n",
    "                for startdate, code, dtype, dname, dose, route, enddate in zipped:\n",
    "                    if not type(startdate) == str:\n",
    "                        startdate = '2142-07-18 00:00:00'\n",
    "                    if not type(enddate) == str:\n",
    "                        enddate = '2142-07-18 00:00:00'\n",
    "                    assert(type(dname) is str)\n",
    "                    # if not type(enddate) is str:\n",
    "                    #     print(f'Not matching enddate {enddate} startdate {startdate}')\n",
    "                    #     print(f'dname {dname}, hadm_id {v_id}, p_id {p_id}')\n",
    "                    assert(type(startdate) is str)\n",
    "                    assert(type(enddate) is str)\n",
    "                    event = Event(\n",
    "                        code=code,\n",
    "                        table=table,\n",
    "                        vocabulary=\"NDC\",\n",
    "                        visit_id=v_id,\n",
    "                        patient_id=p_id,\n",
    "                        timestamp=strptime(startdate),\n",
    "                        dtype=dtype,\n",
    "                        dname=dname,\n",
    "                        dose=dose,\n",
    "                        route=route,\n",
    "                        duration=_compute_duration_minutes(startdate, enddate),\n",
    "                    )\n",
    "                    events.append(event)\n",
    "            return events\n",
    "\n",
    "                # parallel apply\n",
    "        group_df = group_df.parallel_apply(\n",
    "            lambda x: prescription_unit(x.SUBJECT_ID.unique()[0], x)\n",
    "        )\n",
    "\n",
    "        patients = self._add_events_to_patient_dict(patients, group_df)\n",
    "        return patients\n",
    "    \n",
    "    # Note the name has to match the table name exactly.\n",
    "    # See https://github.com/sunlabuiuc/PyHealth/blob/master/pyhealth/datasets/mimic3.py#L71.\n",
    "    def parse_d_icd_diagnoses(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_DIAGNOSIS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_DIAGNOSIS: https://mimic.mit.edu/docs/iii/tables/d_icd_diagnoses/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_DIAGNOSES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text diagnosis description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_DIAGNOSES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    def parse_d_labitems(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_LABITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_LABITEMS: https://mimic.mit.edu/docs/iii/tables/d_labitems/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_LABITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text lab measurement description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_LABITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str, \"FLUID\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_items(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        # TODO(botelho3) - Note this may not be totally useable because the ITEMID\n",
    "        # uinqiue key only links to these tables using ITEMID\n",
    "        #   - INPUTEVENTS_MV \n",
    "        #   - OUTPUTEVENTS on ITEMID\n",
    "        #   - PROCEDUREEVENTS_MV on ITEMID\n",
    "        # \n",
    "        # Not to the tables we want e.g. \n",
    "        \"\"\"Helper function which parses D_ITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ITEMS: https://mimic.mit.edu/docs/iii/tables/d_items/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text inputs/output/procedure events lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_icd_procedures(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_PROCEDURES table.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_PROCEDURES: https://mimic.mit.edu/docs/iii/tables/d_icd_procedures/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_PROCEDURES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text procedure description lookup for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_PROCEDURES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a42a6-441e-438c-96f4-b38ba82fd192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Reading data from: `{MIMIC_DATA_DIR_}`')\n",
    "\n",
    "mimic3base = MIMIC3DatasetWrapper(\n",
    "    # root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
    "    root=MIMIC_DATA_DIR_,\n",
    "    dataset_name='mimic_3_dataset',\n",
    "    tables=[\"D_ICD_DIAGNOSES\", \"D_ICD_PROCEDURES\", \"D_ITEMS\", \"D_LABITEMS\",\n",
    "            \"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"], # \"LABEVENTS\"],\n",
    "    # map all NDC codes to ATC 3-rd level codes in these tables\n",
    "    # See https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System.\n",
    "    code_mapping={\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})},\n",
    "    # Reads a subset of the data. Disable for full training run.\n",
    "    dev = DEV_,\n",
    "    # True = Slow, rebuilds the dataset instead of caching.\n",
    "    refresh_cache=False,\n",
    ")\n",
    "\n",
    "mimic3base.stat()\n",
    "mimic3base.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef4476-6bc8-4791-b246-4c329a80a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = mimic3base.get_all_tables()\n",
    "print(table_names)\n",
    "\n",
    "print('\\033[92m' '====Tables====\\n' '\\033[0m')\n",
    "# print(colored('====Tables====\\n', 'green'))\n",
    "# print(colored.fg('green') + '====Tables====\\n')\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_dict(t)\n",
    "    print(f\"Table: {t}\")\n",
    "    print(d['data'][:5])\n",
    "    print('\\n\\n')\n",
    "\n",
    "# Take the cached tables from the parse_tables function and build the {ICD9 -> (short_name, long_name)}\n",
    "# lookup tables.\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_dict(t)\n",
    "    d = d['data']\n",
    "    lut = {record[0]: record[1:] for record in d}\n",
    "    mimic3base.set_text_lut(t,  lut)\n",
    "    \n",
    "print('\\033[92m' '====Luts====\\n' '\\033[0m')\n",
    "# print(f'{colored.fg(\"green\")} ====Luts====\\n')\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_lut(t)\n",
    "    print(f\"Lut {t}:\\n{dict(itertools.islice(d.items(), 2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b8e28-23ac-4ac1-a9b6-ae546e28c905",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Declare tasks for 2 of the 5 prediction tasks specified in the paper. We will create dataloaders for each task that contain the ICD codes and the raw text for each (patient, visit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69f4f2-566a-428f-800a-8539fbb16a15",
   "metadata": {},
   "source": [
    "#### CodeEMB Pred tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08692aad-db67-487b-9b26-dbde0ab6adce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "# def readmission_pred_task(patient, time_window=3):\n",
    "#     \"\"\"\n",
    "#     patient is a <pyhealth.data.Patient> object\n",
    "#     \"\"\"\n",
    "#     samples = []\n",
    "\n",
    "#     # we will drop the last visit\n",
    "#     for i in range(len(patient) - 1):\n",
    "#         visit: Visit = patient[i]\n",
    "#         next_visit: Visit = patient[i + 1]\n",
    "\n",
    "#         # step 1: define the readmission label \n",
    "#         # get time difference between current visit and next visit\n",
    "#         time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "#         readmission_label = 1 if time_diff < time_window else 0\n",
    "        \n",
    "#         # step 2: get code-based feature information\n",
    "#         conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "#         procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "#         drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "#         drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "#         # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "#         if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "#             continue\n",
    "#         if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "#             # Exclude stays with less than 5 procedures.\n",
    "#             continue\n",
    "        \n",
    "#         # step 3.5: build text lists from the ICD codes\n",
    "#         diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "#         proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "#         conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "#         procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "#         drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "#                       for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "#         # step 4: assemble the samples\n",
    "#         # TODO: should also exclude visit with age < 18\n",
    "#         samples.append(\n",
    "#             {\n",
    "#                 \"visit_id\": visit.visit_id,\n",
    "#                 \"patient_id\": patient.patient_id,\n",
    "#                 # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "#                 \"conditions\": conditions,\n",
    "#                 \"procedures\": procedures,\n",
    "#                 \"conditions_text\": conditions_text,\n",
    "#                 \"procedures_text\": procedures_text,\n",
    "#                 \"drugs\": drugs,\n",
    "#                 \"drugs_text\": drugs_text,\n",
    "#                 # \"labevents\": labevents,\n",
    "#                 # \"labevents_text\": labevents_text\n",
    "#                 \"label\": readmission_label,\n",
    "#             }\n",
    "#         )\n",
    "#     # no cohort selection\n",
    "#     return samples\n",
    "\n",
    "\n",
    "# def mortality_pred_task(MORTALITY_PER_VISIT_ICD_9_CODE_COUNT_, patient):\n",
    "#     \"\"\"\n",
    "#     patient is a <pyhealth.data.Patient> object\n",
    "#     \"\"\"\n",
    "#     samples = []\n",
    "\n",
    "#     # loop over all visits but the last one\n",
    "#     for i in range(len(patient) - 1):\n",
    "\n",
    "#         # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "#         # there are no vists.attr_dict keys\n",
    "#         visit = patient[i]\n",
    "#         next_visit = patient[i + 1]\n",
    "#         # if i == 0: print(visit)\n",
    "\n",
    "#         # step 1: define the mortality_label\n",
    "#         if next_visit.discharge_status not in [0, 1]:\n",
    "#             mortality_label = 0\n",
    "#         else:\n",
    "#             mortality_label = int(next_visit.discharge_status)\n",
    "\n",
    "#         # step 2: get code-based feature information\n",
    "#         conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "#         procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "#         drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "#         drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "#         # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "#         # if i == 0: print(conditions)\n",
    "#         # if i == 0: print(procedures)\n",
    "#         # if i == 0: print(drugs)\n",
    "#         # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "#         # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "#         # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "#         if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "#             continue\n",
    "#         if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "#             # Exclude stays with less than 5 procedures.\n",
    "#             continue\n",
    "        \n",
    "#         # step 3.5: build text lists from the ICD codes\n",
    "#         diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "#         proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "#         # if i == 0: print(diag_lut)\n",
    "#         # if i == 0: print(proc_lut)\n",
    "#         # Index 0 is shortname, index 1 is longname.\n",
    "#         # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "#         # print(proc_lut.get(procedures[0]))\n",
    "#         conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "#         procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "#         drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "#                       for d in [d.attr_dict for d in drugs_full]]\n",
    "#         # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "#         # labevents_text =\n",
    "        \n",
    "#         # step 4: assemble the samples\n",
    "#         samples.append(\n",
    "#             {\n",
    "#                 \"visit_id\": visit.visit_id,\n",
    "#                 \"patient_id\": patient.patient_id,\n",
    "#                 # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "#                 \"conditions\": conditions,\n",
    "#                 \"procedures\": procedures,\n",
    "#                 \"conditions_text\": conditions_text,\n",
    "#                 \"procedures_text\": procedures_text,\n",
    "#                 \"drugs\": drugs,\n",
    "#                 \"drugs_text\": drugs_text,\n",
    "#                 # \"labevents\": labevents,\n",
    "#                 # \"labevents_text\": labevents_text\n",
    "#                 \"label\": mortality_label,\n",
    "#             }\n",
    "#         )\n",
    "#     return samples\n",
    "\n",
    "def readmission_pred_task_cemb(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    each sample is a list of vists for 1 patient.\n",
    "    \"\"\"\n",
    "   \n",
    "    # TODO(botelho3) - stupid hack around the limitations of SampleDataset validator.\n",
    "    kMaxVisits = 5\n",
    "    if len(patient) < 1 or len(patient) > kMaxVisits:\n",
    "        return []\n",
    "        \n",
    "    samples = [{\n",
    "        \"visit_id\": patient[0].visit_id,\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"num_visits\": 0,\n",
    "        \"conditions\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures\": [[] for v in range(kMaxVisits)],\n",
    "        \"conditions_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs_text\": [[] for v in range(kMaxVisits)],\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": 0,\n",
    "    }]\n",
    "\n",
    "    # They were admitted > 1 time.\n",
    "    global_readmission_label = 1 if len(patient) > 1 else 0\n",
    "\n",
    "    # loop over all visits\n",
    "    out_idx = 0\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], str(d['dose']), d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        # Each field is a list of visits.\n",
    "        samples[0]['num_visits'] = samples[0]['num_visits'] + 1\n",
    "        samples[0]['conditions'][out_idx]=conditions\n",
    "        samples[0]['procedures'][out_idx]=procedures\n",
    "        samples[0]['conditions_text'][out_idx]= conditions_text\n",
    "        samples[0]['procedures_text'][out_idx] = procedures_text\n",
    "        samples[0]['drugs'][out_idx] = drugs\n",
    "        samples[0]['drugs_text'][out_idx] = drugs_text\n",
    "        out_idx = out_idx + 1\n",
    "    samples[0]['label'] = global_readmission_label\n",
    "   \n",
    "    # Record all unique codes and their frequency for LUT.\n",
    "    for visit in samples[0]['conditions']:\n",
    "        for code in visit:\n",
    "            CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "           \n",
    "    # If none of the samples met the criteria return an empty list.\n",
    "    if samples[0]['num_visits'] == 0:\n",
    "        return []\n",
    "\n",
    "    # Potentially multiply the sample n-times to increase dataset size.\n",
    "    samples.extend(\n",
    "        list(deepcopy(samples[0]) for s in range(SAMPLE_MULTIPLIER_-1))\n",
    "    )\n",
    "        \n",
    "    return samples\n",
    "    \n",
    "    \n",
    "def mortality_pred_task_cemb(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    each sample is a list of vists for 1 patient.\n",
    "    \"\"\"\n",
    "   \n",
    "    # TODO(botelho3) - stupid hack around the limitations of SampleDataset validator.\n",
    "    kMaxVisits = 5\n",
    "    if len(patient) < 1 or len(patient) > kMaxVisits:\n",
    "        return []\n",
    "        \n",
    "    samples = [{\n",
    "        \"visit_id\": patient[0].visit_id,\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"num_visits\": 0,\n",
    "        \"conditions\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures\": [[] for v in range(kMaxVisits)],\n",
    "        \"conditions_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs_text\": [[] for v in range(kMaxVisits)],\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": 0,\n",
    "    }]\n",
    "    \n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "\n",
    "    # loop over all visits\n",
    "    out_idx = 0\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], str(d['dose']), d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        # Each field is a list of visits.\n",
    "        samples[0]['num_visits'] = samples[0]['num_visits'] + 1\n",
    "        samples[0]['conditions'][out_idx]=conditions\n",
    "        samples[0]['procedures'][out_idx]=procedures\n",
    "        samples[0]['conditions_text'][out_idx]= conditions_text\n",
    "        samples[0]['procedures_text'][out_idx] = procedures_text\n",
    "        samples[0]['drugs'][out_idx] = drugs\n",
    "        samples[0]['drugs_text'][out_idx] = drugs_text\n",
    "        out_idx = out_idx + 1\n",
    "    samples[0]['label'] = global_mortality_label\n",
    "   \n",
    "    # Record all unique codes and their frequency for LUT.\n",
    "    for visit in samples[0]['conditions']:\n",
    "        for code in visit:\n",
    "            CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "           \n",
    "    # If none of the samples met the criteria return an empty list.\n",
    "    if samples[0]['num_visits'] == 0:\n",
    "        return []\n",
    "\n",
    "    # Potentially multiply the sample n-times to increase dataset size.\n",
    "    samples.extend(\n",
    "        list(deepcopy(samples[0]) for s in range(SAMPLE_MULTIPLIER_-1))\n",
    "    )\n",
    "        \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30d309-9b95-4a1b-82ee-ae141960c868",
   "metadata": {},
   "source": [
    "#### DescEmb Pred Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ef5a4-fa6d-4720-a4e7-e91cc3746ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "#{code: idx for idx, code in enumerate(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys())}\n",
    "def readmission_pred_task_demb(CODE_COUNT, patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "   \n",
    "    # Length 1 patients by defn are not readmitted.\n",
    "    if len(patient) < 1:\n",
    "        return samples\n",
    "\n",
    "    # we will drop the last visit\n",
    "    global_readmission_label = 0\n",
    "    global_readmission_label = 1 if len(patient) > 1 else 0\n",
    "#     for i in range(len(patient) - 1):\n",
    "#         visit: Visit = patient[i]\n",
    "#         next_visit: Visit = patient[i + 1]\n",
    "\n",
    "#         # step 1: define the readmission label \n",
    "#         # get time difference between current visit and next visit\n",
    "#         time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "#         readmission_label = 1 if time_diff < time_window else 0\n",
    "#         global_readmission_label |= readmission_label  \n",
    "       \n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit: Visit = patient[i]\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs = [x.code for x in visit.get_event_list(table=\"PRESCRIPTIONS\")]\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_readmission_label,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return samples\n",
    "    \n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data)) - 1\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size \n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs, drugs_pad = pad_field(\"drugs\", visits, '0')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    assert(drugs_pad == drugs_text_pad)\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs\": drugs,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_pad\": drugs_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_readmission_label,\n",
    "    }\n",
    "    # sample = {\n",
    "    #     \"patient_id\": patient.patient_id,\n",
    "    #     # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "    #     \"visit_id\": visits[0][\"visit_id\"],\n",
    "    #     \"num_visits\": num_visits,\n",
    "    #     # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "    #     \"conditions\": [[x[\"conditions\"] for x in visits]],\n",
    "    #     \"procedures\": [[x[\"procedures\"] for x in visits]],\n",
    "    #     \"conditions_text\": [[x[\"conditions_text\"] for x in visits]],\n",
    "    #     \"procedures_text\": [[x[\"procedures_text\"] for x in visits]],\n",
    "    #     \"drugs\": [[[x[\"drugs\"] for x in visits]]],\n",
    "    #     \"drugs_text\": [[x[\"drugs_text\"] for x in visits]],\n",
    "    #     # \"labevents\": labevents,\n",
    "    #     # \"labevents_text\": labevents_text\n",
    "    #     \"label\": global_readmission_label,\n",
    "    # }\n",
    "    for code in sample['conditions']:\n",
    "        CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "\n",
    "    samples.extend(\n",
    "        list(deepcopy(sample) for s in range(SAMPLE_MULTIPLIER_))\n",
    "    )\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_task_demb(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "\n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "        \n",
    "    # loop over all visits but the last one\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit: Visit.\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs = [x.code for x in visit.get_event_list(table=\"PRESCRIPTIONS\")]\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples into a pyHealth Visit.\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_mortality_label,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return [] \n",
    "   \n",
    "    # pyHealth requires that all list fields in sample are equal size.\n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data))\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size\n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs, drugs_pad = pad_field(\"drugs\", visits, '0')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    assert(drugs_pad == drugs_text_pad)\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs\": drugs,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_pad\": drugs_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_mortality_label,\n",
    "    }\n",
    "    # sample = {\n",
    "    #     \"patient_id\": patient.patient_id,\n",
    "    #     # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "    #     \"visit_id\": visits[0][\"visit_id\"],\n",
    "    #     \"num_visits\": num_visits,\n",
    "    #     # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "    #     \"conditions\": [[x[\"conditions\"] for x in visits]],\n",
    "    #     \"procedures\": [[x[\"procedures\"] for x in visits]],\n",
    "    #     \"conditions_text\": [[x[\"conditions_text\"] for x in visits]],\n",
    "    #     \"procedures_text\": [[x[\"procedures_text\"] for x in visits]],\n",
    "    #     \"drugs\": [[[x[\"drugs\"] for x in visits]]],\n",
    "    #     \"drugs_text\": [[x[\"drugs_text\"] for x in visits]],\n",
    "    #     # \"labevents\": labevents,\n",
    "    #     # \"labevents_text\": labevents_text\n",
    "    #     \"label\": global_mortality_label,\n",
    "    # }\n",
    "   \n",
    "   \n",
    "    # For every condition in the sample (all visits). Record frequency.\n",
    "    # Will be used to build code->index LUT.\n",
    "    for code in sample['conditions']:\n",
    "        CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "\n",
    "    if SAMPLE_MULTIPLIER_:\n",
    "        samples.extend(\n",
    "            list(deepcopy(sample) for s in range(SAMPLE_MULTIPLIER_))\n",
    "        )\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371b36c-571d-482e-86b2-bbfb92522a24",
   "metadata": {},
   "source": [
    "#### Test Load Readmission Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d047ff-6412-462c-9294-a5ba2d3bdba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_task() returns a SampleEHRDataset object\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "task_fn = functools.partial(readmission_pred_task_demb, READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_)\n",
    "readm_dataset = mimic3base.set_task(task_fn, task_name=readmission_pred_task_demb.__name__)\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "readm_dataset.stat()\n",
    "readm_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "print(f\"READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(READMISSION_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "      f\"{READMISSION_PER_PATIENT_ICD_9_CODE2IDX_}\")\n",
    "del readm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2738557-c9c4-4a33-9fd6-b2dd00bd1d22",
   "metadata": {},
   "source": [
    "#### Test Load Mortality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70586b66-a814-4898-892b-d1bdd339ce7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "task_fn = functools.partial(mortality_pred_task_demb, MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_)\n",
    "mor_dataset = mimic3base.set_task(task_fn, task_name=mortality_pred_task_demb.__name__)\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "mor_dataset.stat()\n",
    "mor_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "print(f\"MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "      f\"{MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_}\")\n",
    "del mor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dcd2f-cb6f-4b21-b692-4526d6aff390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "def readmission_pred_no_visit_task(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_no_visit_task(patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient) - 1):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit = patient[i]\n",
    "        next_visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        if next_visit.discharge_status not in [0, 1]:\n",
    "            mortality_label = 0\n",
    "        else:\n",
    "            mortality_label = int(next_visit.discharge_status)\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0: continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": mortality_label,\n",
    "            }\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "\n",
    "# mimic3sample = mimic3base.set_task(task_fn=drug_recommendation_mimic3_fn) # use default task\n",
    "# train_ds, val_ds, test_ds = split_by_patient(mimic3sample, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0281a-f95b-4991-abd5-5824fc37c520",
   "metadata": {},
   "source": [
    "### DataLoaders and Collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9be7a-3736-4bda-af61-5dcf331bb664",
   "metadata": {},
   "source": [
    "#### Bert Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c8e3c-0482-4688-b2d6-d83d490060a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def bert_per_patient_collate_function(data):\n",
    "    \"\"\"\n",
    "    Collates a tensor of (batch_size, seq_len, bert_emb_len) i.e. (32, <events per patient>, 768)\n",
    "    Need to pad the second dimension to max(<variable_per_patient>).\n",
    "    \n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (batch_size, max #conditions, max embedding size) of type torch.float\n",
    "        masks: a tensor of shape (batch_size, max #conditions, max embedding_size) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (batch_size) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    # print(f\"bert_per_patient_collate_function data[0] {data[0]}\")\n",
    "    tmp, labels = zip(*data)\n",
    "    sequences = tmp \n",
    "    if not torch.is_tensor(tmp[0]):\n",
    "        sequences = [torch.from_numpy(x) for x in tmp]\n",
    "    \n",
    "    # Quick stats on the amount of memory in each batch.\n",
    "    sizes = [t.element_size() * t.nelement() for t in sequences]\n",
    "    print(f'BertCollate tensor sizes\\n'\n",
    "          f'cum_size:{sum(sizes) / 1.0e6} MB\\n'\n",
    "          f'sizes:{sizes}\\n'\n",
    "          f'{[tuple(x.shape) for x in sequences]}\\n')\n",
    "    \n",
    "    # Convert output labels for each sample in batch to tensor.\n",
    "    # shape: (batch_size, 1)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "   \n",
    "    num_patients = len(sequences)\n",
    "    num_events = [patient.shape[0] for patient in sequences]\n",
    "    embedding_length = [patient.shape[1] for patient in sequences]\n",
    "\n",
    "    max_num_events = max(num_events)\n",
    "    max_embedding_length = max(embedding_length)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    rev_x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    # Mask dimensions are 1 less than inputs.\n",
    "    masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        # Patient (#events, 768)\n",
    "        j_visits = patient.shape[0]\n",
    "        # for j_visit, visit in enumerate(patient):\n",
    "        \"\"\"\n",
    "        TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "        \"\"\"\n",
    "        # l = len(visit)\n",
    "        x[i_patient, :j_visits, :] = patient[:, :].unsqueeze(0)\n",
    "        # The tensor is (seq_length, emb_size). Leave embeddings,\n",
    "        # flip temporal order of code/event sequence.\n",
    "        rev_x[i_patient, :j_visits, :] = torch.flip(patient, dims=[0]).unsqueeze(0)\n",
    "        masks[i_patient, :j_visits] = 1\n",
    "        rev_masks[i_patient, :j_visits] = 1\n",
    "      \n",
    "        # TODO(botelho3) - comment this out to reduce spew.\n",
    "        # if i_patient == 0:\n",
    "        #     print(f\"------ p: {i_patient} ------\")\n",
    "        #     print(x[i_patient, :, :25])\n",
    "        #     print(rev_x[i_patient, :, :25])\n",
    "        #     print(masks[i_patient, :, :25])\n",
    "        #     print(rev_masks[i_patient, :, :25])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "def bert_per_patient_collate_function_new_trainer(data):\n",
    "    \"\"\"\n",
    "    Collates a tensor of (batch_size, seq_len, bert_emb_len) i.e. (32, <events per patient>, 768)\n",
    "    Need to pad the second dimension to max(<variable_per_patient>).\n",
    "    \n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (batch_size, max #conditions, max embedding size) of type torch.float\n",
    "        masks: a tensor of shape (batch_size, max #conditions, max embedding_size) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (batch_size) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    # print(f\"bert_per_patient_collate_function data[0] {data[0]}\")\n",
    "    tmp, labels = zip(*data)\n",
    "    sequences = None\n",
    "    if not torch.is_tensor(tmp[0]):\n",
    "        sequences = [torch.from_numpy(x) for x in tmp]\n",
    "    \n",
    "    # Quick stats on the amount of memory in each batch.\n",
    "    sizes = [t.element_size() * t.nelement() for t in sequences]\n",
    "    \n",
    "    # Convert output labels for each sample in batch to tensor.\n",
    "    # shape: (batch_size, 1)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "   \n",
    "    num_patients = len(sequences)\n",
    "    num_events = [patient.shape[0] for patient in sequences]\n",
    "    embedding_length = [patient.shape[1] for patient in sequences]\n",
    "\n",
    "    max_num_events = max(num_events)\n",
    "    max_embedding_length = max(embedding_length)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    rev_x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    # Mask dimensions are 1 less than inputs.\n",
    "    masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        # Patient (#events, 768)\n",
    "        j_visits = patient.shape[0]\n",
    "        # for j_visit, visit in enumerate(patient):\n",
    "        \"\"\"\n",
    "        TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "        \"\"\"\n",
    "        # l = len(visit)\n",
    "        x[i_patient, :j_visits, :] = patient[:, :].unsqueeze(0)\n",
    "        # The tensor is (seq_length, emb_size). Leave embeddings,\n",
    "        # flip temporal order of code/event sequence.\n",
    "        rev_x[i_patient, :j_visits, :] = torch.flip(patient, dims=[0]).unsqueeze(0)\n",
    "        masks[i_patient, :j_visits] = 1\n",
    "        rev_masks[i_patient, :j_visits] = 1\n",
    "      \n",
    "        # TODO(botelho3) - comment this out to reduce spew.\n",
    "        # if i_patient == 0:\n",
    "        #     print(f\"------ p: {i_patient} ------\")\n",
    "        #     print(x[i_patient, :, :25])\n",
    "        #     print(rev_x[i_patient, :, :25])\n",
    "        #     print(masks[i_patient, :, :25])\n",
    "        #     print(rev_masks[i_patient, :, :25])\n",
    "    return {\n",
    "        'x': x,\n",
    "        'masks': masks,\n",
    "        'rev_x': rev_x,\n",
    "        'rev_masks': rev_masks,\n",
    "        'y': y,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32036013-ce5d-407b-af7a-312cf2bc5f15",
   "metadata": {},
   "source": [
    "#### Bert Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afd1f5-0f3b-4f4b-9c85-f74bb367ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use DistilBERT to decrease embedding time.\n",
    "# https://huggingface.co/docs/transformers/model_doc/distilbert?highlight=distilberttokenizerfast#distilbert\n",
    "\n",
    "class BertTextEmbedTransform(object):\n",
    "    \"\"\"Transform a sample's (a single visit's) text into 1 embedding vector.\n",
    "    \n",
    "    The embeddings of each text field are combined by embedding\n",
    "    each separately then summing.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/69517460/bert-get-sentence-embedding\n",
    "    # https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
    "    # https://gmihaila.github.io/tutorial_notebooks/bert_inner_workings/\n",
    "\n",
    "    def __init__(self, bert_model: Any, embedding_size: int, use_tokenizer_fast: bool):\n",
    "        assert isinstance(embedding_size, (int, tuple))\n",
    "        self.use_gpu = BERT_USE_GPU_\n",
    "        self.cuda = GPU_STR_\n",
    "        self.cache_dir = os.path.join(os.getcwd(), CACHE_DIR_)\n",
    "        self.bert_config = BertConfig()\n",
    "        # self.bert_config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        # self.bert_model = BertModel(self.bert_config).from_pretrained('bert-base-uncased', config=self.bert_config)\n",
    "        # self.bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        with torch.no_grad():\n",
    "            self.bert_model = BertModel(self.bert_config).from_pretrained('bert-base-uncased')\n",
    "        if self.use_gpu:\n",
    "            self.bert_model.to(self.cuda)\n",
    "        self.bert_config = self.bert_model.config\n",
    "        self.bert_model.eval()\n",
    "        # We unfortunately can't put the tokenizer on the GPU.\n",
    "        # https://stackoverflow.com/questions/66096703/running-huggingface-bert-tokenizer-on-gpu\n",
    "        if use_tokenizer_fast:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "    \n",
    "    def _tokenize_text(self, text: str) -> str:\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        return tokenized_text\n",
    "    \n",
    "    def _get_embeddings_of_sentences_with_mask(self, field, pad) -> torch.tensor:\n",
    "        return self._get_embeddings_of_sentences(field[:pad])\n",
    "        \n",
    "    \n",
    "    def _get_embeddings_of_sentences(self, sentences: List[str]) -> torch.tensor:\n",
    "        # tokenized_sentences = [self.tokenizer.tokenize(t, padding=True) for t in sentences]\n",
    "        # Tokenize the input sentence with attention masks.\n",
    "        batch_enc = self.tokenizer.batch_encode_plus(sentences, padding=True,\n",
    "                                return_attention_mask=True, return_length=True)\n",
    "        # print(f'input sentence: {sentences[0]}\\n'\n",
    "        #       f'token sentence: {self.tokenizer.decode(batch_enc[\"input_ids\"][0])}\\n')\n",
    "        batch_enc_tensor = batch_enc.convert_to_tensors(tensor_type=TensorType.PYTORCH)\n",
    "        if self.use_gpu:\n",
    "            batch_enc_tensor.to(self.cuda)\n",
    "      \n",
    "        # Run BERT model forward pass on tokenized input.\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.bert_model(input_ids=batch_enc_tensor['input_ids'],\n",
    "                                         attention_mask=batch_enc_tensor['attention_mask'],\n",
    "                                         token_type_ids=batch_enc_tensor['token_type_ids'],\n",
    "                                         # could turn off so we're faster\n",
    "                                         output_attentions=True)\n",
    "        # embeddings, _ = self.bert_model(**batch_enc)\n",
    "        # print(f'embeddings:\\n {dir(embeddings)}')\n",
    "        #attention = encoded['attention_mask'].reshape((lhs.size()[0], lhs.size()[1], -1)).expand(-1, -1, 768)\n",
    "       \n",
    "        # These may be on GPU.\n",
    "        return embeddings.last_hidden_state, embeddings.attentions \n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        len_embeddings = (len(sample['conditions_text']) +\n",
    "                          len(sample['procedures_text']) +\n",
    "                          len(sample['drugs_text']))\n",
    "      \n",
    "        sample_text = []\n",
    "        sample_masks = []\n",
    "        if sample.get('conditions_text_pad'):\n",
    "            pad = sample['conditions_text_pad']\n",
    "            sample_text.extend(sample['conditions_text'][:pad])\n",
    "        if sample.get('procedures_text_pad'):\n",
    "            pad = sample['procedures_text_pad']\n",
    "            sample_text.extend(sample['procedures_text'][:pad])\n",
    "        if sample.get('drugs_text_pad'):\n",
    "            pad = sample['drugs_text_pad']\n",
    "            sample_text.extend(sample['drugs_text'][:pad])\n",
    "           \n",
    "        sample_embeddings, sample_attentions = self._get_embeddings_of_sentences(sample_text)\n",
    "        \n",
    "        # https://stackoverflow.com/questions/61323621/how-to-understand-hidden-states-of-the-returns-in-bertmodelhuggingface-transfo\n",
    "        # https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important\n",
    "        # Take only the last hidden state embeddings from BERT.\n",
    "        # We need to take index 0, because it's the CLS token prepended to our sentence.\n",
    "        # The [CLS] represents the meaning of the whole sentence.\n",
    "        # embeddings = torch.squeeze(sample_embeddings[:, -1, :], dim=1)\n",
    "        embeddings = torch.squeeze(sample_embeddings[:, 0, :], dim=1)\n",
    "        if self.use_gpu:\n",
    "            assert(embeddings.device == torch.device('cuda:0'))\n",
    "        else:\n",
    "            assert(embeddings.device == torch.device('cpu'))\n",
    "        if self.use_gpu:\n",
    "            embeddings = embeddings.to('cpu')\n",
    "        \n",
    "        # We could multiply by attentions here:\n",
    "        # attentions = torch.squeeze(sample_attentions[:, -1, :], dim=1)\n",
    "        # embeddings = embeddings * attentions\n",
    "\n",
    "        # The 1st dimension is seq length. The second dimension is embedding length of each sentence.\n",
    "        assert(embeddings.shape[-1] == self.bert_config.hidden_size)\n",
    "        assert(len(embeddings.shape) == 2)\n",
    "        assert(embeddings.dtype == torch.float)\n",
    "        \n",
    "        # Return the ((model_inputs), label) \n",
    "        return (embeddings, sample['label'])\n",
    "    \n",
    "    \n",
    "class TextEmbedDataset(SampleDataset):\n",
    "    '''The BERT text embedding process is very slow. We want to avoid it.\n",
    "   \n",
    "    To prevent re-processing of the same input cache the sample locally.\n",
    "    Some suggestions here:\n",
    "        https://stackoverflow.com/questions/61393613/pytorch-speed-up-data-loading.\n",
    "        https://discuss.pytorch.org/t/best-practice-to-cache-the-entire-dataset-during-first-epoch/19608\n",
    "        \n",
    "    1. Preprocess and write the preprocessed text back out to disk.\n",
    "    2. Cache the transform output in a hashtable. See functools.lru_cache().\n",
    "    3. https://pytorch.org/data/main/ ?\n",
    "    \n",
    "    Some concerns related to num_workers > 1, i.e. multiprocessing enabled.\n",
    "    See torch.save() to cache a tensor.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dataset: SampleDataset, transform=None, should_cache=True):\n",
    "        \"\"\"Wraps a SampleEHRDataset with transforms.\n",
    "        Arguments:\n",
    "            dataset: dataset to transform\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.transformed = [False for x in dataset]\n",
    "        self.should_cache = should_cache\n",
    "        super().__init__([x for x in dataset])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            assert(False)\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Cache the transformed version of the data.\n",
    "        sample = None\n",
    "        if self.should_cache and self.transform and not self.transformed[idx]:\n",
    "            self.samples[idx] = self.transform(self.samples[idx])\n",
    "            self.transformed[idx] = True\n",
    "            sample = self.samples[idx]\n",
    "        elif not self.should_cache:\n",
    "            sample = self.transform(self.samples[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056b60a-5861-4e47-b694-891053cc8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_xform = BertTextEmbedTransform(None, EMBEDDING_DIM_, use_tokenizer_fast=True)\n",
    "BERT_EMBEDDING_SIZE = bert_xform.bert_config.hidden_size\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "MORT_DEMB_CODE2IDX = {}\n",
    "MORT_DEMB_CODE_COUNT = {}\n",
    "task_fn = functools.partial(mortality_pred_task_demb, MORT_DEMB_CODE_COUNT)\n",
    "mortality_demb_dataset = mimic3base.set_task(task_fn, task_name=mortality_pred_task_demb.__name__)\n",
    "MORT_DEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORT_DEMB_CODE_COUNT.keys()))\n",
    "}\n",
    "print(f\"MORT_DEMB_CODE2IDX {len(MORT_DEMB_CODE2IDX)}\")\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "mortality_demb_dataset = TextEmbedDataset(mortality_demb_dataset, transform=bert_xform)\n",
    "\n",
    "\n",
    "# mort_demb_train_ds, mort_demb_val_ds, mort_demb_test_ds = split_by_patient(mortality_demb_dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# # create dataloaders (torch.data.DataLoader)\n",
    "# # mort_train_loader = get_dataloader(train_ds, batch_size=32, shuffle=True, collate_fn)\n",
    "# # mort_val_loader = get_dataloader(val_ds, batch_size=32, shuffle=False)\n",
    "# # mort_test_loader = get_dataloader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# mort_demb_train_loader = DataLoader(\n",
    "#     mort_demb_train_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_per_patient_collate_function,\n",
    "# )\n",
    "# mort_demb_val_loader = DataLoader(\n",
    "#     mort_demb_val_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_per_patient_collate_function,\n",
    "# )\n",
    "# mort_demb_test_loader = DataLoader(\n",
    "#     mort_demb_test_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_per_patient_collate_function,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c8c52-edda-4bdd-95ef-a40106028915",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_xform = BertTextEmbedTransform(None, EMBEDDING_DIM_, use_tokenizer_fast=True)\n",
    "BERT_EMBEDDING_SIZE = bert_xform.bert_config.hidden_size\n",
    "\n",
    "# Readmission Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "READM_DEMB_CODE2IDX = {}\n",
    "READM_DEMB_CODE_COUNT = {}\n",
    "task_fn = functools.partial(readmission_pred_task_demb, READM_DEMB_CODE_COUNT)\n",
    "readmission_demb_dataset = mimic3base.set_task(task_fn, task_name=readmission_pred_task_demb.__name__)\n",
    "READM_DEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(READM_DEMB_CODE_COUNT.keys()))\n",
    "}\n",
    "print(f\"READM_DEMB_CODE2IDX {len(READM_DEMB_CODE2IDX)}\")\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "readmission_demb_dataset = TextEmbedDataset(readmission_demb_dataset, transform=bert_xform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fccee-d5da-428c-a54a-0678a2d19b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Verify DataLoader properties.\n",
    "    # Quick test without running the whole RNN training process.\n",
    "\n",
    "    # from torch.utils.data import DataLoader\n",
    "\n",
    "    # loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "    loader_iter = iter(mort_demb_train_loader)\n",
    "    # for _ in loader_iter:\n",
    "    #     pass\n",
    "    try:\n",
    "        x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "    except StopIteration as e:\n",
    "        print(e)\n",
    "\n",
    "    assert x.dtype == torch.float\n",
    "    assert rev_x.dtype == torch.float\n",
    "    assert y.dtype == torch.float\n",
    "    assert masks.dtype == torch.bool\n",
    "    assert rev_masks.dtype == torch.bool\n",
    "\n",
    "    assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "    assert y.shape == (BATCH_SIZE_, 1)\n",
    "    assert masks.shape == (BATCH_SIZE_, 10, 3)\n",
    "\n",
    "    # assert x[0][0].sum() == 9\n",
    "    # assert masks[0].sum() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c1852-2654-4d7c-9caa-4cd29ae6fd91",
   "metadata": {},
   "source": [
    "#### CodeEmb Collate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992467f7-d834-4d4a-872a-714389fa203c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def code_emb_per_visit_collate_function(code2idx: Dict[str, int], data: List[Any]) -> Tuple[Any]:\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "    sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "    is stored in `mask`.\n",
    "                                   \n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "    x: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.long\n",
    "    masks: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.bool\n",
    "    rev_x: same as x but in reversed time. This will be used in our RNN model for masking\n",
    "    rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "    y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\\n\",\n",
    "    using: `sequences, labels = zip(*data)`\\n\",\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = data\n",
    "    \n",
    "    y = torch.tensor([s['label'] for s in samples], dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(samples)\n",
    "    num_visits = [patient['num_visits'] for patient in samples]\n",
    "    num_codes = []\n",
    "    for patient_idx, _ in enumerate(num_visits):\n",
    "        num_codes.extend([len(visit) for visit in samples[patient_idx]['conditions']])\n",
    "        \n",
    "    # print(f'num samples: {len(samples)}')\n",
    "    # print(f'num visits: {num_visits}')\n",
    "    # print(f'num codes: {num_codes}')\n",
    "    # print(f'code_emb_per_patient_collate_function data[0]:\\n{data[0]}')\n",
    " \n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    # max_num_codes = len(MORTALITY_PER_VISIT_ICD_9_CODE2IDX_)\n",
    "    assert(max_num_codes > 0)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(samples):\n",
    "        nvisits = patient['num_visits']\n",
    "        for j_visit in range(nvisits):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            codes = patient['conditions'][j_visit] \n",
    "            indices = [code2idx[code] for code in codes]\n",
    "            l = len(codes)\n",
    "            if len(indices) < 1:\n",
    "                print(f'No code indices after lookup')\n",
    "                print(f'codes {codes}'+'\\n'+f'indices{indices}')\n",
    "                print(f'patient\\n{patient}')\n",
    "                assert(len(indices) >= 1)\n",
    "            # for idx in indices:\n",
    "            x[i_patient, j_visit, 0:l] = torch.tensor(indices)\n",
    "            rev_x[i_patient, nvisits-1-j_visit, 0:l] = torch.tensor(indices)\n",
    "            masks[i_patient, j_visit, 0:l] = 1\n",
    "            rev_masks[i_patient, nvisits-1-j_visit, 0:l] = 1\n",
    "        # if i_patient == 0:\n",
    "        #     print(f\"------ code_emb_per_visit_collate_function p: {i_patient} ------\")\n",
    "        #     print(x[i_patient, :, ])\n",
    "        #     print(rev_x[i_patient, :, ])\n",
    "        #     print(masks[i_patient, :, ])\n",
    "        #     print(rev_masks[i_patient, :, ])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9670e0-b22c-4384-b8ad-f0fca7f1a80e",
   "metadata": {},
   "source": [
    "#### CodeEmb Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4ae73-f079-47ae-ae6f-890d0c5fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "MORT_CEMB_CODE2IDX = {}\n",
    "MORT_CEMB_CODE_COUNT = {}\n",
    "mortality_cemb_ds = mimic3base.set_task(\n",
    "    task_fn=functools.partial(mortality_pred_task_cemb, MORT_CEMB_CODE_COUNT),\n",
    "    task_name=mortality_pred_task_cemb.__name__)\n",
    "# The set_task(...) function iterates over all samples.\n",
    "# Applying the task to each before returning a new dataset.\n",
    "# Since all samples have been processed we have observed all codes and can\n",
    "# build the code->index LUT.\n",
    "MORT_CEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(set(MORT_CEMB_CODE_COUNT.keys())))\n",
    "}\n",
    "print(f'MORT_CEMB_CODE2IDX {len(MORT_CEMB_CODE2IDX)}')\n",
    "\n",
    "# # We need to provide the code->index LUT to the collate function.\n",
    "wrap_code_emb_per_visit_collate_function = functools.partial(\n",
    "    code_emb_per_visit_collate_function,\n",
    "    MORT_CEMB_CODE2IDX)\n",
    "\n",
    "# # Split the dataset into train, val, and test.\n",
    "# mort_cemb_train_ds, mort_cemb_val_ds, mort_cemb_test_ds = split_by_patient(mortality_cemb_ds, [0.8, 0.1, 0.1])\n",
    "# mort_cemb_train_loader = DataLoader(\n",
    "#     mort_cemb_train_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    "# )\n",
    "# mort_cemb_val_loader = DataLoader(\n",
    "#     mort_cemb_val_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    "# )\n",
    "# mort_cemb_test_loader = DataLoader(\n",
    "#     mort_cemb_test_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718157a8-727d-4b1a-a416-320974a4d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "READM_CEMB_CODE2IDX = {}\n",
    "READM_CEMB_CODE_COUNT = {}\n",
    "readm_cemb_ds = mimic3base.set_task(\n",
    "    task_fn=functools.partial(readmission_pred_task_cemb, READM_CEMB_CODE_COUNT),\n",
    "    task_name=readmission_pred_task_cemb.__name__)\n",
    "# The set_task(...) function iterates over all samples.\n",
    "# Applying the task to each before returning a new dataset.\n",
    "# Since all samples have been processed we have observed all codes and can\n",
    "# build the code->index LUT.\n",
    "READM_CEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(set(READM_CEMB_CODE_COUNT.keys())))\n",
    "}\n",
    "print(f'READM_CEMB_CODE2IDX {len(READM_CEMB_CODE2IDX)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ac8e5-cab2-4540-b072-43a5b7404dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Verify DataLoader properties.\n",
    "    # Quick test without running the whole RNN training process.\n",
    "\n",
    "    # from torch.utils.data import DataLoader\n",
    "\n",
    "    # loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "    loader_iter = iter(mort_cemb_train_loader)\n",
    "    for _ in loader_iter:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "    except StopIteration as e:\n",
    "        print(e)\n",
    "\n",
    "    # assert x.dtype == torch.float\n",
    "    # assert rev_x.dtype == torch.float\n",
    "    # assert y.dtype == torch.float\n",
    "    # assert masks.dtype == torch.bool\n",
    "    # assert rev_masks.dtype == torch.bool\n",
    "\n",
    "    # assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "    # assert y.shape == (BATCH_SIZE_, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97059047-b368-40b8-8d31-e59266d80c05",
   "metadata": {},
   "source": [
    "#### Dataset Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102a015b-943e-4a42-b9d6-5468169a0cc3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# The collate function idea doesn't work because collating requires the batch size.\n",
    "# The batch size can change so it doesn't really work.\n",
    "# Unless we can use https://pytorch.org/docs/stable/data.html#disable-automatic-batching\n",
    "# But then the train/val/test datasets & batch sizes are fixed after the initial save to file.\n",
    "# Should probably incorporate the batch size into the file hash.\n",
    "# That would also mean we should return \n",
    "\n",
    "\n",
    "# class SimpleDataset(Dataset):\n",
    "#     '''Wraps a list. For post cache datasets.\n",
    "#     '''\n",
    "#     def __init__(self, samples: Iterable):\n",
    "#         self.samples = samples\n",
    "        \n",
    "#     def __len__(self):\n",
    "#         return len(self.samples)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         return self.samples[index]\n",
    "    \n",
    "\n",
    "# class DatasetCacher:\n",
    "#     '''Caches a dataset to file. Returns dataset from file with loader if desired.\n",
    "#         Datasets are defined by 3 params:\n",
    "#             1. name\n",
    "#             2. batch_size\n",
    "#             3. length\n",
    "#     '''\n",
    "#     def __init__(self):\n",
    "#         pass\n",
    "   \n",
    "\n",
    "#     def _HashStr(self, s) -> str:\n",
    "#         return hashlib.md5(s.encode()).hexdigest() \n",
    "\n",
    "\n",
    "#     def _MakeMetadata(self, name: str, batch_size: int, length: int) -> Dict:\n",
    "#         metadata = {\n",
    "#             'auto_batch': batch_size == 0,\n",
    "#             'batch_size': batch_size,\n",
    "#             'length': length,\n",
    "#             'dataset_name': name\n",
    "#         }\n",
    "#         return metadata\n",
    "\n",
    "    \n",
    "#     def _GetHashString(self, args) -> str:\n",
    "#         '''Compute filename for cache.\n",
    "#         '''\n",
    "#         args_to_hash = [\n",
    "#             args['dataset_name'],\n",
    "#             args['auto_batch'],\n",
    "#             args['batch_size'],\n",
    "#             args['length']\n",
    "#         ]\n",
    "#         filename = '_'.join([\n",
    "#             self._HashStr(\"+\".join([str(arg) for arg in args_to_hash])),\n",
    "#             args['dataset_name'],\n",
    "#             'bs_' + str(args['batch_size']),\n",
    "#             'len_' + str(args['length']),\n",
    "#             '.pkl',\n",
    "#         ])\n",
    "#         return os.path.join(MODULE_CACHE_PATH, filename)\n",
    "\n",
    "    \n",
    "#     def DatasetToCacheFromLoader(self, loader: DataLoader,\n",
    "#                                  task_fn: Callable,\n",
    "#                                  batch_size: int=0,\n",
    "#                                  overwrite: bool=False,\n",
    "#                                  extra_data: Dict=None):\n",
    "#         to_pickle = {\n",
    "#             'dataset': [x for x in loader],\n",
    "#             'metadata': self._MakeMetadata(task_fn.__name__, batch_size, len(loader.dataset))\n",
    "#         }\n",
    "#         if extra_data:\n",
    "#             to_pickle['extra_data'] = extra_data\n",
    "#         fname = self._GetHashString(to_pickle['metadata'])\n",
    "#         logging.info(f'Caching dataset {to_pickle[\"metadata\"]} to file `{fname}`.')\n",
    "#         if not os.path.exists(fname) or overwrite:\n",
    "#             save_pickle(to_pickle, fname)\n",
    "#         return to_pickle['metadata']\n",
    "        \n",
    "    \n",
    "#     def DatasetToCache(self, dataset: Dataset,\n",
    "#                            task_fn: Callable,\n",
    "#                            batch_size: int=0,\n",
    "#                            overwrite: bool=False,\n",
    "#                            extra_data: Dict=None) -> None:\n",
    "#         '''Write all items in the dataset to file.\n",
    "        \n",
    "#           If the dataset already exists on disk this function is a no-op.\n",
    "#           The dataset is hashed by iterating over the whole dataset and writing\n",
    "#           all items to disk using pickle.\n",
    "#         '''\n",
    "#         to_pickle = {\n",
    "#             'dataset': [x for x in dataset],\n",
    "#             'metadata': self._MakeMetadata(task_fn.__name__, batch_size, len(dataset))\n",
    "#         }\n",
    "#         if extra_data:\n",
    "#             to_pickle['extra_data'] = extra_data\n",
    "#         fname = self._GetHashString(to_pickle['metadata'])\n",
    "#         logging.info(f'Caching dataset {to_pickle[\"metadata\"]} to file `{fname}`.')\n",
    "#         if not os.path.exists(fname) or overwrite:\n",
    "#             save_pickle(to_pickle, fname)\n",
    "#         return to_pickle['metadata']\n",
    "    \n",
    "    \n",
    "#     def DatasetFromCache(self, task_fn: Callable,\n",
    "#                          batch_size: int,\n",
    "#                          length: int) -> Dataset:\n",
    "#         '''Retrieve all items in the dataset from file.\n",
    "#         '''\n",
    "#         metadata = self._MakeMetadata(task_fn.__name__, batch_size, length)\n",
    "#         fname = self._GetHashString(metadata)\n",
    "#         logging.info(f'Loading cached dataset {metadata} from file `{fname}`.')\n",
    "#         d = load_pickle(fname)\n",
    "#         return SimpleDataset(d['dataset']), d['metadata'] \n",
    "   \n",
    "\n",
    "#     def DataloaderFromCache(self, *args, **kwargs) -> DataLoader:\n",
    "#         dataset, metadata = self.DatasetFromCache(*args, **kwargs)\n",
    "#         batch_size = metadata['batch_size']\n",
    "#         # Disable auto batch if batch_size=0. Good for already collated data.\n",
    "#         loader = DataLoader(dataset,\n",
    "#                    batch_size=None if batch_size < 2 else batch_size,\n",
    "#                    shuffle=False)\n",
    "#         return loader, metadata\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba52f9-55a3-4ab3-9ff0-a1fb3685fa4a",
   "metadata": {},
   "source": [
    "##### Cemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e3313-641c-406d-aba8-b1fd83df44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to cut down the data size the collate fn is going to have to happen later during batch run.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "wrap_code_emb_per_visit_collate_function = functools.partial(\n",
    "    code_emb_per_visit_collate_function,\n",
    "    MORT_CEMB_CODE2IDX)\n",
    "mort_cemb_loader = DataLoader(\n",
    "    mortality_cemb_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n",
    "extra_data = {\n",
    "    'code2idx': MORT_CEMB_CODE2IDX,\n",
    "    'embed_index_size': len(MORT_CEMB_CODE2IDX),\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "mort_cemb_cacher_metadata = cacher.DatasetToCacheFromLoader(mort_cemb_loader,\n",
    "                      mortality_pred_task_cemb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)\n",
    "# i = iter(mort_cemb_loader)\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6e300-dc9a-4049-815d-c1ab6fe44335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(mort_cemb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(mort_cemb_loader.dataset)}\n",
    "mort_cemb_loader, mort_cemb_metadata = (\n",
    "    cacher.DataloaderFromCache(mortality_pred_task_cemb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(mort_cemb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b87a71-15bd-4b7d-9412-e95a22451de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to cut down the data size the collate fn is going to have to happen later during batch run.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "wrap_code_emb_per_visit_collate_function = functools.partial(\n",
    "    code_emb_per_visit_collate_function,\n",
    "    READM_CEMB_CODE2IDX)\n",
    "readm_cemb_loader = DataLoader(\n",
    "    readm_cemb_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n",
    "extra_data = {\n",
    "    'code2idx': READM_CEMB_CODE2IDX,\n",
    "    'embed_index_size': len(READM_CEMB_CODE2IDX),\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "readm_cemb_cacher_metadata = cacher.DatasetToCacheFromLoader(readm_cemb_loader,\n",
    "                      readmission_pred_task_cemb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)\n",
    "# i = iter(mort_cemb_loader)\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b297dd3-bb56-425e-990b-2bcd6cf99dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(readm_cemb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(readm_cemb_loader.dataset)}\n",
    "readm_cemb_loader, readm_cemb_metadata = (\n",
    "    cacher.DataloaderFromCache(readmission_pred_task_cemb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(readm_cemb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d281471-8c3b-42c8-b153-12993d6c06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    del readm_cemb_ds\n",
    "    del readm_cemb_loader\n",
    "    del mortality_cemb_ds\n",
    "    del mortality_cemb_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ab8b7-2172-4657-a825-0b772a5780bc",
   "metadata": {},
   "source": [
    "##### Demb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591bc67-93b6-4217-a454-bd92d76b6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need packed_sequence for this.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "# indices = list(range(0, min(2000, len(mortality_demb_dataset)) ))\n",
    "indices = list(range(0, min(10000, len(mortality_demb_dataset)) ))\n",
    "print(len(mortality_demb_dataset))\n",
    "print(len(torch.utils.data.Subset(mortality_demb_dataset, indices)))\n",
    "# mort_demb_loader = DataLoader(\n",
    "#     torch.utils.data.Subset(mort_demb_train_ds, indices),\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_per_patient_collate_function,\n",
    "# )\n",
    "extra_data = {\n",
    "    'embed_index_size': 768,\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "\n",
    "def numpy_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    return (tensor.numpy(), label)\n",
    "\n",
    "def bytes_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    bio = io.BytesIO()\n",
    "    torch.save(tensor, bio)\n",
    "    return (bio.getvalue(), label)\n",
    "    \n",
    "# mort_demb_cacher_metadata = cacher.DatasetToCacheFromLoader(mort_demb_loader,\n",
    "#                       mortality_pred_task_demb,\n",
    "#                       batch_size=0,  # we already batched using loader+collate.\n",
    "#                       overwrite=True,\n",
    "#                       extra_data=extra_data)\n",
    "mort_demb_loader = DataLoader(\n",
    "    torch.utils.data.Subset(mortality_demb_dataset, indices),\n",
    "    batch_size=1,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=numpy_conversion_function,\n",
    "    # collate_fn=bytes_conversion_function,\n",
    ")\n",
    "mort_demb_cacher_metadata = cacher.DatasetToCacheFromLoader(mort_demb_loader,\n",
    "                      mortality_pred_task_demb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)\n",
    "# i = iter(mort_demb_loader)\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b458f-350d-4d81-b637-c802d44a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(mort_demb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(mort_demb_loader.dataset)}\n",
    "mort_demb_loader, mort_demb_metadata = (\n",
    "    cacher.DataloaderFromCache(mortality_pred_task_demb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(mort_demb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247d011-d158-4f78-b699-3d28da9004f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need packed_sequence for this.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "# indices = list(range(0, min(2000, len(readmission_demb_dataset)) ))\n",
    "indices = list(range(0, min(10000, len(readmission_demb_dataset)) ))\n",
    "print(len(readmission_demb_dataset))\n",
    "print(len(torch.utils.data.Subset(readmission_demb_dataset, indices)))\n",
    "extra_data = {\n",
    "    'embed_index_size': 768,\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "\n",
    "def numpy_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    return (tensor.numpy(), label)\n",
    "\n",
    "def bytes_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    bio = io.BytesIO()\n",
    "    torch.save(tensor, bio)\n",
    "    return (bio.getvalue(), label)\n",
    "    \n",
    "# mort_demb_cacher_metadata = cacher.DatasetToCacheFromLoader(mort_demb_loader,\n",
    "#                       mortality_pred_task_demb,\n",
    "#                       batch_size=0,  # we already batched using loader+collate.\n",
    "#                       overwrite=True,\n",
    "#                       extra_data=extra_data)\n",
    "readm_demb_loader = DataLoader(\n",
    "    torch.utils.data.Subset(readmission_demb_dataset, indices),\n",
    "    batch_size=1,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=numpy_conversion_function,\n",
    "    # collate_fn=bytes_conversion_function,\n",
    ")\n",
    "readm_demb_cacher_metadata = cacher.DatasetToCacheFromLoader(readm_demb_loader,\n",
    "                      readmission_pred_task_demb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)\n",
    "# i = iter(mort_demb_loader)\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db8b92-221a-417d-b33b-46a51c8c46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(readm_demb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(readm_demb_loader.dataset)}\n",
    "readm_demb_loader, readm_demb_metadata = (\n",
    "    cacher.DataloaderFromCache(readmission_pred_task_demb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(readm_demb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bebc5-23b3-4a1f-966c-a3921d5de853",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    del bert_xform\n",
    "    del mort_demb_loader\n",
    "    del mortality_demb_dataset\n",
    "    del readm_demb_loader\n",
    "    del readmission_demb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e770aa-7bc1-4a16-a936-bef845b4a23e",
   "metadata": {},
   "source": [
    "### Bert Fine Tune Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd4958c-5ea2-463c-93e6-2844428351d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertFineTuneTransform(object):\n",
    "    \"\"\"Transform a sample's (a single visit's) text into 1 embedding vector.\n",
    "    \n",
    "    The embeddings of each text field are combined by embedding\n",
    "    each separately then summing.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "        self.tokenzier = AutoTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "        print(f'Tokenizer: {self.tokenizer}')\n",
    "        # max length of sentence not real emb size.\n",
    "        self.emb_size = 20  # 15 worked here.\n",
    "    \n",
    "    def _get_embeddings_of_sentences_with_mask(self, field, pad) -> torch.tensor:\n",
    "        return self._get_embeddings_of_sentences(field[:pad])\n",
    "        \n",
    "    def _get_embeddings_of_sentences(self, sentences: List[str]) -> torch.tensor:\n",
    "        batch_enc = self.tokenizer.batch_encode_plus(sentences, padding=True,\n",
    "                                return_attention_mask=True, return_length=True,\n",
    "                                truncation='longest_first', max_length=self.emb_size)\n",
    "        batch_enc_tensor = batch_enc.convert_to_tensors(tensor_type=TensorType.PYTORCH)\n",
    "        # decoded = self.tokenizer.decode(batch_enc['input_ids'][0])\n",
    "        # print(f'decoded {decoded}')\n",
    "        # assert(decoded[0:5] == '[CLS]')\n",
    "        return batch_enc_tensor\n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        len_embeddings = (len(sample['conditions_text']) +\n",
    "                          len(sample['procedures_text']) +\n",
    "                          len(sample['drugs_text']))\n",
    "        sample_text = []\n",
    "        sample_masks = []\n",
    "        if sample.get('conditions_text_pad'):\n",
    "            pad = sample['conditions_text_pad']\n",
    "            sample_text.extend(sample['conditions_text'][:pad])\n",
    "        if sample.get('procedures_text_pad'):\n",
    "            pad = sample['procedures_text_pad']\n",
    "            sample_text.extend(sample['procedures_text'][:pad])\n",
    "        if sample.get('drugs_text_pad'):\n",
    "            pad = sample['drugs_text_pad']\n",
    "            sample_text.extend(sample['drugs_text'][:pad])\n",
    "            \n",
    "        batch_enc_tensor = self._get_embeddings_of_sentences(sample_text)\n",
    "        assert(batch_enc_tensor['input_ids'].device == torch.device('cpu'))\n",
    "        assert(batch_enc_tensor['token_type_ids'].device == torch.device('cpu'))\n",
    "        # print(f'FT transform shape\\n'\n",
    "        #       f\"iids {batch_enc_tensor['input_ids'].shape}\\n\"\n",
    "        #       f\"ttids {batch_enc_tensor['token_type_ids'].shape}\")\n",
    "        # Return the ((model_inputs), label) \n",
    "        d = {\n",
    "            'input_ids': batch_enc_tensor['input_ids'],\n",
    "            'attention_masks': batch_enc_tensor['attention_mask'],\n",
    "            'token_type_ids': batch_enc_tensor['token_type_ids'],\n",
    "        }\n",
    "        return (d, sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64574cb-7628-4451-a7ec-af3405760e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_fine_tune_collate(data):\n",
    "    \"\"\"\n",
    "    Collates a tensor of (batch_size, seq_len, bert_emb_len) i.e. (32, <events per patient>, 768)\n",
    "    Need to pad the second dimension to max(<variable_per_patient>).\n",
    "    \n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: list of ({'inputs_ids':tensor, 'attention_mask':tensor, 'token_type_ids':tensor}, y)\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (batch_size, max #conditions, max embedding size) of type torch.float\n",
    "        masks: a tensor of shape (batch_size, max #conditions, max embedding_size) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (batch_size) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    kMaxEvents = 25\n",
    "    kMaxWordLength = 20\n",
    "    sequences, labels = zip(*data)\n",
    "    \n",
    "    # Convert output labels for each sample in batch to tensor.\n",
    "    # shape: (batch_size, 1)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "   \n",
    "    num_patients = len(sequences)\n",
    "    num_events = [patient['input_ids'].shape[0] for patient in sequences]\n",
    "    embedding_length = [patient['input_ids'].shape[1] for patient in sequences]\n",
    "    # if len(sequences) > 1:\n",
    "    #     assert(sequences[0]['input_ids'].shape[1] == sequences[1]['input_ids'].shape[1])\n",
    "\n",
    "    max_num_events = min(kMaxEvents, max(num_events))\n",
    "    max_embedding_length = min(kMaxWordLength, max(embedding_length))\n",
    "    \n",
    "    input_ids = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.long)\n",
    "    rev_input_ids = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.long)\n",
    "    attention_masks = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.long)\n",
    "    rev_attention_masks = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.long)\n",
    "    token_type_ids = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.long)\n",
    "    rev_token_type_ids = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        # Patient 3 tensors (#events, sentence_length+pad)\n",
    "        j_visits = min(patient['input_ids'].shape[0], max_num_events)\n",
    "        w_len = min(patient['input_ids'].shape[1], max_embedding_length)\n",
    "        iids = patient['input_ids']\n",
    "        ams = patient['attention_masks']\n",
    "        ttids = patient['token_type_ids']\n",
    "        assert(iids.shape == ams.shape)\n",
    "        assert(iids.shape == ttids.shape)\n",
    "        \"\"\"\n",
    "        TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "        \"\"\"\n",
    "        input_ids[i_patient, :j_visits, :w_len] = iids[:j_visits, :w_len].unsqueeze(0)\n",
    "        attention_masks[i_patient, :j_visits, :w_len] = ams[:j_visits, :w_len].unsqueeze(0)\n",
    "        token_type_ids[i_patient, :j_visits, :w_len] = ttids[:j_visits, :w_len].unsqueeze(0)\n",
    "        # The tensor is (seq_length, sentence_length+pad). Leave embeddings,\n",
    "        # flip temporal order of code/event sequence.\n",
    "        rev_input_ids[i_patient, :j_visits, :iids.shape[-1]] = torch.flip(iids, dims=[0]).unsqueeze(0)[:,:j_visits, :w_len]\n",
    "        rev_attention_masks[i_patient, :j_visits, :ams.shape[-1]] = torch.flip(ams, dims=[0]).unsqueeze(0)[:,:j_visits, :w_len]\n",
    "        rev_token_type_ids[i_patient, :j_visits, :ttids.shape[-1]] = torch.flip(ttids, dims=[0]).unsqueeze(0)[:,:j_visits, :w_len]\n",
    "        masks[i_patient, :j_visits] = 1\n",
    "        rev_masks[i_patient, :j_visits] = 1\n",
    "      \n",
    "        # TODO(botelho3) - comment this out to reduce spew.\n",
    "        # if i_patient == 0:\n",
    "        #     print(f\"------ p: {i_patient} ------\")\n",
    "        #     print(x[i_patient, :, :25])\n",
    "        #     print(rev_x[i_patient, :, :25])\n",
    "        #     print(masks[i_patient, :, :25])\n",
    "        #     print(rev_masks[i_patient, :, :25])\n",
    "        \n",
    "    # print(f'Bert Fine Tune Collate\\n'\n",
    "    #       f'seq {sequences[0][\"input_ids\"].shape}, '\n",
    "    #       f'seq {sequences[-1][\"input_ids\"].shape}\\n'\n",
    "    #       f'input_ids {input_ids.shape}, '\n",
    "    #       f'rev_attention_masks {rev_attention_masks.shape}')\n",
    "    x = {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_masks,\n",
    "        'token_type_ids': token_type_ids,\n",
    "    }\n",
    "    rev_x = {\n",
    "        'input_ids': rev_input_ids,\n",
    "        'attention_mask': rev_attention_masks,\n",
    "        'token_type_ids': rev_token_type_ids,\n",
    "    }\n",
    "    return {\n",
    "        'x': x,\n",
    "        'rev_x': rev_x,\n",
    "        'masks': masks,\n",
    "        'rev_masks': rev_masks,\n",
    "        'y': y,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b392f-3f9d-4ca2-b75a-11d5b2f0e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_ft_xform = BertFineTuneTransform()\n",
    "BERT_FT_EMBEDDING_SIZE = bert_ft_xform.emb_size\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "MORT_DEMBFT_CODE2IDX = {}\n",
    "MORT_DEMBFT_CODE_COUNT = {}\n",
    "task_fn = functools.partial(mortality_pred_task_demb, MORT_DEMBFT_CODE_COUNT)\n",
    "mortality_dembft_ds = mimic3base.set_task(task_fn, task_name=mortality_pred_task_demb.__name__)\n",
    "MORT_DEMBFT_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORT_DEMBFT_CODE_COUNT.keys()))\n",
    "}\n",
    "\n",
    "\n",
    "indices = list(range(0, min(10000, len(mortality_dembft_ds)) ))\n",
    "print(len(mortality_dembft_ds))\n",
    "print(len(torch.utils.data.Subset(mortality_dembft_ds, indices)))\n",
    "\n",
    "print(f\"MORT_DEMB_CODE2IDX {len(MORT_DEMBFT_CODE2IDX)}\")\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "tmp = TextEmbedDataset(mortality_dembft_ds, transform=bert_ft_xform, should_cache=False)\n",
    "mortality_dembft_dataset = torch.utils.data.Subset(tmp, indices)\n",
    "print(len(mortality_dembft_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e216981-5973-4cd4-bf6e-39c1266c2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Verify DataLoader properties.\n",
    "    # Quick test without running the whole RNN training process.\n",
    "    mort_dembft_loader = DataLoader(\n",
    "        mortality_dembft_dataset,\n",
    "        batch_size=BATCH_SIZE_,\n",
    "        shuffle=SHUFFLE_,\n",
    "        collate_fn=bert_fine_tune_collate,\n",
    "    )\n",
    "    # loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "    loader_iter = iter(mort_dembft_loader)\n",
    "    # for _ in loader_iter:\n",
    "    #     pass\n",
    "    try:\n",
    "        x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "    except StopIteration as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26b868-8bb4-4c84-9457-b6e6244c269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free some memory\n",
    "del bert_ft_xform\n",
    "del mortality_dembft_ds\n",
    "del mortality_dembft_dataset\n",
    "gc.collect()\n",
    "del bert_xform\n",
    "del mortality_demb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc740d9c-bb55-4e54-b877-f1354ea793fc",
   "metadata": {},
   "source": [
    "# Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    # print(f'y_pred: {torch.sum(y_pred)}, y_true {torch.sum(y_true)}\\n'\n",
    "    #       f'y_score\\n{y_score}\\n'\n",
    "    #       f'y_pred\\n{y_pred}\\n'\n",
    "    #       f'y_true\\n{y_true}\\n')\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    rcurve = roc_curve(y_true, y_score)\n",
    "    precision_curve, recall_curve, thresholds = precision_recall_curve(y_true, y_score)\n",
    "    \n",
    "    \n",
    "\n",
    "    # your code here\n",
    "    return p, r, f, roc_auc, rcurve, precision_curve, recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, masks, rev_x, rev_masks)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # your code here\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc, rcurve, _, _ = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e77bd4-a894-45ea-b621-e6da4d0029ee",
   "metadata": {},
   "source": [
    "### Model: CodeEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04566514-f29b-4683-802f-f370867a907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model here\n",
    "mort_code_naive_rnn = CembRNN(num_codes = len(MORT_CEMB_CODE2IDX))\n",
    "mort_code_naive_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938907a3-04c8-40f7-a772-f66d923e0832",
   "metadata": {},
   "source": [
    "### Model: DescEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model here\n",
    "mort_desc_naive_rnn = DembRNN()\n",
    "mort_desc_naive_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeef88e-bc4b-454e-adb5-d65eee2169bf",
   "metadata": {},
   "source": [
    "### Train: CodeEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ded72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training CodeEmb.')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mort_code_naive_rnn.parameters(), lr=0.001)\n",
    "# number of epochs to train the model\n",
    "n_epochs =10 \n",
    "train(mort_code_naive_rnn, mort_cemb_train_loader, mort_cemb_val_loader,\n",
    "      n_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fde9e-4bcc-4914-b6f4-6f9a8e8b8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve, prec_curve, rec_curve = eval_model(mort_code_naive_rnn, mort_cemb_val_loader)\n",
    "print('Validation Set p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "      .format(p, r, f, roc_auc))\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec_curve, prec_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769dec9-05ef-42cd-be38-076c25d5e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve,  prec_curve, rec_curve = eval_model(mort_code_naive_rnn, mort_cemb_test_loader)\n",
    "print('Test Set p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "      .format(p, r, f, roc_auc))\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec_curve, prec_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b02354-bdfd-4064-8ffe-7d0dc807c8a1",
   "metadata": {},
   "source": [
    "### Train: DescEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d00785-0639-4365-b5dc-b00e4cc3ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mort_desc_naive_rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11370bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "train(mort_desc_naive_rnn, mort_demb_train_loader, mort_demb_val_loader,\n",
    "      n_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce9a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve,  prec_curve, rec_curve = eval_model(mort_desc_naive_rnn, mort_demb_val_loader)\n",
    "print('Validation Set p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "      .format(p, r, f, roc_auc))\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec_curve, prec_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341a4ae-e258-43ad-a0b1-ff9af60b18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve,  prec_curve, rec_curve = eval_model(mort_desc_naive_rnn, mort_demb_test_loader)\n",
    "print('Test Set p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "      .format(p, r, f, roc_auc))\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec_curve, prec_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d93b7f-35a9-4b64-997e-4aabf5f5db96",
   "metadata": {},
   "source": [
    "# Condensed Training using Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad06aa7-4577-4d78-a9e6-dd54ebca2064",
   "metadata": {},
   "source": [
    "### Plot Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5383d9-ec6d-404c-bafe-9b2c7b9c3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_types = ['code_emb', 'desc_emb', 'desc_emb_ft']\n",
    "predict_model_types = ['code_emb', 'desc_emb', 'desc_emb_ft']\n",
    "task_types = ['mort', 'readm']\n",
    "\n",
    "def PlotAucRecallResults(val_results, test_results, emb_type: str, task_type: str):\n",
    "    ''' Plot AUC-ROC curve and P-R curve.\n",
    "    \n",
    "        val_results: validation set\n",
    "        test_results: test set\n",
    "    '''\n",
    "    p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = val_results\n",
    "    plt.plot(rcurve[0], rcurve[1])\n",
    "    plt.title(' '.join(['ROC Curve / AUC', emb_type, 'for task:', task_type]))\n",
    "    plt.xlabel('FPR')\n",
    "    plt.ylabel('TPR')\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(recall_curve, precision_curve)\n",
    "    plt.title('PR Curve')\n",
    "    plt.title(' '.join(['PR Curve', emb_type, task_type]))\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.show() \n",
    "#     p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = test_results\n",
    "#     plt.plot(rcurve[0], rcurve[1])\n",
    "#     plt.title(' '.join(['ROC Curve / AUC', emb_type, 'for task:', task_type]))\n",
    "#     plt.xlabel('FPR')\n",
    "#     plt.ylabel('TPR')\n",
    "#     plt.show()\n",
    "\n",
    "#     plt.plot(recall_curve, precision_curve)\n",
    "#     plt.title(' '.join(['PR Curve', emb_type, task_type]))\n",
    "#     plt.xlabel('Recall')\n",
    "#     plt.ylabel('Precision')\n",
    "#     plt.show() \n",
    "\n",
    "    \n",
    "def PlotDiffResults(results_a, results_b, task_type: str, labels: List[str]):\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    for i in range(len(results_set_labels)):\n",
    "        # ROC Curves for val, test.\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = results_a[i][-1]\n",
    "        plt.plot(rcurve[0], rcurve[1], label=labels[0] + '_' + results_set_labels[i])\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = results_b[i][-1]\n",
    "        plt.plot(rcurve[0], rcurve[1], label=labels[1] + '_' + results_set_labels[i])\n",
    "        plt.title(' '.join(['ROC Curve / AUC', 'for task:', task_type]))\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # PR Curves for val, test.\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = results_a[i][-1]\n",
    "        plt.plot(recall_curve, precision_curve, label=labels[0] + '_' + results_set_labels[i])\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = results_b[i][-1]\n",
    "        plt.plot(recall_curve, precision_curve, label=labels[1] + '_' + results_set_labels[i])\n",
    "        plt.title(' '.join(['PR Curve', 'for task:', task_type]))\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "\n",
    "        \n",
    "def PlotPrecisionRecallAcrossEpoch(val_results, test_results):\n",
    "    num_epochs = len(val_results)\n",
    "    epochs = range(num_epochs)\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    \n",
    "    recall_val = []\n",
    "    precision_val = []\n",
    "    recall_test = []\n",
    "    precision_test = []\n",
    "    for i in epochs:\n",
    "        recall_val.append(val_results[i][1])\n",
    "        precision_val.append(val_results[i][0])\n",
    "        recall_test.append(test_results[i][1])\n",
    "        precision_test.append(test_results[i][0])\n",
    "   \n",
    "    plt.plot(epochs, recall_val, label='_'.join(['recall', results_set_labels[0]]))\n",
    "    plt.plot(epochs, recall_test, label='_'.join(['recall', results_set_labels[1]]))\n",
    "    plt.plot(epochs, precision_val, label='_'.join(['precision', results_set_labels[0]]))\n",
    "    plt.plot(epochs, precision_test, label='_'.join(['precision', results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Recall/Precision vs epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall/Precision')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def PlotPrecisionRecallDiffAcrossEpoch(results_a, results_b):\n",
    "    assert(len(results_a) == len(results_b))\n",
    "    num_epochs = len(results_a)\n",
    "    epochs = range(num_epochs)\n",
    "    labels = ['a', 'b']\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    \n",
    "    recall_val_a = []\n",
    "    precision_val_a = []\n",
    "    recall_test_a = []\n",
    "    precision_test_a = []\n",
    "    recall_val_b = []\n",
    "    precision_val_b = []\n",
    "    recall_test_b = []\n",
    "    precision_test_b = []\n",
    "    for i in epochs:\n",
    "        recall_val_a.append(results_a[i][1])\n",
    "        precision_val_a.append(results_a[i][0])\n",
    "        recall_test_a.append(results_a[i][1])\n",
    "        precision_test_a.append(results_a[i][0])\n",
    "        recall_val_b.append(results_b[i][1])\n",
    "        precision_val_b.append(results_b[i][0])\n",
    "        recall_test_b.append(results_b[i][1])\n",
    "        precision_test_b.append(results_b[i][0])\n",
    "   \n",
    "    plt.plot(epochs, recall_val_a, label='_'.join(['recall', labels[0], results_set_labels[0]]))\n",
    "    plt.plot(epochs, recall_test_a, label='_'.join(['recall', labels[0], results_set_labels[1]]))\n",
    "    plt.plot(epochs, recall_val_b, label='_'.join(['recall', labels[1], results_set_labels[0]]))\n",
    "    plt.plot(epochs, recall_test_b, label='_'.join(['recall', labels[1], results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Recall vs Epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot(epochs, precision_val_a, label='_'.join(['precision', labels[0], results_set_labels[0]]))\n",
    "    plt.plot(epochs, precision_test_a, label='_'.join(['precision', labels[0], results_set_labels[1]]))\n",
    "    plt.plot(epochs, precision_val_b, label='_'.join(['precision', labels[1], results_set_labels[0]]))\n",
    "    plt.plot(epochs, precision_test_b, label='_'.join(['precision', labels[1], results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Precision vs Epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def PlotAccuracyAcrossEpoch(results_a, results_b=None):\n",
    "    if results_b:\n",
    "        assert(len(results_a[0]) == len(results_b[0]))\n",
    "    num_epochs = len(results_a[0])\n",
    "    epochs = list(range(num_epochs))\n",
    "    task_labels = ['a', 'b']\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    \n",
    "    accuracy_a_val = []\n",
    "    accuracy_a_test = []\n",
    "    accuracy_b_val = []\n",
    "    accuracy_b_test = []\n",
    "    for i in epochs:\n",
    "        if results_a is not None:\n",
    "            accuracy_a_val.append(results_a[0][i][7])\n",
    "            accuracy_a_test.append(results_a[1][i][7])\n",
    "        if results_b is not None:\n",
    "            accuracy_b_val.append(results_b[0][i][7])\n",
    "            accuracy_b_test.append(results_b[1][i][7])\n",
    "            \n",
    "    plt.plot(epochs, accuracy_a_val, label='_'.join(['recall', task_labels[0], results_set_labels[0]]))\n",
    "    plt.plot(epochs, accuracy_a_test, label='_'.join(['recall', task_labels[0], results_set_labels[1]]))\n",
    "    if results_b is not None:\n",
    "        plt.plot(epochs, accuracy_b_val, label='_'.join(['accuracy', task_labels[1], results_set_labels[0]]))\n",
    "        plt.plot(epochs, accuracy_b_test, label='_'.join(['accuracy', task_labels[1], results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Accuracy vs Epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def PrintTrainTime(times):\n",
    "    print('Total train time: {:.2} s'.format(sum(times)))\n",
    "    print('Per epoch times: ' + ', '.join(['{:.2}'.format(t) for t in times]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b96518-be15-4323-ad4e-25ccb33aaa59",
   "metadata": {},
   "source": [
    "### CodeEmb - Mort - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f908a57-e47d-461a-872b-8ffac66aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "\n",
    "embed_model_types = ['code_emb', 'desc_emb', 'desc_emb_ft']\n",
    "predict_model_types = ['code_emb', 'desc_emb', 'desc_emb_ft']\n",
    "task_types = ['mort', 'readm']\n",
    "# Dev set.\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "# READM_DEMB_CODE2IDX 1523\n",
    "# READM_CEMB_CODE2IDX 1456\n",
    "# Full set.\n",
    "# READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 6555\n",
    "# MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 6732\n",
    "# Full set.\n",
    "# MORT_DEMB_CODE2IDX 6732\n",
    "# MORT_CEMB_CODE2IDX 6546\n",
    "# READM_CEMB_CODE2IDX 6546\n",
    "# READM_DEMB_CODE2IDX 6555\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'code_emb'\n",
    "args.predict_model_type = 'code_emb'\n",
    "args.collate_fn=None\n",
    "args.embed_index_size = 1456 # mort_cemb_metadata['extra_data']['embed_index_size']\n",
    "args.pred_embed_size = 128\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ab114-394f-49e2-84b5-acf100b31989",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "cemb_mort_dev_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fb1a0-de96-4f31-8449-d09aa1c37183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = cemb_mort_dev_results[0]\n",
    "test_set_results = cemb_mort_dev_results[1]\n",
    "PrintTrainTime(cemb_mort_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Cemb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(cemb_mort_dev_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c0384-6918-4d12-a5fd-bec8b52770aa",
   "metadata": {},
   "source": [
    "### DescEmb - Mort - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28731f43-6486-4e30-b17a-b78d73375040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b473b08-adc9-4ce0-a167-eab1d7551286",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "demb_mort_dev_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4598c1-6fc6-4099-9386-87ee8f9f3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in demb_mort_results.\n",
    "val_set_results = demb_mort_dev_results[0]\n",
    "test_set_results = demb_mort_dev_results[1]\n",
    "PrintTrainTime(demb_mort_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(demb_mort_dev_results)\n",
    "labels = ['cemb', 'demb']\n",
    "PlotDiffResults(cemb_mort_dev_results, demb_mort_dev_results, task_type='Mortality', labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8992a6-baaa-4dfc-8576-1d9c3141f15b",
   "metadata": {},
   "source": [
    "### CodeEmb - Readm - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296f7f4-b820-4e27-ad86-7f8cc46747dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# Dev set.\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "# READM_DEMB_CODE2IDX 1523\n",
    "# READM_CEMB_CODE2IDX 1456\n",
    "# Full set.\n",
    "# READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 6555\n",
    "# MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 6732\n",
    "# Full set.\n",
    "# MORT_DEMB_CODE2IDX 6732\n",
    "# MORT_CEMB_CODE2IDX 6546\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'readm'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'code_emb'\n",
    "args.predict_model_type = 'code_emb'\n",
    "args.collate_fn=None\n",
    "args.embed_index_size = 1456\n",
    "args.pred_embed_size = 128\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f098d-0288-4e20-a2b9-b2e8b5ca5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "cemb_readm_dev_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577cfd45-41ed-4321-aa5f-1aec2c7d2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = cemb_readm_dev_results[0]\n",
    "test_set_results = cemb_readm_dev_results[1]\n",
    "PrintTrainTime(cemb_readm_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Cemb', task_type='Readmission')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(cemb_readm_dev_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf2af0-598d-458c-9ca1-1d4189c56625",
   "metadata": {},
   "source": [
    "### DescEmb - Readm - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65f8b9-5632-499c-9e42-bba7b4ac396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'readm'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb54cb5-9f45-422a-a23c-1227ab31ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "demb_readm_dev_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fed3d-9d78-45a1-8a82-5ebd408c795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in demb_mort_results.\n",
    "val_set_results = demb_readm_dev_results[0]\n",
    "test_set_results = demb_readm_dev_results[1]\n",
    "PrintTrainTime(demb_readm_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(demb_readm_dev_results)\n",
    "labels = ['cemb', 'demb']\n",
    "PlotDiffResults(cemb_readm_dev_results, demb_readm_dev_results, task_type='Mortality', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ebac5-2752-4a6c-818a-b058930e20f6",
   "metadata": {},
   "source": [
    "### CodeEmb - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8496646-b25b-4170-b0a2-3a1cbb5a9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# Dev set.\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "# Full set.\n",
    "# READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 6555\n",
    "# MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 6732\n",
    "# Full set.\n",
    "# MORT_DEMB_CODE2IDX 6732\n",
    "# MORT_CEMB_CODE2IDX 6546\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'code_emb'\n",
    "args.predict_model_type = 'code_emb'\n",
    "args.collate_fn=None\n",
    "args.embed_index_size = 6546  # mort_cemb_metadata['extra_data']['embed_index_size']\n",
    "args.pred_embed_size = 128\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953a1a7-271d-4c5e-93eb-29188b57c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "cemb_mort_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0425611-1882-44d3-839c-f2c894265b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = cemb_mort_full_results[0]\n",
    "test_set_results = cemb_mort_full_results[1]\n",
    "PrintTrainTime(cemb_mort_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Cemb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(cemb_mort_full_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024acbfb-0d8d-49a3-b1ac-11b9f3dad369",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DescEmb - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985f3fd-3e87-4b31-9ff0-438184e8bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer \n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c629cc3-4780-4eef-a610-69d90c928e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "demb_mort_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b3e43-a636-4ca4-9f30-383cfff67cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = demb_mort_full_results[0]\n",
    "test_set_results = demb_mort_full_results[1]\n",
    "PrintTrainTime(demb_mort_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(demb_mort_full_results)\n",
    "labels = ['cemb', 'demb']\n",
    "PlotDiffResults(cemb_mort_full_results, demb_mort_full_results, task_type='Mortality', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ce321-3240-467b-b443-f636e4197c33",
   "metadata": {},
   "source": [
    "### CodeEmb - Readm - Full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e55d77-5075-42b1-9905-b637a17cbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# Dev set.\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "# READM_DEMB_CODE2IDX 1523\n",
    "# READM_CEMB_CODE2IDX 1456\n",
    "# Full set.\n",
    "# READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 6555\n",
    "# MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 6732\n",
    "# Full set.\n",
    "# MORT_DEMB_CODE2IDX 6732\n",
    "# MORT_CEMB_CODE2IDX 6546\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'readm'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'code_emb'\n",
    "args.predict_model_type = 'code_emb'\n",
    "args.collate_fn=None\n",
    "args.embed_index_size = 6546\n",
    "args.pred_embed_size = 128\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c65aec-c2f7-4e08-b648-0f37f94222f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33426 / 32 * 0.8 = 837\n",
    "trainer = Trainer(args)\n",
    "cemb_readm_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15a307-386a-440d-939f-b5daaf8e22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = cemb_readm_full_results[0]\n",
    "test_set_results = cemb_readm_full_results[1]\n",
    "PrintTrainTime(cemb_readm_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Cemb', task_type='Readmission')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(cemb_readm_full_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc6ac8-ef50-4cae-9789-bd909d109131",
   "metadata": {},
   "source": [
    "### DescEmb - Readm - Full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a660b0-a8f7-47a2-840d-84247b3434d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'readm'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b411d5-9058-47e6-bb38-a1aa6e9738fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 / 32 * 0.8 = 250\n",
    "trainer = Trainer(args)\n",
    "demb_readm_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2829d06-4e9a-4db0-881c-d71464ded609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in demb_mort_results.\n",
    "val_set_results = demb_readm_full_results[0]\n",
    "test_set_results = demb_readm_full_results[1]\n",
    "PrintTrainTime(demb_readm_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Readmission')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(demb_readm_full_results)\n",
    "labels = ['cemb', 'demb']\n",
    "PlotDiffResults(cemb_readm_full_results, demb_readm_full_results, task_type='Readmission', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719519d-6bde-473b-a93b-d148bec8f6ba",
   "metadata": {},
   "source": [
    "### DescEmbFt - Mort - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5a4d3-b8db-4a37-9080-f0332e01c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "import torch\n",
    "# from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb_ft'\n",
    "args.predict_model_type = 'desc_emb_ft'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_fine_tune_collate\n",
    "args.no_use_cached_dataset = mortality_dembft_dataset\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95d438-55f3-41fb-b04f-17979585ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "gc.collect()\n",
    "del trainer\n",
    "gc.collect()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a20ebb-4167-4791-b20a-a490b69a1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "dembft_mort_dev_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54676cb-eef9-4b5c-bd3b-9dc2d4b9b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in demb_mort_results.\n",
    "val_set_results = dembft_mort_dev_results[0]\n",
    "test_set_results = dembft_mort_dev_results[1]\n",
    "PrintTrainTime(dembft_mort_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(dembft_mort_dev_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6764e42-107f-420d-9c3e-aee19848c6cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DescEmbFt - Mort - Full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cc0fa-105e-4db1-be77-2e125301e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb_ft'\n",
    "args.predict_model_type = 'desc_emb_ft'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_fine_tune_collate\n",
    "args.no_use_cached_dataset = mortality_dembft_dataset\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70346c81-9b48-44b6-ab24-fe5a3fb024de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "gc.collect()\n",
    "del trainer\n",
    "gc.collect()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d14dc-da6c-4d4b-ae8d-8e09ce5fd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "dembft_mort_full_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f953d4a-f70d-4497-a9f1-33305b088d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = dembft_mort_full_results[0]\n",
    "test_set_results = dembft_mort_full_results[1]\n",
    "PrintTrainTime(dembft_mort_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(dembft_mort_full_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf69d78-6151-4eb8-9788-ea3ed5b3797d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec866fcf-1b3c-487f-ae70-67be9fc636b2",
   "metadata": {},
   "source": [
    "# eICU Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221daacb-8e59-4849-912a-dff5550f479f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Reading data from: `{EICU_DATA_DIR_}`')\n",
    "\n",
    "eicubase = eICUDataset(\n",
    "    # root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
    "    root=EICU_DATA_DIR_,\n",
    "    dataset_name='eicu_dataset',\n",
    "    tables=[\"diagnosis\", \"treatment\", \"medication\"],\n",
    "    # map all NDC codes to ATC 3-rd level codes in these tables\n",
    "    # See https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System.\n",
    "    code_mapping={\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})},\n",
    "    # Reads a subset of the data. Disable for full training run.\n",
    "    dev = False,\n",
    "    # True = Slow, rebuilds the dataset instead of caching.\n",
    "    refresh_cache=False,\n",
    ")\n",
    "\n",
    "eicubase.stat()\n",
    "eicubase.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752e1632-795a-4fac-8a76-9489306d741f",
   "metadata": {},
   "source": [
    "#### CodeEMB Pred Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668288fe-2951-4216-8c98-055922c83f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eicu_mortality_pred_task_cemb(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    each sample is a list of vists for 1 patient.\n",
    "    \"\"\"\n",
    "   \n",
    "    # TODO(botelho3) - stupid hack around the limitations of SampleDataset validator.\n",
    "    kMaxVisits = 5\n",
    "    if len(patient) < 1 or len(patient) > kMaxVisits:\n",
    "        return []\n",
    "        \n",
    "    samples = [{\n",
    "        \"visit_id\": patient[0].visit_id,\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"num_visits\": 0,\n",
    "        \"conditions\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures\": [[] for v in range(kMaxVisits)],\n",
    "        \"conditions_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs_text\": [[] for v in range(kMaxVisits)],\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": 0,\n",
    "    }]\n",
    "    \n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "\n",
    "    # loop over all visits\n",
    "    out_idx = 0\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"diagnosis\")\n",
    "        procedures = visit.get_code_list(table=\"treatment\")\n",
    "        drugs = visit.get_code_list(table=\"medication\")\n",
    "        drugs_full = visit.get_event_list(table=\"medication\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        # diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        # proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        print(f'condition {conditions}')\n",
    "        print(f'proc {procedures}')\n",
    "        print(f'drugs {drugs_full}')\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], str(d['dose']), d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        # Each field is a list of visits.\n",
    "        samples[0]['num_visits'] = samples[0]['num_visits'] + 1\n",
    "        samples[0]['conditions'][out_idx]=conditions\n",
    "        samples[0]['procedures'][out_idx]=procedures\n",
    "        samples[0]['conditions_text'][out_idx]= conditions_text\n",
    "        samples[0]['procedures_text'][out_idx] = procedures_text\n",
    "        samples[0]['drugs'][out_idx] = drugs\n",
    "        samples[0]['drugs_text'][out_idx] = drugs_text\n",
    "        out_idx = out_idx + 1\n",
    "    samples[0]['label'] = global_mortality_label\n",
    "   \n",
    "    # Record all unique codes and their frequency for LUT.\n",
    "    for visit in samples[0]['conditions']:\n",
    "        for code in visit:\n",
    "            CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "           \n",
    "    # If none of the samples met the criteria return an empty list.\n",
    "    if samples[0]['num_visits'] == 0:\n",
    "        return []\n",
    "        \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28ad30-bb14-4112-aacb-8f8ea6859987",
   "metadata": {},
   "source": [
    "#### DescEMB Pred Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a83b4f-226d-4d14-be66-f2d67eb58121",
   "metadata": {},
   "outputs": [],
   "source": [
    "re_drug_prefix = re.compile('Event with eICU_DRUGNAME code{.*}from table medication')\n",
    "re_bar = re.compile('\\|')\n",
    "ICD_9_LUT_ = {}\n",
    "ICD_10_LUT_ = {}\n",
    "fname = os.path.expanduser('~/sw/icd10cm-code descriptions- April 1 2023/icd10cm-codes- April 1 2023.txt')\n",
    "with open(fname, 'r') as f:\n",
    "    for l in f:\n",
    "        code, desc = l.split(sep=' ', maxsplit=1)\n",
    "        ICD_10_LUT_[code] = desc\n",
    "        \n",
    "def eicu_mortality_pred_task_demb(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "    \n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        mortality_label = 0 if visit.discharge_status == 'Alive' else 1\n",
    "        global_mortality_label |= mortality_label\n",
    "        \n",
    "    # loop over all visits but the last one\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit: Visit.\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"diagnosis\")\n",
    "        procedures = visit.get_code_list(table=\"treatment\")\n",
    "        # drugs = [x.code for x in visit.get_event_list(table=\"medication\")]\n",
    "        drugs_full = visit.get_event_list(table=\"medication\")\n",
    "        drugs_full = [d.code for d in drugs_full]\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            # print(f'Excluded something 0 {len(conditions)}, {len(procedures)}, {len(drugs_full)}')\n",
    "            # print(f'conditions {conditions}')\n",
    "            # print(f'procedures {procedures}')\n",
    "            # print(f'drugs_full {drugs_full}')\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        # diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        # proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        \n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        # print(f'condition {conditions}')\n",
    "        # print(f'proc {procedures}')\n",
    "        # print(f'drugs {drugs_full}')\n",
    "        # conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        # procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        conditions = filter(lambda x: True if x[0].isalpha() else False, conditions)\n",
    "        conditions = [cond.replace('.', '') for cond in conditions]\n",
    "        conditions_text = [ICD_10_LUT_.get(cond, '') for cond in conditions] \n",
    "        procedures_text = [re_bar.sub(' ', proc) for proc in procedures]\n",
    "        drugs_text = [re_drug_prefix.sub('\\1', str(d)) for d in drugs_full]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples into a pyHealth Visit.\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_mortality_label,\n",
    "            }\n",
    "        )\n",
    "   \n",
    "    \n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return [] \n",
    "    \n",
    "   \n",
    "    # pyHealth requires that all list fields in sample are equal size.\n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data))\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size\n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_mortality_label,\n",
    "    }\n",
    "   \n",
    "    # For every condition in the sample (all visits). Record frequency.\n",
    "    # Will be used to build code->index LUT.\n",
    "    for code in sample['conditions']:\n",
    "        CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "        \n",
    "    samples.append(sample)\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c228b82-a360-466f-bfb9-a19019ec5ae6",
   "metadata": {},
   "source": [
    "#### Test Load Mortality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773721d7-5b98-452a-9721-e752cf0f2619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "EICU_MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "EICU_MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "task_fn = functools.partial(eicu_mortality_pred_task_demb, EICU_MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_)\n",
    "mor_dataset = eicubase.set_task(task_fn, task_name=eicu_mortality_pred_task_demb.__name__)\n",
    "EICU_MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(EICU_MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "mor_dataset.stat()\n",
    "mor_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "# print(f\"MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(EICU_MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "#       f\"{EICU_MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_}\")\n",
    "del mor_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f3b6ef-c674-4c46-993b-61de79ece108",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1f71d-5fdf-43d2-a4ea-e1997e417298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model_state_from_file(fname):\n",
    "    path = os.path.join(os.getcwd(), 'modelcache', fname)\n",
    "        #     torch.save(\n",
    "        #     {\n",
    "        #         # 'model_state_dict': self.model.module.state_dict() if (\n",
    "        #         #     isinstance(self.model, DataParallel)\n",
    "        #         # ) else self.model.state_dict(),\n",
    "        #         'model_state_dict': self.model.state_dict(),\n",
    "        #         'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "        #         'epochs': epoch,\n",
    "        #         'args': self.args,\n",
    "        #     },\n",
    "        #     path\n",
    "        # )\n",
    "    d = torch.load(path)\n",
    "    model_state_dict = d['model_state_dict']\n",
    "    args = d['args']\n",
    "    print(f'Loading model state with args{args} from \\n {path}')\n",
    "    return model_state_dict, args\n",
    "\n",
    "def load_model_from_file(embed_model_type: str,\n",
    "                         task: str,\n",
    "                         is_dev: bool):\n",
    "    fname = f'model_{embed_model_type}_task_{task}_isdev{is_dev}'\n",
    "    fname = fname + \"_best.pt\"\n",
    "    model_state_dict, args = load_model_state_from_file(fname)\n",
    "    model = EHRModel(args)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    return model, args\n",
    "   \n",
    "model = load_model_from_file('desc_emb_ft', 'mort', is_dev=False) \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcb48a6-4236-4eeb-8a11-c51ed2372cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7905508d-94e4-4719-833d-e5b7f968adcd",
   "metadata": {},
   "source": [
    "#### Load DescEmb - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01292417-5a9f-428d-bd89-8ad5203ede31",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74077ba6-84c2-410f-be32-89e3ed589553",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "613465d1-bb3f-40f1-ba47-6024e0afae74",
   "metadata": {},
   "source": [
    "#### Load DescEmbFt - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe91252-6d32-4d65-afce-7512432c744a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dba2c6d-f4f5-4572-953f-c8130525d8ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

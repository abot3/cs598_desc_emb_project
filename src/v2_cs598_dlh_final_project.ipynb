{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccba6af-1867-491d-b9d0-d99965738c64",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da368cd-3045-4ec0-86fb-2ce15b5d1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General includes.\n",
    "import os\n",
    "import io\n",
    "import re\n",
    "import errno\n",
    "import gc\n",
    "import random\n",
    "import threading\n",
    "import math\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import pickle\n",
    "import tqdm\n",
    "import hashlib\n",
    "\n",
    "#from termcolor import colored, cprint\n",
    "import colored\n",
    "from datetime import datetime, timedelta\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Typing includes.\n",
    "from typing import Dict, List, Optional, Any, Tuple, Callable, Iterable\n",
    "\n",
    "# Numerical includes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, precision_recall_curve\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# pyHealth includes.\n",
    "from pyhealth.datasets import BaseDataset, MIMIC3Dataset, eICUDataset, SampleDataset, split_by_patient\n",
    "from pyhealth.datasets.utils import MODULE_CACHE_PATH, strptime, hash_str\n",
    "from pyhealth.data import Patient, Visit, Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e714b1-ca5d-4d7b-9ace-6b4372adfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertTokenizerFast\n",
    "from transformers import TensorType\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "646e197e-2577-427c-a385-7f212883d2aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Local imports\n",
    "from tasks.code_emb_funcs import *\n",
    "from tasks.desc_emb_funcs import *\n",
    "from tasks.eicu_funcs import *\n",
    "from tasks.dataset_transforms import *\n",
    "from tasks.collate_funcs import *\n",
    "# from tasks import code_emb_funcs, desc_emb_funcs, eicu_funcs\n",
    "# from tasks import dataset_transforms, collate_funcs\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c5a65f3-5120-479b-9f90-d3e6949f7254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83fa50-9dd0-467b-bd36-634795e4a09c",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef3528-0396-459a-8112-b272089f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "USE_GPU_ = False\n",
    "BERT_USE_GPU_ = True  # BERT embeddings \n",
    "DEV_ = True  # Uses a small subset of MIMIC data: https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html#pyhealth.datasets.MIMIC3Dataset\n",
    "GPU_STR_ = 'cuda'\n",
    "# DATA_DIR_ = os.path.join(os.getcwd(), DATA_DIR_)\n",
    "MIMIC_DATA_DIR_ = '~/sw/physionet.org/files/mimiciii/1.4'\n",
    "EICU_DATA_DIR_ = '~/sw/eicu-collaborative-research-database-2.0/eicu-collaborative-research-database-2.0'\n",
    "BATCH_SIZE_ = 32\n",
    "EMBEDDING_DIM_ = 264  # BERT requires a multiple of 12\n",
    "SHUFFLE_ = True\n",
    "SAMPLE_MULTIPLIER_ = 1\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "seed = 90210\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)\n",
    "        \n",
    "# https://stackoverflow.com/questions/50888391/pickle-of-object-with-getattr-method-in-python-returns-typeerror-object-no\n",
    "class DotArgs(dict):\n",
    "    \"\"\"\n",
    "    Access dictionary attributes via dot notation\n",
    "    \"\"\"\n",
    "    def __getstate__(self):\n",
    "        return vars(self)\n",
    "\n",
    "    def __setstate__(self, state):\n",
    "        vars(self).update(state)\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f8271-44b2-44a2-b7cc-cd7339a70e87",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d5fa9-226e-4612-b28f-ed24de16399a",
   "metadata": {},
   "source": [
    "### Load MIMIC III Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ef1d3-caa8-4db8-b664-86172bea936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    from pyhealth.medcode import InnerMap, ICD9CM\n",
    "\n",
    "    icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "    icd9cm.lookup(\"428.0\") # get detailed info\n",
    "    icd9cm.get_ancestors(\"428.0\") # get parents\n",
    "\n",
    "    print(icd9cm.lookup(\"78951\")) # get detailed info\n",
    "    print(f'78951 ancestors {icd9cm.get_ancestors(\"78951\")}') # get parents\n",
    "\n",
    "    print(icd9cm.lookup(\"7895\")) # get detailed info\n",
    "    print(f'7895 ancestors {icd9cm.get_ancestors(\"7895\")}') # get parents\n",
    "\n",
    "\n",
    "    print(icd9cm.lookup(\"7894\")) # get detailed info\n",
    "    print(f'7894 ancestors {icd9cm.get_ancestors(\"7894\")}') # get parents\n",
    "\n",
    "    print(icd9cm.lookup(\"78942\")) # get detailed info\n",
    "    print(f'78941 ancestors {icd9cm.get_ancestors(\"78941\")}') # get parents\n",
    "\n",
    "    print(ICD9CM.standardize('78951'))\n",
    "    print(ICD9CM.standardize('7895'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb3443-8dfa-4149-904b-04fc24d7a33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def _compute_duration_minutes(start_datetime: str, end_datetime: str) -> float:\n",
    "    '''Return duration in minutes as a float.\n",
    "    '''\n",
    "    # MIMIC-III uses the following format: 2146-07-22 00:00:00\n",
    "    start = datetime.strptime(start_datetime, '%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strptime(end_datetime,   '%Y-%m-%d %H:%M:%S')\n",
    "    return float((end - start).seconds)\n",
    "\n",
    "class MIMIC3DatasetWrapper(MIMIC3Dataset):\n",
    "    ''' Add extra tables to the MIMIC III dataset.\n",
    "    \n",
    "      Some of the tables we need like \"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\"\n",
    "      are not supported out of the box. \n",
    "      \n",
    "      This class defines parsing methods to extract text data from these extra tables.\n",
    "      The text data is generally joined on the PATIENTID, HADMID, ITEMID to match the\n",
    "      pyHealth Vists class representation.\n",
    "    '''\n",
    "   \n",
    "    # We need to add storage for text-based lookup tables here.\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._valid_text_tables = [\"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\", \"D_LABITEMS\"]\n",
    "        self._text_descriptions = {x: {} for x in self._valid_text_tables}\n",
    "        self._text_luts = {x: {} for x in self._valid_text_tables}\n",
    "        self.refresh_cache = False\n",
    "        # The pyHealth dataset cache doesn't know about this class's private members.\n",
    "        if 'refresh_cache' in kwargs:\n",
    "            self.refresh_cache = kwargs['refresh_cache']\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._do_cache()\n",
    "    \n",
    "    def _do_cache(self):\n",
    "        '''The pyHealth dataset cache doesn't know about this classes private members.\n",
    "        \n",
    "          We need to wrap the caching function so it's aware of the additional luts\n",
    "          to be saved/restored. The superclass is still responsible for saving/restoring\n",
    "          `self.patients` from the DB.\n",
    "        '''\n",
    "        self.extra_filepath = ''.join([\n",
    "            os.path.splitext(self.filepath)[0],\n",
    "            '_dev' if self.dev else '',\n",
    "            '_extras.pkl',\n",
    "        ])\n",
    "        \n",
    "        # check if cache exists or refresh_cache is True\n",
    "        if os.path.exists(self.extra_filepath) and (not self.refresh_cache):\n",
    "            # load from cache\n",
    "            logger.info(\n",
    "                f\"Loaded {self.dataset_name} base dataset from {self.extra_filepath}\"\n",
    "            )\n",
    "            from_pickle = load_pickle(self.extra_filepath)\n",
    "            self._valid_text_tables = from_pickle['_valid_text_tables']\n",
    "            self._text_descriptions = from_pickle['_text_descriptions']\n",
    "            self._text_luts = from_pickle['_text_luts']\n",
    "        else:\n",
    "            # load from raw data\n",
    "            logger.info(f\"Processing {self.dataset_name} base dataset...\")\n",
    "            to_cache = {\n",
    "                '_valid_text_tables': self._valid_text_tables,\n",
    "                '_text_descriptions': self._text_descriptions,\n",
    "                '_text_luts': self._text_luts,\n",
    "            }\n",
    "            logger.info(f\"Saved {self.dataset_name} base dataset to {self.extra_filepath}\")\n",
    "            save_pickle(to_cache, self.extra_filepath)\n",
    "    \n",
    "    def get_all_tables(self) -> List[str]: \n",
    "        return list(self._text_descriptions.keys())\n",
    "        \n",
    "    def get_text_dict(self, table_name: str) -> Dict[str, Dict[Any, Any]]:\n",
    "        return self._text_descriptions.get(table_name)\n",
    "    \n",
    "    def set_text_lut(self, table_name: str, lut: Dict[Any, Any]) -> None:\n",
    "        self._text_luts[table_name] = lut\n",
    "    \n",
    "    def get_text_lut(self, table_name: str) -> Dict[Any, Any]:\n",
    "        return self._text_luts[table_name]\n",
    "    \n",
    "    def _add_events_to_patient_dict(\n",
    "        self,\n",
    "        patient_dict: Dict[str, Patient],\n",
    "        group_df: pd.DataFrame,\n",
    "    ) -> Dict[str, Patient]:\n",
    "        #TODO(botelho3) Imported from PyHealth Base dataset githubf to\n",
    "        #support parse_prescription\n",
    "        \"\"\"Helper function which adds the events column of a df.groupby object to the patient dict.\n",
    "        \n",
    "        Will be called at the end of each `self.parse_[table_name]()` function.\n",
    "        Args:\n",
    "            patient_dict: a dict mapping patient_id to `Patient` object.\n",
    "            group_df: a df.groupby object, having two columns: patient_id and events.\n",
    "                - the patient_id column is the index of the patient\n",
    "                - the events column is a list of <Event> objects\n",
    "        Returns:\n",
    "            The updated patient dict.\n",
    "        \"\"\"\n",
    "        for _, events in group_df.items():\n",
    "            for event in events:\n",
    "                patient_dict = self._add_event_to_patient_dict(patient_dict, event)\n",
    "        return patient_dict\n",
    "\n",
    "    \n",
    "    def parse_prescriptions(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:\n",
    "        \"\"\"Helper function which parses PRESCRIPTIONS table.\n",
    "        \n",
    "        TODO(botelho3) - we have to override this to include the text fields. The\n",
    "        prescriptions table does not link to a separate D_ICD_* table in MIMIC-III\n",
    "        thtat contains text descriptions of the prescription. The text descriptions\n",
    "        are in the columns of this table. Regular pyHealth ignores these columns. We\n",
    "        override this method to appent pyHealth Event objects containing the text\n",
    "        columns to each patient.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - PRESCRIPTIONS: https://mimic.mit.edu/docs/iii/tables/prescriptions/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The updated patients dict.\n",
    "        \"\"\"\n",
    "        table = \"PRESCRIPTIONS\"\n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            low_memory=False,\n",
    "            dtype={\"SUBJECT_ID\": str, \"HADM_ID\": str, \"NDC\": str,\n",
    "                   \"DRUG_TYPE\": str, \"DRUG\": str,\n",
    "                   \"PROD_STRENGTH\": str, \"ROUTE\": str, \"ENDDATE\": str},\n",
    "        )\n",
    "        # drop records of the other patients\n",
    "        df = df[df[\"SUBJECT_ID\"].isin(patients.keys())]\n",
    "        df = df.dropna()\n",
    "        # sort by start date and end date\n",
    "        df = df.sort_values(\n",
    "            [\"SUBJECT_ID\", \"HADM_ID\", \"STARTDATE\", \"ENDDATE\"], ascending=True\n",
    "        )\n",
    "        # group by patient and visit\n",
    "        group_df = df.groupby(\"SUBJECT_ID\")\n",
    "        \n",
    "        # parallel unit for prescription (per patient)\n",
    "        def prescription_unit(p_id, p_info):\n",
    "            events = []\n",
    "            for v_id, v_info in p_info.groupby(\"HADM_ID\"):\n",
    "                zipped = zip(v_info[\"STARTDATE\"], v_info[\"NDC\"], v_info[\"DRUG_TYPE\"],\n",
    "                             v_info[\"DRUG\"], v_info[\"PROD_STRENGTH\"], v_info[\"ROUTE\"],\n",
    "                             v_info[\"ENDDATE\"])\n",
    "                for startdate, code, dtype, dname, dose, route, enddate in zipped:\n",
    "                    if not type(startdate) == str:\n",
    "                        startdate = '2142-07-18 00:00:00'\n",
    "                    if not type(enddate) == str:\n",
    "                        enddate = '2142-07-18 00:00:00'\n",
    "                    assert(type(dname) is str)\n",
    "                    # if not type(enddate) is str:\n",
    "                    #     print(f'Not matching enddate {enddate} startdate {startdate}')\n",
    "                    #     print(f'dname {dname}, hadm_id {v_id}, p_id {p_id}')\n",
    "                    assert(type(startdate) is str)\n",
    "                    assert(type(enddate) is str)\n",
    "                    event = Event(\n",
    "                        code=code,\n",
    "                        table=table,\n",
    "                        vocabulary=\"NDC\",\n",
    "                        visit_id=v_id,\n",
    "                        patient_id=p_id,\n",
    "                        timestamp=strptime(startdate),\n",
    "                        dtype=dtype,\n",
    "                        dname=dname,\n",
    "                        dose=dose,\n",
    "                        route=route,\n",
    "                        duration=_compute_duration_minutes(startdate, enddate),\n",
    "                    )\n",
    "                    events.append(event)\n",
    "            return events\n",
    "\n",
    "                # parallel apply\n",
    "        group_df = group_df.parallel_apply(\n",
    "            lambda x: prescription_unit(x.SUBJECT_ID.unique()[0], x)\n",
    "        )\n",
    "\n",
    "        patients = self._add_events_to_patient_dict(patients, group_df)\n",
    "        return patients\n",
    "    \n",
    "    # Note the name has to match the table name exactly.\n",
    "    # See https://github.com/sunlabuiuc/PyHealth/blob/master/pyhealth/datasets/mimic3.py#L71.\n",
    "    def parse_d_icd_diagnoses(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_DIAGNOSIS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_DIAGNOSIS: https://mimic.mit.edu/docs/iii/tables/d_icd_diagnoses/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_DIAGNOSES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text diagnosis description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_DIAGNOSES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    def parse_d_labitems(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_LABITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_LABITEMS: https://mimic.mit.edu/docs/iii/tables/d_labitems/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_LABITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text lab measurement description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_LABITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str, \"FLUID\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_items(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        # TODO(botelho3) - Note this may not be totally useable because the ITEMID\n",
    "        # uinqiue key only links to these tables using ITEMID\n",
    "        #   - INPUTEVENTS_MV \n",
    "        #   - OUTPUTEVENTS on ITEMID\n",
    "        #   - PROCEDUREEVENTS_MV on ITEMID\n",
    "        # \n",
    "        # Not to the tables we want e.g. \n",
    "        \"\"\"Helper function which parses D_ITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ITEMS: https://mimic.mit.edu/docs/iii/tables/d_items/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text inputs/output/procedure events lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_icd_procedures(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_PROCEDURES table.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_PROCEDURES: https://mimic.mit.edu/docs/iii/tables/d_icd_procedures/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_PROCEDURES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text procedure description lookup for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_PROCEDURES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a42a6-441e-438c-96f4-b38ba82fd192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    print(f'Reading data from: `{MIMIC_DATA_DIR_}`')\n",
    "    if 'mimic3base' in globals():\n",
    "        del mimic3base \n",
    "    gc.collect()\n",
    "\n",
    "    mimic3base = MIMIC3DatasetWrapper(\n",
    "        # root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
    "        root=MIMIC_DATA_DIR_,\n",
    "        dataset_name='mimic_3_dataset',\n",
    "        tables=[\"D_ICD_DIAGNOSES\", \"D_ICD_PROCEDURES\", \"D_ITEMS\", \"D_LABITEMS\",\n",
    "                \"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"], # \"LABEVENTS\"],\n",
    "        # map all NDC codes to ATC 3-rd level codes in these tables\n",
    "        # See https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System.\n",
    "        code_mapping={\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})},\n",
    "        # Reads a subset of the data. Disable for full training run.\n",
    "        dev = DEV_,\n",
    "        # True = Slow, rebuilds the dataset instead of caching.\n",
    "        refresh_cache=False,\n",
    "    )\n",
    "\n",
    "    mimic3base.stat()\n",
    "    mimic3base.info()\n",
    "    # table_names = mimic3base.get_all_tables()\n",
    "    # print(table_names)\n",
    "\n",
    "    # print('\\033[92m' '====Tables====\\n' '\\033[0m')\n",
    "    # # print(colored('====Tables====\\n', 'green'))\n",
    "    # # print(colored.fg('green') + '====Tables====\\n')\n",
    "    # for t in table_names:\n",
    "    #     d = mimic3base.get_text_dict(t)\n",
    "    #     print(f\"Table: {t}\")\n",
    "    #     print(d['data'][:5])\n",
    "    #     print('\\n\\n')\n",
    "\n",
    "    # # Take the cached tables from the parse_tables function and build the {ICD9 -> (short_name, long_name)}\n",
    "    # # lookup tables.\n",
    "    # for t in table_names:\n",
    "    #     d = mimic3base.get_text_dict(t)\n",
    "    #     d = d['data']\n",
    "    #     lut = {record[0]: record[1:] for record in d}\n",
    "    #     mimic3base.set_text_lut(t,  lut)\n",
    "\n",
    "    # print('\\033[92m' '====Luts====\\n' '\\033[0m')\n",
    "    # # print(f'{colored.fg(\"green\")} ====Luts====\\n')\n",
    "    # for t in table_names:\n",
    "    #     d = mimic3base.get_text_lut(t)\n",
    "    #     print(f\"Lut {t}:\\n{dict(itertools.islice(d.items(), 2))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aca8e11-6ce6-403b-81a9-92a9a18ef938",
   "metadata": {},
   "source": [
    "### Load eICU Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221daacb-8e59-4849-912a-dff5550f479f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if True:\n",
    "    print(f'Reading data from: `{EICU_DATA_DIR_}`')\n",
    "    if 'eicubase' in globals():\n",
    "        del eicubase \n",
    "    gc.collect()\n",
    "\n",
    "    eicubase = eICUDataset(\n",
    "        # root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
    "        root=EICU_DATA_DIR_,\n",
    "        dataset_name='eicu_dataset',\n",
    "        tables=[\"diagnosis\", \"treatment\", \"medication\"],\n",
    "        # map all NDC codes to ATC 3-rd level codes in these tables\n",
    "        # See https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System.\n",
    "        code_mapping={\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})},\n",
    "        # Reads a subset of the data. Disable for full training run.\n",
    "        dev = False,\n",
    "        # True = Slow, rebuilds the dataset instead of caching.\n",
    "        refresh_cache = False,\n",
    "    )\n",
    "\n",
    "    eicubase.stat()\n",
    "    eicubase.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d0fb35d-de22-4f08-ad59-c42677eb35ae",
   "metadata": {},
   "source": [
    "### Plot Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d5383d9-ec6d-404c-bafe-9b2c7b9c3430",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_model_types = ['code_emb', 'desc_emb', 'desc_emb_ft']\n",
    "predict_model_types = ['code_emb', 'desc_emb', 'desc_emb_ft']\n",
    "task_types = ['mort', 'readm']\n",
    "\n",
    "def PlotAucRecallResults(val_results, test_results, emb_type: str, task_type: str):\n",
    "    ''' Plot AUC-ROC curve and P-R curve.\n",
    "    \n",
    "        val_results: validation set\n",
    "        test_results: test set\n",
    "    '''\n",
    "    if val_results:\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = val_results\n",
    "        plt.plot(rcurve[0], rcurve[1])\n",
    "        plt.title(' '.join(['ROC Curve / AUC Val Set', emb_type, 'for task:', task_type]))\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(recall_curve, precision_curve)\n",
    "        plt.title('PR Curve')\n",
    "        plt.title(' '.join(['PR Curve', emb_type, task_type]))\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.show() \n",
    "        print('Val AUPRC:  {:.2}'.format(auc(recall_curve, precision_curve)))\n",
    "   \n",
    "    if test_results:\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = test_results\n",
    "        plt.plot(rcurve[0], rcurve[1])\n",
    "        plt.title(' '.join(['ROC Curve / AUC Test Set', emb_type, 'for task:', task_type]))\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(recall_curve, precision_curve)\n",
    "        plt.title(' '.join(['PR Curve', emb_type, task_type]))\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.show() \n",
    "        print('Test AUPRC:  {:.2}'.format(auc(recall_curve, precision_curve)))\n",
    "\n",
    "\n",
    "    \n",
    "def PlotDiffResults(results_a, results_b, task_type: str, labels: List[str]):\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    for i in range(len(results_set_labels)):\n",
    "        # ROC Curves for val, test.\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = results_a[i][-1]\n",
    "        plt.plot(rcurve[0], rcurve[1], label=labels[0] + '_' + results_set_labels[i])\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = results_b[i][-1]\n",
    "        plt.plot(rcurve[0], rcurve[1], label=labels[1] + '_' + results_set_labels[i])\n",
    "        plt.title(' '.join(['ROC Curve / AUC', 'for task:', task_type]))\n",
    "        plt.xlabel('FPR')\n",
    "        plt.ylabel('TPR')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        # PR Curves for val, test.\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = results_a[i][-1]\n",
    "        plt.plot(recall_curve, precision_curve, label=labels[0] + '_' + results_set_labels[i])\n",
    "        p, r, f, roc_auc, rcurve, precision_curve, recall_curve, acc = results_b[i][-1]\n",
    "        plt.plot(recall_curve, precision_curve, label=labels[1] + '_' + results_set_labels[i])\n",
    "        plt.title(' '.join(['PR Curve', 'for task:', task_type]))\n",
    "        plt.xlabel('Recall')\n",
    "        plt.ylabel('Precision')\n",
    "        plt.legend()\n",
    "        plt.show() \n",
    "\n",
    "        \n",
    "def PlotPrecisionRecallAcrossEpoch(val_results, test_results):\n",
    "    num_epochs = len(val_results)\n",
    "    epochs = range(num_epochs)\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    \n",
    "    recall_val = []\n",
    "    precision_val = []\n",
    "    recall_test = []\n",
    "    precision_test = []\n",
    "    for i in epochs:\n",
    "        if val_results:\n",
    "            recall_val.append(val_results[i][1])\n",
    "            precision_val.append(val_results[i][0])\n",
    "        if test_results:\n",
    "            recall_test.append(test_results[i][1])\n",
    "            precision_test.append(test_results[i][0])\n",
    "  \n",
    "    if val_results:\n",
    "        plt.plot(epochs, recall_val, label='_'.join(['recall', results_set_labels[0]]))\n",
    "        plt.plot(epochs, precision_val, label='_'.join(['precision', results_set_labels[0]]))\n",
    "    if test_results:\n",
    "        plt.plot(epochs, recall_test, label='_'.join(['recall', results_set_labels[1]]))\n",
    "        plt.plot(epochs, precision_test, label='_'.join(['precision', results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Recall/Precision vs epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall/Precision')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def PlotPrecisionRecallDiffAcrossEpoch(results_a, results_b):\n",
    "    assert(len(results_a) == len(results_b))\n",
    "    num_epochs = len(results_a)\n",
    "    epochs = range(num_epochs)\n",
    "    labels = ['a', 'b']\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    \n",
    "    recall_val_a = []\n",
    "    precision_val_a = []\n",
    "    recall_test_a = []\n",
    "    precision_test_a = []\n",
    "    recall_val_b = []\n",
    "    precision_val_b = []\n",
    "    recall_test_b = []\n",
    "    precision_test_b = []\n",
    "    for i in epochs:\n",
    "        recall_val_a.append(results_a[i][1])\n",
    "        precision_val_a.append(results_a[i][0])\n",
    "        recall_test_a.append(results_a[i][1])\n",
    "        precision_test_a.append(results_a[i][0])\n",
    "        recall_val_b.append(results_b[i][1])\n",
    "        precision_val_b.append(results_b[i][0])\n",
    "        recall_test_b.append(results_b[i][1])\n",
    "        precision_test_b.append(results_b[i][0])\n",
    "   \n",
    "    plt.plot(epochs, recall_val_a, label='_'.join(['recall', labels[0], results_set_labels[0]]))\n",
    "    plt.plot(epochs, recall_test_a, label='_'.join(['recall', labels[0], results_set_labels[1]]))\n",
    "    plt.plot(epochs, recall_val_b, label='_'.join(['recall', labels[1], results_set_labels[0]]))\n",
    "    plt.plot(epochs, recall_test_b, label='_'.join(['recall', labels[1], results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Recall vs Epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    plt.plot(epochs, precision_val_a, label='_'.join(['precision', labels[0], results_set_labels[0]]))\n",
    "    plt.plot(epochs, precision_test_a, label='_'.join(['precision', labels[0], results_set_labels[1]]))\n",
    "    plt.plot(epochs, precision_val_b, label='_'.join(['precision', labels[1], results_set_labels[0]]))\n",
    "    plt.plot(epochs, precision_test_b, label='_'.join(['precision', labels[1], results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Precision vs Epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def PlotAccuracyAcrossEpoch(results_a, results_b=None):\n",
    "    if results_b:\n",
    "        assert(len(results_a[0]) == len(results_b[0]))\n",
    "    num_epochs = len(results_a[0])\n",
    "    epochs = list(range(num_epochs))\n",
    "    task_labels = ['a', 'b']\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    \n",
    "    accuracy_a_val = []\n",
    "    accuracy_a_test = []\n",
    "    accuracy_b_val = []\n",
    "    accuracy_b_test = []\n",
    "    for i in epochs:\n",
    "        if results_a is not None:\n",
    "            accuracy_a_val.append(results_a[0][i][7])\n",
    "            accuracy_a_test.append(results_a[1][i][7])\n",
    "        if results_b is not None:\n",
    "            accuracy_b_val.append(results_b[0][i][7])\n",
    "            accuracy_b_test.append(results_b[1][i][7])\n",
    "            \n",
    "    plt.plot(epochs, accuracy_a_val, label='_'.join(['recall', task_labels[0], results_set_labels[0]]))\n",
    "    plt.plot(epochs, accuracy_a_test, label='_'.join(['recall', task_labels[0], results_set_labels[1]]))\n",
    "    if results_b is not None:\n",
    "        plt.plot(epochs, accuracy_b_val, label='_'.join(['accuracy', task_labels[1], results_set_labels[0]]))\n",
    "        plt.plot(epochs, accuracy_b_test, label='_'.join(['accuracy', task_labels[1], results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Accuracy vs Epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def PlotAccuracyAcrossEpoch(results_a, results_b=None):\n",
    "    if results_b:\n",
    "        assert(len(results_a[0]) == len(results_b[0]))\n",
    "    num_epochs = len(results_a[0])\n",
    "    epochs = list(range(num_epochs))\n",
    "    task_labels = ['a', 'b']\n",
    "    results_set_labels = ['validation', 'test']\n",
    "    \n",
    "    accuracy_a_val = []\n",
    "    accuracy_a_test = []\n",
    "    accuracy_b_val = []\n",
    "    accuracy_b_test = []\n",
    "    for i in epochs:\n",
    "        if results_a is not None:\n",
    "            accuracy_a_val.append(results_a[0][i][7])\n",
    "            accuracy_a_test.append(results_a[1][i][7])\n",
    "        if results_b is not None:\n",
    "            accuracy_b_val.append(results_b[0][i][7])\n",
    "            accuracy_b_test.append(results_b[1][i][7])\n",
    "            \n",
    "    plt.plot(epochs, accuracy_a_val, label='_'.join(['recall', task_labels[0], results_set_labels[0]]))\n",
    "    plt.plot(epochs, accuracy_a_test, label='_'.join(['recall', task_labels[0], results_set_labels[1]]))\n",
    "    if results_b is not None:\n",
    "        plt.plot(epochs, accuracy_b_val, label='_'.join(['accuracy', task_labels[1], results_set_labels[0]]))\n",
    "        plt.plot(epochs, accuracy_b_test, label='_'.join(['accuracy', task_labels[1], results_set_labels[1]]))\n",
    "    plt.title(' '.join(['Accuracy vs Epoch']))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    \n",
    "def PrintFinalAccuracy(results_a, results_b=None):\n",
    "    if results_b:\n",
    "        assert(len(results_a[0]) == len(results_b[0]))\n",
    "    num_epochs = len(results_a[0])\n",
    "    \n",
    "    if results_a:\n",
    "        if len(results_a) > 1:\n",
    "            accuracy_a_val = results_a[0][-1][7]\n",
    "            accuracy_a_test = results_a[1][-1][7]\n",
    "            print('val accuracy:  {:.2}'.format(accuracy_a_val))\n",
    "            print('test accuracy: {:.2}'.format(accuracy_a_test))\n",
    "        else:\n",
    "            accuracy_a_test = results_a[-1][7]\n",
    "            print('test accuracy: {:.2}'.format(accuracy_a_test))\n",
    "    if results_b:\n",
    "        if len(results_b) > 1:\n",
    "            accuracy_b_val = results_b[0][-1][7]\n",
    "            accuracy_b_test = results_b[1][-1][7]\n",
    "            print('val accuracy:  {:.2}'.format(accuracy_b_val))\n",
    "            print('test accuracy: {:.2}'.format(accuracy_b_test))\n",
    "        else:\n",
    "            accuracy_b_test = results_b[-1][7]\n",
    "            print('test accuracy: {:.2}'.format(accuracy_b_test))\n",
    "    \n",
    "    \n",
    "def PrintTrainTime(times):\n",
    "    print('Total train time: {:.2} s'.format(sum(times)))\n",
    "    print('Per epoch times: ' + ', '.join(['{:.2}'.format(t) for t in times]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b8e28-23ac-4ac1-a9b6-ae546e28c905",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Declare tasks for 2 of the 5 prediction tasks specified in the paper. We will create dataloaders for each task that contain the ICD codes and the raw text for each (patient, visit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69f4f2-566a-428f-800a-8539fbb16a15",
   "metadata": {},
   "source": [
    "#### CodeEMB Pred tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30d309-9b95-4a1b-82ee-ae141960c868",
   "metadata": {},
   "source": [
    "#### DescEmb Pred Tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371b36c-571d-482e-86b2-bbfb92522a24",
   "metadata": {},
   "source": [
    "#### Test Load Readmission Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d047ff-6412-462c-9294-a5ba2d3bdba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    # set_task() returns a SampleEHRDataset object\n",
    "    READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "    READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "    task_fn = functools.partial(readmission_pred_task_demb, READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_)\n",
    "    readm_dataset = mimic3base.set_task(task_fn, task_name=readmission_pred_task_demb.__name__)\n",
    "    READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "        code: idx for idx, code in enumerate(sorted(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "    }\n",
    "    readm_dataset.stat()\n",
    "    readm_dataset.samples[1]\n",
    "    # TODO(botelho3) could try a freq codes limit on this.\n",
    "    print(f\"READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(READMISSION_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "          f\"{READMISSION_PER_PATIENT_ICD_9_CODE2IDX_}\")\n",
    "    del readm_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2738557-c9c4-4a33-9fd6-b2dd00bd1d22",
   "metadata": {},
   "source": [
    "#### Test Load Mortality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70586b66-a814-4898-892b-d1bdd339ce7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "    MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "    task_fn = functools.partial(mortality_pred_task_demb, MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_)\n",
    "    mor_dataset = mimic3base.set_task(task_fn, task_name=mortality_pred_task_demb.__name__)\n",
    "    MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "        code: idx for idx, code in enumerate(sorted(MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "    }\n",
    "    mor_dataset.stat()\n",
    "    mor_dataset.samples[1]\n",
    "    # TODO(botelho3) could try a freq codes limit on this.\n",
    "    print(f\"MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "          f\"{MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_}\")\n",
    "    del mor_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0281a-f95b-4991-abd5-5824fc37c520",
   "metadata": {},
   "source": [
    "### DataLoaders and Collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9be7a-3736-4bda-af61-5dcf331bb664",
   "metadata": {},
   "source": [
    "#### Bert Collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32036013-ce5d-407b-af7a-312cf2bc5f15",
   "metadata": {},
   "source": [
    "#### Bert Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056b60a-5861-4e47-b694-891053cc8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_xform = BertTextEmbedTransform(None, EMBEDDING_DIM_, use_tokenizer_fast=True, use_gpu=BERT_USE_GPU_)\n",
    "BERT_EMBEDDING_SIZE = bert_xform.bert_config.hidden_size\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "MORT_DEMB_CODE2IDX = {}\n",
    "MORT_DEMB_CODE_COUNT = {}\n",
    "task_fn = functools.partial(mortality_pred_task_demb, MORT_DEMB_CODE_COUNT)\n",
    "mortality_demb_dataset = mimic3base.set_task(task_fn, task_name=mortality_pred_task_demb.__name__)\n",
    "MORT_DEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORT_DEMB_CODE_COUNT.keys()))\n",
    "}\n",
    "print(f\"MORT_DEMB_CODE2IDX {len(MORT_DEMB_CODE2IDX)}\")\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "mortality_demb_dataset = TextEmbedDataset(mortality_demb_dataset, transform=bert_xform)\n",
    "\n",
    "\n",
    "# mort_demb_train_ds, mort_demb_val_ds, mort_demb_test_ds = split_by_patient(mortality_demb_dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# # create dataloaders (torch.data.DataLoader)\n",
    "# # mort_train_loader = get_dataloader(train_ds, batch_size=32, shuffle=True, collate_fn)\n",
    "# # mort_val_loader = get_dataloader(val_ds, batch_size=32, shuffle=False)\n",
    "# # mort_test_loader = get_dataloader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "# mort_demb_train_loader = DataLoader(\n",
    "#     mort_demb_train_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_per_patient_collate_function,\n",
    "# )\n",
    "# mort_demb_val_loader = DataLoader(\n",
    "#     mort_demb_val_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_per_patient_collate_function,\n",
    "# )\n",
    "# mort_demb_test_loader = DataLoader(\n",
    "#     mort_demb_test_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_per_patient_collate_function,\n",
    "# )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3c8c52-edda-4bdd-95ef-a40106028915",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_xform = BertTextEmbedTransform(None, EMBEDDING_DIM_, use_tokenizer_fast=True, use_gpu=BERT_USE_GPU_)\n",
    "BERT_EMBEDDING_SIZE = bert_xform.bert_config.hidden_size\n",
    "\n",
    "# Readmission Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "READM_DEMB_CODE2IDX = {}\n",
    "READM_DEMB_CODE_COUNT = {}\n",
    "task_fn = functools.partial(readmission_pred_task_demb, READM_DEMB_CODE_COUNT)\n",
    "readmission_demb_dataset = mimic3base.set_task(task_fn, task_name=readmission_pred_task_demb.__name__)\n",
    "READM_DEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(READM_DEMB_CODE_COUNT.keys()))\n",
    "}\n",
    "print(f\"READM_DEMB_CODE2IDX {len(READM_DEMB_CODE2IDX)}\")\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "readmission_demb_dataset = TextEmbedDataset(readmission_demb_dataset, transform=bert_xform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fccee-d5da-428c-a54a-0678a2d19b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Verify DataLoader properties.\n",
    "    # Quick test without running the whole RNN training process.\n",
    "\n",
    "    # from torch.utils.data import DataLoader\n",
    "\n",
    "    # loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "    loader_iter = iter(mort_demb_train_loader)\n",
    "    # for _ in loader_iter:\n",
    "    #     pass\n",
    "    try:\n",
    "        x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "    except StopIteration as e:\n",
    "        print(e)\n",
    "\n",
    "    assert x.dtype == torch.float\n",
    "    assert rev_x.dtype == torch.float\n",
    "    assert y.dtype == torch.float\n",
    "    assert masks.dtype == torch.bool\n",
    "    assert rev_masks.dtype == torch.bool\n",
    "\n",
    "    assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "    assert y.shape == (BATCH_SIZE_, 1)\n",
    "    assert masks.shape == (BATCH_SIZE_, 10, 3)\n",
    "\n",
    "    # assert x[0][0].sum() == 9\n",
    "    # assert masks[0].sum() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c1852-2654-4d7c-9caa-4cd29ae6fd91",
   "metadata": {},
   "source": [
    "#### CodeEmb Collate "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9670e0-b22c-4384-b8ad-f0fca7f1a80e",
   "metadata": {},
   "source": [
    "#### CodeEmb Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4ae73-f079-47ae-ae6f-890d0c5fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "MORT_CEMB_CODE2IDX = {}\n",
    "MORT_CEMB_CODE_COUNT = {}\n",
    "mortality_cemb_ds = mimic3base.set_task(\n",
    "    task_fn=functools.partial(mortality_pred_task_cemb, MORT_CEMB_CODE_COUNT),\n",
    "    task_name=mortality_pred_task_cemb.__name__)\n",
    "# The set_task(...) function iterates over all samples.\n",
    "# Applying the task to each before returning a new dataset.\n",
    "# Since all samples have been processed we have observed all codes and can\n",
    "# build the code->index LUT.\n",
    "MORT_CEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(set(MORT_CEMB_CODE_COUNT.keys())))\n",
    "}\n",
    "print(f'MORT_CEMB_CODE2IDX {len(MORT_CEMB_CODE2IDX)}')\n",
    "\n",
    "# # We need to provide the code->index LUT to the collate function.\n",
    "wrap_code_emb_per_visit_collate_function = functools.partial(\n",
    "    code_emb_per_visit_collate_function,\n",
    "    MORT_CEMB_CODE2IDX)\n",
    "\n",
    "# # Split the dataset into train, val, and test.\n",
    "# mort_cemb_train_ds, mort_cemb_val_ds, mort_cemb_test_ds = split_by_patient(mortality_cemb_ds, [0.8, 0.1, 0.1])\n",
    "# mort_cemb_train_loader = DataLoader(\n",
    "#     mort_cemb_train_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    "# )\n",
    "# mort_cemb_val_loader = DataLoader(\n",
    "#     mort_cemb_val_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    "# )\n",
    "# mort_cemb_test_loader = DataLoader(\n",
    "#     mort_cemb_test_ds,\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718157a8-727d-4b1a-a416-320974a4d121",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "READM_CEMB_CODE2IDX = {}\n",
    "READM_CEMB_CODE_COUNT = {}\n",
    "readm_cemb_ds = mimic3base.set_task(\n",
    "    task_fn=functools.partial(readmission_pred_task_cemb, READM_CEMB_CODE_COUNT),\n",
    "    task_name=readmission_pred_task_cemb.__name__)\n",
    "# The set_task(...) function iterates over all samples.\n",
    "# Applying the task to each before returning a new dataset.\n",
    "# Since all samples have been processed we have observed all codes and can\n",
    "# build the code->index LUT.\n",
    "READM_CEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(set(READM_CEMB_CODE_COUNT.keys())))\n",
    "}\n",
    "print(f'READM_CEMB_CODE2IDX {len(READM_CEMB_CODE2IDX)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ac8e5-cab2-4540-b072-43a5b7404dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    # Verify DataLoader properties.\n",
    "    # Quick test without running the whole RNN training process.\n",
    "\n",
    "    # from torch.utils.data import DataLoader\n",
    "\n",
    "    # loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "    loader_iter = iter(mort_cemb_train_loader)\n",
    "    for _ in loader_iter:\n",
    "        pass\n",
    "\n",
    "    try:\n",
    "        x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "    except StopIteration as e:\n",
    "        print(e)\n",
    "\n",
    "    # assert x.dtype == torch.float\n",
    "    # assert rev_x.dtype == torch.float\n",
    "    # assert y.dtype == torch.float\n",
    "    # assert masks.dtype == torch.bool\n",
    "    # assert rev_masks.dtype == torch.bool\n",
    "\n",
    "    # assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "    # assert y.shape == (BATCH_SIZE_, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a54dde-d0c7-4b39-9dfe-cb2f4580fda4",
   "metadata": {},
   "source": [
    "#### Bert ICU Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a9c2e3-7753-41fa-a065-bca3db612e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_xform = BertTextEmbedTransform(None, EMBEDDING_DIM_,\n",
    "                                    use_tokenizer_fast=True, use_gpu=BERT_USE_GPU_)\n",
    "BERT_EMBEDDING_SIZE = bert_xform.bert_config.hidden_size\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "CODE2IDX = {}\n",
    "CODE_COUNT = {}\n",
    "task_fn = functools.partial(eicu_mortality_pred_task_demb, CODE_COUNT)\n",
    "eicu_mortality_demb_dataset = eicubase.set_task(task_fn, task_name=eicu_mortality_pred_task_demb.__name__)\n",
    "CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(CODE_COUNT.keys()))\n",
    "}\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "eicu_mortality_demb_dataset = TextEmbedDataset(eicu_mortality_demb_dataset,\n",
    "                                               transform=bert_xform,\n",
    "                                               should_cache=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b13a5-6db6-4629-b722-7cb47f0919dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# indices = range(0, min(50000, len(eicu_mortality_demb_dataset)) )\n",
    "# eicu_mort_demb_loader = DataLoader(\n",
    "#     torch.utils.data.Subset(eicu_mortality_demb_dataset, indices),\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_per_patient_collate_function_new_trainer,\n",
    "# )\n",
    "# loader_iter = iter(eicu_mort_demb_loader)\n",
    "# # for _ in loader_iter:\n",
    "# #     pass\n",
    "# try:\n",
    "#     x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "# except StopIteration as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97059047-b368-40b8-8d31-e59266d80c05",
   "metadata": {},
   "source": [
    "#### Dataset Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e22e04f-c7f9-4c0c-b531-19fad45feddd",
   "metadata": {},
   "source": [
    "Caches a dataset to disk via pickle after preprocessing/collate functions have been applied. Can be used to transform samples into more compact representation or perform preprocessing once instead of during each epoch."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ba52f9-55a3-4ab3-9ff0-a1fb3685fa4a",
   "metadata": {},
   "source": [
    "##### Cemb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e3313-641c-406d-aba8-b1fd83df44cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to cut down the data size the collate fn is going to have to happen later during batch run.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "wrap_code_emb_per_visit_collate_function = functools.partial(\n",
    "    code_emb_per_visit_collate_function,\n",
    "    MORT_CEMB_CODE2IDX)\n",
    "mort_cemb_loader = DataLoader(\n",
    "    mortality_cemb_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n",
    "extra_data = {\n",
    "    'code2idx': MORT_CEMB_CODE2IDX,\n",
    "    'embed_index_size': len(MORT_CEMB_CODE2IDX),\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "mort_cemb_cacher_metadata = cacher.DatasetToCacheFromLoader(mort_cemb_loader,\n",
    "                      mortality_pred_task_cemb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)\n",
    "# i = iter(mort_cemb_loader)\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c6e300-dc9a-4049-815d-c1ab6fe44335",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(mort_cemb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(mort_cemb_loader.dataset)}\n",
    "mort_cemb_loader, mort_cemb_metadata = (\n",
    "    cacher.DataloaderFromCache(mortality_pred_task_cemb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(mort_cemb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b87a71-15bd-4b7d-9412-e95a22451de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to cut down the data size the collate fn is going to have to happen later during batch run.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "wrap_code_emb_per_visit_collate_function = functools.partial(\n",
    "    code_emb_per_visit_collate_function,\n",
    "    READM_CEMB_CODE2IDX)\n",
    "readm_cemb_loader = DataLoader(\n",
    "    readm_cemb_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n",
    "extra_data = {\n",
    "    'code2idx': READM_CEMB_CODE2IDX,\n",
    "    'embed_index_size': len(READM_CEMB_CODE2IDX),\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "readm_cemb_cacher_metadata = cacher.DatasetToCacheFromLoader(readm_cemb_loader,\n",
    "                      readmission_pred_task_cemb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)\n",
    "# i = iter(mort_cemb_loader)\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b297dd3-bb56-425e-990b-2bcd6cf99dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(readm_cemb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(readm_cemb_loader.dataset)}\n",
    "readm_cemb_loader, readm_cemb_metadata = (\n",
    "    cacher.DataloaderFromCache(readmission_pred_task_cemb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(readm_cemb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d281471-8c3b-42c8-b153-12993d6c06f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    del readm_cemb_ds\n",
    "    del readm_cemb_loader\n",
    "    del mortality_cemb_ds\n",
    "    del mortality_cemb_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750ab8b7-2172-4657-a825-0b772a5780bc",
   "metadata": {},
   "source": [
    "##### Demb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6591bc67-93b6-4217-a454-bd92d76b6a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need packed_sequence for this.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "indices = list(range(0, min(10000, len(mortality_demb_dataset)) ))\n",
    "print(len(mortality_demb_dataset))\n",
    "print(len(torch.utils.data.Subset(mortality_demb_dataset, indices)))\n",
    "extra_data = {\n",
    "    'embed_index_size': 768,\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "\n",
    "def numpy_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    return (tensor.numpy(), label)\n",
    "\n",
    "def bytes_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    bio = io.BytesIO()\n",
    "    torch.save(tensor, bio)\n",
    "    return (bio.getvalue(), label)\n",
    "    \n",
    "mort_demb_loader = DataLoader(\n",
    "    torch.utils.data.Subset(mortality_demb_dataset, indices),\n",
    "    batch_size=1,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=numpy_conversion_function,\n",
    "    # collate_fn=bytes_conversion_function,\n",
    ")\n",
    "mort_demb_cacher_metadata = cacher.DatasetToCacheFromLoader(mort_demb_loader,\n",
    "                      mortality_pred_task_demb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)\n",
    "# i = iter(mort_demb_loader)\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38b458f-350d-4d81-b637-c802d44a35f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(mort_demb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(mort_demb_loader.dataset)}\n",
    "mort_demb_loader, mort_demb_metadata = (\n",
    "    cacher.DataloaderFromCache(mortality_pred_task_demb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(mort_demb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7247d011-d158-4f78-b699-3d28da9004f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need packed_sequence for this.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "indices = list(range(0, min(10000, len(readmission_demb_dataset)) ))\n",
    "print(len(readmission_demb_dataset))\n",
    "print(len(torch.utils.data.Subset(readmission_demb_dataset, indices)))\n",
    "extra_data = {\n",
    "    'embed_index_size': 768,\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "\n",
    "def numpy_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    return (tensor.numpy(), label)\n",
    "\n",
    "def bytes_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    bio = io.BytesIO()\n",
    "    torch.save(tensor, bio)\n",
    "    return (bio.getvalue(), label)\n",
    "    \n",
    "readm_demb_loader = DataLoader(\n",
    "    torch.utils.data.Subset(readmission_demb_dataset, indices),\n",
    "    batch_size=1,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=numpy_conversion_function,\n",
    "    # collate_fn=bytes_conversion_function,\n",
    ")\n",
    "readm_demb_cacher_metadata = cacher.DatasetToCacheFromLoader(readm_demb_loader,\n",
    "                      readmission_pred_task_demb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)\n",
    "# i = iter(mort_demb_loader)\n",
    "# print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db8b92-221a-417d-b33b-46a51c8c46f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(readm_demb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(readm_demb_loader.dataset)}\n",
    "readm_demb_loader, readm_demb_metadata = (\n",
    "    cacher.DataloaderFromCache(readmission_pred_task_demb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(readm_demb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6bebc5-23b3-4a1f-966c-a3921d5de853",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    del bert_xform\n",
    "    del mort_demb_loader\n",
    "    del mortality_demb_dataset\n",
    "    del readm_demb_loader\n",
    "    del readmission_demb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59600576-c21f-4338-8c27-90600cef3721",
   "metadata": {},
   "source": [
    "##### Demb-eICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac044d6a-6928-4623-b5a8-e48ddb751fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Might need packed_sequence for this.\n",
    "cacher = DatasetCacher()\n",
    "\n",
    "# indices = list(range(0, min(10000, len(mortality_demb_dataset)) ))\n",
    "# print(len(mortality_demb_dataset))\n",
    "# print(len(torch.utils.data.Subset(mortality_demb_dataset, indices)))\n",
    "extra_data = {\n",
    "    'embed_index_size': 768,\n",
    "    'keywords': ['x', 'masks', 'rev_x', 'rev_masks', 'y'],\n",
    "}\n",
    "\n",
    "\n",
    "def numpy_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    return (tensor.numpy(), label)\n",
    "\n",
    "def bytes_conversion_function(sample):\n",
    "    # This is faster than pickling tensors\n",
    "    # https://github.com/pytorch/pytorch/issues/9168\n",
    "    tensor, label = sample[0]\n",
    "    bio = io.BytesIO()\n",
    "    torch.save(tensor, bio)\n",
    "    return (bio.getvalue(), label)\n",
    "\n",
    "indices = range(0, min(50000, len(eicu_mortality_demb_dataset)) )\n",
    "eicu_mort_demb_loader = DataLoader(\n",
    "    torch.utils.data.Subset(eicu_mortality_demb_dataset, indices),\n",
    "    batch_size=1,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=numpy_conversion_function,\n",
    ")\n",
    "\n",
    "mort_demb_cacher_metadata = cacher.DatasetToCacheFromLoader(eicu_mort_demb_loader,\n",
    "                      eicu_mortality_pred_task_demb,\n",
    "                      batch_size=0,  # we already batched using loader+collate.\n",
    "                      overwrite=False,\n",
    "                      extra_data=extra_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf8f3b9-1bd3-471e-b3c4-7abc43f81344",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"len dataset {len(eicu_mort_demb_loader.dataset)}\")\n",
    "cacher = DatasetCacher()\n",
    "metadata_in = {'batch_size': 0, 'length': len(eicu_mort_demb_loader.dataset)}\n",
    "eicu_mort_demb_loader, eicu_mort_demb_metadata = (\n",
    "    cacher.DataloaderFromCache(eicu_mortality_pred_task_demb.__name__,\n",
    "                               metadata_in['batch_size'],\n",
    "                               metadata_in['length'])\n",
    ")\n",
    "i = iter(eicu_mort_demb_loader)\n",
    "print(next(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6d6b38-7aff-413b-a035-d8c7843d9164",
   "metadata": {},
   "outputs": [],
   "source": [
    "if False:\n",
    "    del bert_xform\n",
    "    del eicu_mort_demb_loader\n",
    "    del eicu_mortality_demb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e770aa-7bc1-4a16-a936-bef845b4a23e",
   "metadata": {},
   "source": [
    "### Bert Fine Tune Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199b392f-3f9d-4ca2-b75a-11d5b2f0e4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_ft_xform = BertFineTuneTransform()\n",
    "BERT_FT_EMBEDDING_SIZE = bert_ft_xform.emb_size\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "MORT_DEMBFT_CODE2IDX = {}\n",
    "MORT_DEMBFT_CODE_COUNT = {}\n",
    "task_fn = functools.partial(mortality_pred_task_demb, MORT_DEMBFT_CODE_COUNT)\n",
    "mortality_dembft_ds = mimic3base.set_task(task_fn, task_name=mortality_pred_task_demb.__name__)\n",
    "MORT_DEMBFT_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORT_DEMBFT_CODE_COUNT.keys()))\n",
    "}\n",
    "\n",
    "\n",
    "indices = list(range(0, min(10000, len(mortality_dembft_ds)) ))\n",
    "print(len(mortality_dembft_ds))\n",
    "print(len(torch.utils.data.Subset(mortality_dembft_ds, indices)))\n",
    "\n",
    "print(f\"MORT_DEMB_CODE2IDX {len(MORT_DEMBFT_CODE2IDX)}\")\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "tmp = TextEmbedDataset(mortality_dembft_ds, transform=bert_ft_xform, should_cache=False)\n",
    "mortality_dembft_dataset = torch.utils.data.Subset(tmp, indices)\n",
    "print(len(mortality_dembft_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e216981-5973-4cd4-bf6e-39c1266c2a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if True:\n",
    "    # Verify DataLoader properties.\n",
    "    # Quick test without running the whole RNN training process.\n",
    "    mort_dembft_loader = DataLoader(\n",
    "        mortality_dembft_dataset,\n",
    "        batch_size=BATCH_SIZE_,\n",
    "        shuffle=SHUFFLE_,\n",
    "        collate_fn=bert_fine_tune_collate,\n",
    "    )\n",
    "    # loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "    loader_iter = iter(mort_dembft_loader)\n",
    "    # for _ in loader_iter:\n",
    "    #     pass\n",
    "    try:\n",
    "        x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "    except StopIteration as e:\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a26b868-8bb4-4c84-9457-b6e6244c269b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free some memory\n",
    "del bert_ft_xform\n",
    "del mortality_dembft_ds\n",
    "del mortality_dembft_dataset\n",
    "gc.collect()\n",
    "del bert_xform\n",
    "del mortality_demb_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52d93b7f-35a9-4b64-997e-4aabf5f5db96",
   "metadata": {},
   "source": [
    "# Condensed Training using Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b96518-be15-4323-ad4e-25ccb33aaa59",
   "metadata": {},
   "source": [
    "### CodeEmb - Mort - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f908a57-e47d-461a-872b-8ffac66aa12e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "\n",
    "embed_model_types = ['code_emb', 'desc_emb', 'desc_emb_ft']\n",
    "predict_model_types = ['code_emb', 'desc_emb', 'desc_emb_ft']\n",
    "task_types = ['mort', 'readm']\n",
    "# Dev set.\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "# READM_DEMB_CODE2IDX 1523\n",
    "# READM_CEMB_CODE2IDX 1456\n",
    "# Full set.\n",
    "# READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 6555\n",
    "# MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 6732\n",
    "# Full set.\n",
    "# MORT_DEMB_CODE2IDX 6732\n",
    "# MORT_CEMB_CODE2IDX 6546\n",
    "# READM_CEMB_CODE2IDX 6546\n",
    "# READM_DEMB_CODE2IDX 6555\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'code_emb'\n",
    "args.predict_model_type = 'code_emb'\n",
    "args.collate_fn=None\n",
    "args.embed_index_size = 1456 # mort_cemb_metadata['extra_data']['embed_index_size']\n",
    "args.pred_embed_size = 128\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56ab114-394f-49e2-84b5-acf100b31989",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "cemb_mort_dev_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f96fb1a0-de96-4f31-8449-d09aa1c37183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = cemb_mort_dev_results[0]\n",
    "test_set_results = cemb_mort_dev_results[1]\n",
    "PrintTrainTime(cemb_mort_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Cemb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(cemb_mort_dev_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "541c0384-6918-4d12-a5fd-bec8b52770aa",
   "metadata": {},
   "source": [
    "### DescEmb - Mort - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28731f43-6486-4e30-b17a-b78d73375040",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b473b08-adc9-4ce0-a167-eab1d7551286",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "demb_mort_dev_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4598c1-6fc6-4099-9386-87ee8f9f3a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in demb_mort_results.\n",
    "val_set_results = demb_mort_dev_results[0]\n",
    "test_set_results = demb_mort_dev_results[1]\n",
    "PrintTrainTime(demb_mort_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(demb_mort_dev_results)\n",
    "labels = ['cemb', 'demb']\n",
    "PlotDiffResults(cemb_mort_dev_results, demb_mort_dev_results, task_type='Mortality', labels=labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d8992a6-baaa-4dfc-8576-1d9c3141f15b",
   "metadata": {},
   "source": [
    "### CodeEmb - Readm - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e296f7f4-b820-4e27-ad86-7f8cc46747dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# Dev set.\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "# READM_DEMB_CODE2IDX 1523\n",
    "# READM_CEMB_CODE2IDX 1456\n",
    "# Full set.\n",
    "# READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 6555\n",
    "# MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 6732\n",
    "# Full set.\n",
    "# MORT_DEMB_CODE2IDX 6732\n",
    "# MORT_CEMB_CODE2IDX 6546\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'readm'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'code_emb'\n",
    "args.predict_model_type = 'code_emb'\n",
    "args.collate_fn=None\n",
    "args.embed_index_size = 1456\n",
    "args.pred_embed_size = 128\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099f098d-0288-4e20-a2b9-b2e8b5ca5855",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "cemb_readm_dev_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577cfd45-41ed-4321-aa5f-1aec2c7d2e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = cemb_readm_dev_results[0]\n",
    "test_set_results = cemb_readm_dev_results[1]\n",
    "PrintTrainTime(cemb_readm_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Cemb', task_type='Readmission')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(cemb_readm_dev_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceaf2af0-598d-458c-9ca1-1d4189c56625",
   "metadata": {},
   "source": [
    "### DescEmb - Readm - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b65f8b9-5632-499c-9e42-bba7b4ac396d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'readm'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fb54cb5-9f45-422a-a23c-1227ab31ae86",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "demb_readm_dev_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "731fed3d-9d78-45a1-8a82-5ebd408c795b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in demb_mort_results.\n",
    "val_set_results = demb_readm_dev_results[0]\n",
    "test_set_results = demb_readm_dev_results[1]\n",
    "PrintTrainTime(demb_readm_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(demb_readm_dev_results)\n",
    "labels = ['cemb', 'demb']\n",
    "PlotDiffResults(cemb_readm_dev_results, demb_readm_dev_results, task_type='Mortality', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961ebac5-2752-4a6c-818a-b058930e20f6",
   "metadata": {},
   "source": [
    "### CodeEmb - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8496646-b25b-4170-b0a2-3a1cbb5a9b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# Dev set.\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "# Full set.\n",
    "# READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 6555\n",
    "# MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 6732\n",
    "# Full set.\n",
    "# MORT_DEMB_CODE2IDX 6732\n",
    "# MORT_CEMB_CODE2IDX 6546\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'code_emb'\n",
    "args.predict_model_type = 'code_emb'\n",
    "args.collate_fn=None\n",
    "args.embed_index_size = 6546  # mort_cemb_metadata['extra_data']['embed_index_size']\n",
    "args.pred_embed_size = 128\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e953a1a7-271d-4c5e-93eb-29188b57c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "cemb_mort_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0425611-1882-44d3-839c-f2c894265b53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = cemb_mort_full_results[0]\n",
    "test_set_results = cemb_mort_full_results[1]\n",
    "PrintTrainTime(cemb_mort_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Cemb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(cemb_mort_full_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024acbfb-0d8d-49a3-b1ac-11b9f3dad369",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DescEmb - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a985f3fd-3e87-4b31-9ff0-438184e8bd1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer \n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c629cc3-4780-4eef-a610-69d90c928e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "demb_mort_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873b3e43-a636-4ca4-9f30-383cfff67cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = demb_mort_full_results[0]\n",
    "test_set_results = demb_mort_full_results[1]\n",
    "PrintTrainTime(demb_mort_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(demb_mort_full_results)\n",
    "labels = ['cemb', 'demb']\n",
    "PlotDiffResults(cemb_mort_full_results, demb_mort_full_results, task_type='Mortality', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814ce321-3240-467b-b443-f636e4197c33",
   "metadata": {},
   "source": [
    "### CodeEmb - Readm - Full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e55d77-5075-42b1-9905-b637a17cbaba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# Dev set.\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "# READM_DEMB_CODE2IDX 1523\n",
    "# READM_CEMB_CODE2IDX 1456\n",
    "# Full set.\n",
    "# READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: 6555\n",
    "# MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: 6732\n",
    "# Full set.\n",
    "# MORT_DEMB_CODE2IDX 6732\n",
    "# MORT_CEMB_CODE2IDX 6546\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'readm'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'code_emb'\n",
    "args.predict_model_type = 'code_emb'\n",
    "args.collate_fn=None\n",
    "args.embed_index_size = 6546\n",
    "args.pred_embed_size = 128\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c65aec-c2f7-4e08-b648-0f37f94222f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33426 / 32 * 0.8 = 837\n",
    "trainer = Trainer(args)\n",
    "cemb_readm_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee15a307-386a-440d-939f-b5daaf8e22e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = cemb_readm_full_results[0]\n",
    "test_set_results = cemb_readm_full_results[1]\n",
    "PrintTrainTime(cemb_readm_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Cemb', task_type='Readmission')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(cemb_readm_full_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afdc6ac8-ef50-4cae-9789-bd909d109131",
   "metadata": {},
   "source": [
    "### DescEmb - Readm - Full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a660b0-a8f7-47a2-840d-84247b3434d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'readm'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b411d5-9058-47e6-bb38-a1aa6e9738fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10000 / 32 * 0.8 = 250\n",
    "trainer = Trainer(args)\n",
    "demb_readm_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2829d06-4e9a-4db0-881c-d71464ded609",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in demb_mort_results.\n",
    "val_set_results = demb_readm_full_results[0]\n",
    "test_set_results = demb_readm_full_results[1]\n",
    "PrintTrainTime(demb_readm_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Demb', task_type='Readmission')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(demb_readm_full_results)\n",
    "labels = ['cemb', 'demb']\n",
    "PlotDiffResults(cemb_readm_full_results, demb_readm_full_results, task_type='Readmission', labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4719519d-6bde-473b-a93b-d148bec8f6ba",
   "metadata": {},
   "source": [
    "### DescEmbFt - Mort - Dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab5a4d3-b8db-4a37-9080-f0332e01c779",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "import torch\n",
    "# from GPUtil import showUtilization as gpu_usage\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = True  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb_ft'\n",
    "args.predict_model_type = 'desc_emb_ft'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_fine_tune_collate\n",
    "args.no_use_cached_dataset = mortality_dembft_dataset\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a95d438-55f3-41fb-b04f-17979585ff92",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "gc.collect()\n",
    "del trainer\n",
    "gc.collect()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a20ebb-4167-4791-b20a-a490b69a1a60",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "dembft_mort_dev_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54676cb-eef9-4b5c-bd3b-9dc2d4b9b953",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in demb_mort_results.\n",
    "val_set_results = dembft_mort_dev_results[0]\n",
    "test_set_results = dembft_mort_dev_results[1]\n",
    "PrintTrainTime(dembft_mort_dev_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Dembft', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(dembft_mort_dev_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6764e42-107f-420d-9c3e-aee19848c6cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DescEmbFt - Mort - Full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8cc0fa-105e-4db1-be77-2e125301e120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.db_name = 'mimic'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.embed_model_type = 'desc_emb_ft'\n",
    "args.predict_model_type = 'desc_emb_ft'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_fine_tune_collate\n",
    "args.no_use_cached_dataset = mortality_dembft_dataset\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70346c81-9b48-44b6-ab24-fe5a3fb024de",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "gc.collect()\n",
    "del trainer\n",
    "gc.collect()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b8d14dc-da6c-4d4b-ae8d-8e09ce5fd4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "dembft_mort_full_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f953d4a-f70d-4497-a9f1-33305b088d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch in cemb_mort_results.\n",
    "val_set_results = dembft_mort_full_results[0]\n",
    "test_set_results = dembft_mort_full_results[1]\n",
    "PrintTrainTime(dembft_mort_full_results[2])\n",
    "PlotAucRecallResults(val_set_results[-1], test_set_results[-1],\n",
    "                     emb_type='Dembft', task_type='Mortality')\n",
    "PlotPrecisionRecallAcrossEpoch(val_set_results, test_set_results)\n",
    "PlotAccuracyAcrossEpoch(dembft_mort_full_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf69d78-6151-4eb8-9788-ea3ed5b3797d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "39b8acf1-17b4-40c4-974c-112cbfc804d4",
   "metadata": {},
   "source": [
    "# eICU Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6af1f71d-5fdf-43d2-a4ea-e1997e417298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_model(model, data_loader):\n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    # for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "    for sample_dict in tqdm.tqdm(data_loader):\n",
    "        y = sample_dict['y']\n",
    "        y_hat = model(**sample_dict)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    rcurve = roc_curve(y_true, y_score)\n",
    "    precision_curve, recall_curve, thresholds = precision_recall_curve(y_true, y_score)\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return [(p, r, f, roc_auc, rcurve, precision_curve, recall_curve, accuracy)]\n",
    "\n",
    "\n",
    "def load_model_state_from_file(fname):\n",
    "    path = os.path.join(os.getcwd(), 'modelcache', fname)\n",
    "    d = torch.load(path)\n",
    "    model_state_dict = d['model_state_dict']\n",
    "    args = d['args']\n",
    "    print(f'Loading model state with args{args} from \\n {path}')\n",
    "    return model_state_dict, args\n",
    "\n",
    "\n",
    "def load_model_from_file(embed_model_type: str,\n",
    "                         task: str,\n",
    "                         is_dev: bool):\n",
    "    fname = f'model_{embed_model_type}_task_{task}_isdev{is_dev}'\n",
    "    fname = fname + \"_best.pt\"\n",
    "    model_state_dict, args = load_model_state_from_file(fname)\n",
    "    model = EHRModel(args)\n",
    "    model.load_state_dict(model_state_dict)\n",
    "    model.eval()\n",
    "    return model, args"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c28ad30-bb14-4112-aacb-8f8ea6859987",
   "metadata": {},
   "source": [
    "### DescEMB eICU Pred Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a83b4f-226d-4d14-be66-f2d67eb58121",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# re_drug_prefix = re.compile('Event with eICU_DRUGNAME code{.*}from table medication')\n",
    "# re_bar = re.compile('\\|')\n",
    "# ICD_9_LUT_ = {}\n",
    "# ICD_10_LUT_ = {}\n",
    "# fname = os.path.expanduser('~/sw/icd10cm-code descriptions- April 1 2023/icd10cm-codes- April 1 2023.txt')\n",
    "# with open(fname, 'r') as f:\n",
    "#     for l in f:\n",
    "#         code, desc = l.split(sep=' ', maxsplit=1)\n",
    "#         ICD_10_LUT_[code] = desc\n",
    "        \n",
    "# def eicu_mortality_pred_task_demb(CODE_COUNT, patient):\n",
    "#     \"\"\"\n",
    "#     patient is a <pyhealth.data.Patient> object\n",
    "#     \"\"\"\n",
    "#     samples = []\n",
    "#     visits = []\n",
    "#     kMaxListSize = 40\n",
    "    \n",
    "#     global_mortality_label = 0\n",
    "#     # loop over all visits but the last one\n",
    "#     for i in range(len(patient)):\n",
    "\n",
    "#         # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "#         # there are no vists.attr_dict keys\n",
    "#         visit: Visit = patient[i]\n",
    "#         mortality_label = 0 if visit.discharge_status == 'Alive' else 1\n",
    "#         global_mortality_label |= mortality_label\n",
    "        \n",
    "#     # loop over all visits but the last one\n",
    "#     for i, visit in enumerate(patient):\n",
    "#         # visit: Visit.\n",
    "        \n",
    "#         # step 2: get code-based feature information\n",
    "#         conditions = visit.get_code_list(table=\"diagnosis\")\n",
    "#         procedures = visit.get_code_list(table=\"treatment\")\n",
    "#         # drugs = [x.code for x in visit.get_event_list(table=\"medication\")]\n",
    "#         drugs_full = visit.get_event_list(table=\"medication\")\n",
    "#         drugs_full = [d.code for d in drugs_full]\n",
    "#         # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "#         # if i == 0: print(conditions)\n",
    "#         # if i == 0: print(procedures)\n",
    "#         # if i == 0: print(drugs)\n",
    "#         # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "#         # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "#         # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "#         if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "#             # print(f'Excluded something 0 {len(conditions)}, {len(procedures)}, {len(drugs_full)}')\n",
    "#             # print(f'conditions {conditions}')\n",
    "#             # print(f'procedures {procedures}')\n",
    "#             # print(f'drugs_full {drugs_full}')\n",
    "#             continue\n",
    "#         if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "#             # Exclude stays with less than 5 procedures.\n",
    "#             continue\n",
    "        \n",
    "#         # step 3.5: build text lists from the ICD codes\n",
    "#         # diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "#         # proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        \n",
    "#         # if i == 0: print(d_diag)\n",
    "#         # if i == 0: print(d_proc)\n",
    "#         # Index 0 is shortname, index 1 is longname.\n",
    "#         # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "#         # print(d_proc.get(procedures[0]))\n",
    "#         # print(f'condition {conditions}')\n",
    "#         # print(f'proc {procedures}')\n",
    "#         # print(f'drugs {drugs_full}')\n",
    "#         # conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "#         # procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "#         conditions = filter(lambda x: True if x[0].isalpha() else False, conditions)\n",
    "#         conditions = [cond.replace('.', '') for cond in conditions]\n",
    "#         conditions_text = [ICD_10_LUT_.get(cond, '') for cond in conditions] \n",
    "#         procedures_text = [re_bar.sub(' ', proc) for proc in procedures]\n",
    "#         drugs_text = [re_drug_prefix.sub('\\1', str(d)) for d in drugs_full]\n",
    "#         # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "#         # labevents_text =\n",
    "        \n",
    "#         # step 4: assemble the samples into a pyHealth Visit.\n",
    "#         visits.append(\n",
    "#             {\n",
    "#                 \"visit_id\": visit.visit_id,\n",
    "#                 \"patient_id\": patient.patient_id,\n",
    "#                 # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "#                 \"conditions\": conditions,\n",
    "#                 \"procedures\": procedures,\n",
    "#                 \"conditions_text\": conditions_text,\n",
    "#                 \"procedures_text\": procedures_text,\n",
    "#                 \"drugs_text\": drugs_text,\n",
    "#                 # \"labevents\": labevents,\n",
    "#                 # \"labevents_text\": labevents_text\n",
    "#                 \"label\": global_mortality_label,\n",
    "#             }\n",
    "#         )\n",
    "   \n",
    "    \n",
    "#     # Return empty list, didn't meet exclusion criteria.\n",
    "#     num_visits = len(visits)\n",
    "#     if num_visits < 1:\n",
    "#         return [] \n",
    "    \n",
    "   \n",
    "#     # pyHealth requires that all list fields in sample are equal size.\n",
    "#     def pad_field(field, visits, empty_val: Any):\n",
    "#         l = [empty_val for x in range(kMaxListSize)]\n",
    "#         data = [x[field] for x in visits]\n",
    "#         data = list(itertools.chain.from_iterable(data))\n",
    "#         slice_size = min(kMaxListSize, len(data))\n",
    "#         l[:slice_size] = data[:slice_size]\n",
    "#         return l, slice_size\n",
    "    \n",
    "#     conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "#     conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "#     procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "#     procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "#     drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "#     sample = {\n",
    "#         \"patient_id\": patient.patient_id,\n",
    "#         # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "#         \"visit_id\": visits[0][\"visit_id\"],\n",
    "#         \"num_visits\": num_visits,\n",
    "#         # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "#         \"conditions\": conditions,\n",
    "#         \"conditions_text\": conditions_text,\n",
    "#         # \"procedures\": procedures,\n",
    "#         \"procedures_text\": procedures_text,\n",
    "#         \"drugs_text\": drugs_text,\n",
    "        \n",
    "#         \"conditions_pad\": conditions_pad,\n",
    "#         \"procedures_pad\": procedures_pad,\n",
    "#         \"conditions_text_pad\": conditions_text_pad,\n",
    "#         \"procedures_text_pad\": procedures_text_pad,\n",
    "#         \"drugs_text_pad\": drugs_text_pad,\n",
    "#         # \"labevents\": labevents,\n",
    "#         # \"labevents_text\": labevents_text\n",
    "#         \"label\": global_mortality_label,\n",
    "#     }\n",
    "   \n",
    "#     # For every condition in the sample (all visits). Record frequency.\n",
    "#     # Will be used to build code->index LUT.\n",
    "#     for code in sample['conditions']:\n",
    "#         CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "       \n",
    "#     # if len(CODE_COUNT) in [10,11,12]:\n",
    "#     #     print(sample)\n",
    "#     samples.append(sample)\n",
    "#     return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c228b82-a360-466f-bfb9-a19019ec5ae6",
   "metadata": {},
   "source": [
    "### Test Load Mortality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "773721d7-5b98-452a-9721-e752cf0f2619",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if False:\n",
    "    EICU_MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "    EICU_MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "    task_fn = functools.partial(eicu_mortality_pred_task_demb, EICU_MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_)\n",
    "    eicu_mor_dataset = eicubase.set_task(task_fn, task_name=eicu_mortality_pred_task_demb.__name__)\n",
    "    EICU_MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "        code: idx for idx, code in enumerate(sorted(EICU_MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "    }\n",
    "    eicu_mor_dataset.stat()\n",
    "    eicu_mor_dataset.samples[1]\n",
    "    # TODO(botelho3) could try a freq codes limit on this.\n",
    "    # print(f\"MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(EICU_MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "    #       f\"{EICU_MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_}\")\n",
    "    del eicu_mor_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7905508d-94e4-4719-833d-e5b7f968adcd",
   "metadata": {},
   "source": [
    "### Load DescEmb - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01292417-5a9f-428d-bd89-8ad5203ede31",
   "metadata": {},
   "outputs": [],
   "source": [
    "desc_emb_mort_full_model, _ = load_model_from_file('desc_emb', 'mort', is_dev=False) \n",
    "print(desc_emb_mort_full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23eef3c6-ae35-4dd0-90e2-7623e269ccd6",
   "metadata": {},
   "source": [
    "### Eval DescEmb - Mort - Full - eICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22af595b-c2d7-4095-b2b6-a3f82a39aabe",
   "metadata": {},
   "outputs": [],
   "source": [
    "demb_mort_eicu_results = eval_model(desc_emb_mort_full_model, eicu_mort_demb_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f18fc8f-82e6-4a9f-9ced-476f9615fe64",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotAucRecallResults(None, demb_mort_eicu_results[-1],\n",
    "                     emb_type='Demb', task_type='Mortality eICU')\n",
    "PrintFinalAccuracy(demb_mort_eicu_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613465d1-bb3f-40f1-ba47-6024e0afae74",
   "metadata": {},
   "source": [
    "### Load DescEmbFt - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe91252-6d32-4d65-afce-7512432c744a",
   "metadata": {},
   "outputs": [],
   "source": [
    "demb_ft_mort_full_model, _ = load_model_from_file('desc_emb_ft', 'mort', is_dev=False) \n",
    "print(demb_ft_mort_full_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20bcb6c-1824-46e3-9838-847726dfc631",
   "metadata": {},
   "source": [
    "### Eval DescEmbFt - Mort - Full - eICU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d58e3a3-9755-4ba4-8966-c50d0490c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_ft_xform = BertFineTuneTransform()\n",
    "BERT_FT_EMBEDDING_SIZE = bert_ft_xform.emb_size\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "CODE2IDX = {}\n",
    "CODE_COUNT = {}\n",
    "task_fn = functools.partial(eicu_mortality_pred_task_demb, CODE_COUNT)\n",
    "eicu_mortality_dembft_dataset = eicubase.set_task(task_fn, task_name=eicu_mortality_pred_task_demb.__name__)\n",
    "CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(CODE_COUNT.keys()))\n",
    "}\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "eicu_mortality_dembft_dataset = TextEmbedDataset(eicu_mortality_dembft_dataset,\n",
    "                                                 transform=bert_ft_xform,\n",
    "                                                 should_cache=False)\n",
    "\n",
    "indices = range(0, min(50000, len(eicu_mortality_dembft_dataset)) )\n",
    "eicu_mortality_dembft_dataset = torch.utils.data.Subset(eicu_mortality_dembft_dataset, indices)\n",
    "print(len(eicu_mortality_dembft_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcc6750-2d27-42db-b1ae-605a89f20b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eicu_mort_dembft_loader = DataLoader(\n",
    "#     torch.utils.data.Subset(eicu_mortality_dembft_dataset, indices),\n",
    "#     batch_size=BATCH_SIZE_,\n",
    "#     shuffle=SHUFFLE_,\n",
    "#     collate_fn=bert_fine_tune_collate,\n",
    "# )\n",
    "\n",
    "# loader_iter = iter(eicu_mort_dembft_loader)\n",
    "# # for _ in loader_iter:\n",
    "# #     pass\n",
    "# try:\n",
    "#     x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "# except StopIteration as e:\n",
    "#     print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6afa1ac7-1113-4d91-a506-0627fc59354d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dembft_mort_eicu_results = eval_model(demb_ft_mort_full_model, eicu_mort_dembft_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecbe2495-1129-459e-907b-255fa99a1987",
   "metadata": {},
   "outputs": [],
   "source": [
    "PlotAucRecallResults(None, dembft_mort_eicu_results[-1],\n",
    "                     emb_type='Dembft', task_type='Mortality eICU')\n",
    "PrintFinalAccuracy(dembft_mort_eicu_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e659127f-edcb-4571-9a9e-d840ffc4e8e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a43bfd54-d7a1-4897-becf-30de09731a0f",
   "metadata": {},
   "source": [
    "# Condensed Training using Trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3589cf-b6f0-49ee-86aa-fcf60855c4a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DescEmb - Mort - Full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32891b1-b681-47a1-89f5-5f2460c8f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "# MORT_DEMB_CODE2IDX 1625\n",
    "# MORT_CEMB_CODE2IDX 1456\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.db_name = 'eicu'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.eval_only = True\n",
    "args.embed_model_type = 'desc_emb'\n",
    "args.predict_model_type = 'desc_emb'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_per_patient_collate_function_new_trainer \n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35d036e7-b046-4994-9bda-ac63e43910bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "desc_emb_mort_full_model, _ = load_model_from_file('desc_emb', 'mort', is_dev=False) \n",
    "trainer.model = desc_emb_mort_full_model\n",
    "eicu_demb_mort_full_results = trainer.train()\n",
    "del trainer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcd104-2059-4958-8f05-4dc53d3b14f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch.\n",
    "test_set_results = eicu_demb_mort_full_results[1]\n",
    "PrintTrainTime(eicu_demb_mort_full_results[2])\n",
    "PlotAucRecallResults(None, test_set_results[-1],\n",
    "                     emb_type='Demb eICU', task_type='Mortality')\n",
    "PrintFinalAccuracy(test_set_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67abca69-e308-4452-868a-5072f7c3f58e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### DescEmbFt - Mort - Full "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa9425d-e4a9-4ec9-8ba6-0a1b9c02a157",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import trainlib\n",
    "import models\n",
    "import datasets\n",
    "importlib.reload(trainlib)\n",
    "importlib.reload(models)\n",
    "importlib.reload(datasets)\n",
    "from trainlib import Trainer\n",
    "from models import ModelA, EHRModel, CembRNN, DembRNN\n",
    "from datasets import SimpleDataset, DatasetCacher\n",
    "\n",
    "args = DotArgs()\n",
    "args.save_dir = os.path.join(os.getcwd(), 'modelcache')\n",
    "if not os.path.exists(args.save_dir):\n",
    "    os.mkdir(args.save_dir)\n",
    "args.save_prefix = 'checkpoints'\n",
    "args.random_seed = 90210\n",
    "args.task = 'mort'\n",
    "args.db_name = 'eicu'\n",
    "args.is_dev = False  # if dev mode on we use a small subset of the full dataset.\n",
    "assert(args.task in task_types)\n",
    "\n",
    "# Model Args\n",
    "# Load either a code_emb embed model or a desc_emb embed model.\n",
    "args.eval_only = True\n",
    "args.embed_model_type = 'desc_emb_ft'\n",
    "args.predict_model_type = 'desc_emb_ft'\n",
    "args.override_batch_size = BATCH_SIZE_\n",
    "args.collate_fn = bert_fine_tune_collate\n",
    "args.no_use_cached_dataset = eicu_mortality_dembft_dataset\n",
    "args.embed_index_size = 0 # unused mort_demb_metadata['extra_data']['embed_index_size']=768\n",
    "args.pred_embed_size = 0  # unused \n",
    "\n",
    "\n",
    "# Training Args\n",
    "args.load_pretrained_weights = False\n",
    "args.n_epochs = 10\n",
    "args.learning_rate = 1.0e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19ba740-6b92-412d-a6cb-54f99c307cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "gc.collect()\n",
    "del trainer\n",
    "gc.collect()\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b468fd86-e5bf-478a-8262-872e40f25106",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(args)\n",
    "demb_ft_mort_full_model, _ = load_model_from_file('desc_emb_ft', 'mort', is_dev=False) \n",
    "trainer.model = demb_ft_mort_full_model\n",
    "eicu_dembft_mort_full_results = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fef049e-cbbc-44e7-b6d7-b09945e5bed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One entry per epoch.\n",
    "test_set_results = eicu_dembft_mort_full_results[1]\n",
    "PrintTrainTime(eicu_dembft_mort_full_results[2])\n",
    "PlotAucRecallResults(None, test_set_results[-1],\n",
    "                     emb_type='Dembft eICU', task_type='Mortality')\n",
    "PrintFinalAccuracy(test_set_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9d9c13-7153-4e53-9736-39ca70ca3049",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

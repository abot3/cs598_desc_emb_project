{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ccba6af-1867-491d-b9d0-d99965738c64",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da368cd-3045-4ec0-86fb-2ce15b5d1b92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# General includes.\n",
    "import os\n",
    "import errno\n",
    "import gc\n",
    "import random\n",
    "import threading\n",
    "import math\n",
    "import itertools\n",
    "import functools\n",
    "from copy import deepcopy\n",
    "import logging\n",
    "import pickle\n",
    "\n",
    "#from termcolor import colored, cprint\n",
    "import colored\n",
    "from datetime import datetime, timedelta\n",
    "from pandarallel import pandarallel\n",
    "pandarallel.initialize(progress_bar=True)\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Typing includes.\n",
    "from typing import Dict, List, Optional, Any, Tuple\n",
    "\n",
    "# Numerical includes.\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "\n",
    "# pyHealth includes.\n",
    "from pyhealth.datasets import BaseDataset, MIMIC3Dataset, SampleDataset , split_by_patient\n",
    "from pyhealth.datasets.utils import MODULE_CACHE_PATH\n",
    "from pyhealth.data import Patient, Visit, Event\n",
    "from pyhealth.data import Event, Visit, Patient\n",
    "from pyhealth.datasets.utils import strptime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e714b1-ca5d-4d7b-9ace-6b4372adfd9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model imports \n",
    "from transformers import BertTokenizer, BertModel, BertForMaskedLM, BertConfig, BertTokenizerFast\n",
    "from transformers import TensorType\n",
    "from transformers import AutoConfig, AutoModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf83fa50-9dd0-467b-bd36-634795e4a09c",
   "metadata": {},
   "source": [
    "# Globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebef3528-0396-459a-8112-b272089f5d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG)\n",
    "logger = logging.getLogger(__name__)\n",
    "USE_GPU_ = False\n",
    "BERT_USE_GPU_ = True  # BERT embeddings \n",
    "DEV_ = True  # Uses a small subset of MIMIC data: https://pyhealth.readthedocs.io/en/latest/api/datasets/pyhealth.datasets.MIMIC3Dataset.html#pyhealth.datasets.MIMIC3Dataset\n",
    "GPU_STR_ = 'cuda'\n",
    "# DATA_DIR_ = os.path.join(os.getcwd(), DATA_DIR_)\n",
    "DATA_DIR_ = '~/sw/physionet.org/files/mimiciii/1.4'\n",
    "BATCH_SIZE_ = 32\n",
    "EMBEDDING_DIM_ = 264  # BERT requires a multiple of 12\n",
    "SHUFFLE_ = True\n",
    "SAMPLE_MULTIPLIER_ = 1\n",
    "\n",
    "# TBD cache for BERT embeddings.\n",
    "CACHE_DIR_ = 'cache'\n",
    "\n",
    "# Set seed for reproducibility.\n",
    "seed = 90210\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "def load_pickle(filename):\n",
    "    with open(filename, \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "\n",
    "def save_pickle(data, filename):\n",
    "    with open(filename, \"wb\") as f:\n",
    "        pickle.dump(data, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df0f8271-44b2-44a2-b7cc-cd7339a70e87",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0d5fa9-226e-4612-b28f-ed24de16399a",
   "metadata": {},
   "source": [
    "### Load MIMIC III Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607ef1d3-caa8-4db8-b664-86172bea936d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhealth.medcode import InnerMap, ICD9CM\n",
    "\n",
    "icd9cm = InnerMap.load(\"ICD9CM\")\n",
    "icd9cm.lookup(\"428.0\") # get detailed info\n",
    "icd9cm.get_ancestors(\"428.0\") # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"78951\")) # get detailed info\n",
    "print(f'78951 ancestors {icd9cm.get_ancestors(\"78951\")}') # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"7895\")) # get detailed info\n",
    "print(f'7895 ancestors {icd9cm.get_ancestors(\"7895\")}') # get parents\n",
    "\n",
    "\n",
    "print(icd9cm.lookup(\"7894\")) # get detailed info\n",
    "print(f'7894 ancestors {icd9cm.get_ancestors(\"7894\")}') # get parents\n",
    "\n",
    "print(icd9cm.lookup(\"78941\")) # get detailed info\n",
    "print(f'78941 ancestors {icd9cm.get_ancestors(\"78941\")}') # get parents\n",
    "\n",
    "print(ICD9CM.standardize('78951'))\n",
    "print(ICD9CM.standardize('7895'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfb3443-8dfa-4149-904b-04fc24d7a33e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _compute_duration_minutes(start_datetime: str, end_datetime: str) -> float:\n",
    "    '''Return duration in minutes as a float.\n",
    "    '''\n",
    "    # MIMIC-III uses the following format: 2146-07-22 00:00:00\n",
    "    start = datetime.strptime(start_datetime, '%Y-%m-%d %H:%M:%S')\n",
    "    end = datetime.strptime(end_datetime,   '%Y-%m-%d %H:%M:%S')\n",
    "    return float((end - start).seconds)\n",
    "\n",
    "class MIMIC3DatasetWrapper(MIMIC3Dataset):\n",
    "    ''' Add extra tables to the MIMIC III dataset.\n",
    "    \n",
    "      Some of the tables we need like \"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\"\n",
    "      are not supported out of the box. \n",
    "      \n",
    "      This class defines parsing methods to extract text data from these extra tables.\n",
    "      The text data is generally joined on the PATIENTID, HADMID, ITEMID to match the\n",
    "      pyHealth Vists class representation.\n",
    "    '''\n",
    "   \n",
    "    # We need to add storage for text-based lookup tables here.\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self._valid_text_tables = [\"D_ICD_DIAGNOSES\", \"D_ITEMS\", \"D_ICD_PROCEDURES\", \"D_LABITEMS\"]\n",
    "        self._text_descriptions = {x: {} for x in self._valid_text_tables}\n",
    "        self._text_luts = {x: {} for x in self._valid_text_tables}\n",
    "        self.refresh_cache = False\n",
    "        # The pyHealth dataset cache doesn't know about this class's private members.\n",
    "        if 'refresh_cache' in kwargs:\n",
    "            self.refresh_cache = kwargs['refresh_cache']\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self._do_cache()\n",
    "    \n",
    "    def _do_cache(self):\n",
    "        '''The pyHealth dataset cache doesn't know about this classes private members.\n",
    "        \n",
    "          We need to wrap the caching function so it's aware of the additional luts\n",
    "          to be saved/restored. The superclass is still responsible for saving/restoring\n",
    "          `self.patients` from the DB.\n",
    "        '''\n",
    "        self.extra_filepath = ''.join([\n",
    "            os.path.splitext(self.filepath)[0],\n",
    "            '_dev' if self.dev else '',\n",
    "            '_extras.pkl',\n",
    "        ])\n",
    "        \n",
    "        # check if cache exists or refresh_cache is True\n",
    "        if os.path.exists(self.extra_filepath) and (not self.refresh_cache):\n",
    "            # load from cache\n",
    "            logger.info(\n",
    "                f\"Loaded {self.dataset_name} base dataset from {self.extra_filepath}\"\n",
    "            )\n",
    "            from_pickle = load_pickle(self.extra_filepath)\n",
    "            self._valid_text_tables = from_pickle['_valid_text_tables']\n",
    "            self._text_descriptions = from_pickle['_text_descriptions']\n",
    "            self._text_luts = from_pickle['_text_luts']\n",
    "        else:\n",
    "            # load from raw data\n",
    "            logger.info(f\"Processing {self.dataset_name} base dataset...\")\n",
    "            to_cache = {\n",
    "                '_valid_text_tables': self._valid_text_tables,\n",
    "                '_text_descriptions': self._text_descriptions,\n",
    "                '_text_luts': self._text_luts,\n",
    "            }\n",
    "            logger.info(f\"Saved {self.dataset_name} base dataset to {self.extra_filepath}\")\n",
    "            save_pickle(to_cache, self.extra_filepath)\n",
    "    \n",
    "    def get_all_tables(self) -> List[str]: \n",
    "        return list(self._text_descriptions.keys())\n",
    "        \n",
    "    def get_text_dict(self, table_name: str) -> Dict[str, Dict[Any, Any]]:\n",
    "        return self._text_descriptions.get(table_name)\n",
    "    \n",
    "    def set_text_lut(self, table_name: str, lut: Dict[Any, Any]) -> None:\n",
    "        self._text_luts[table_name] = lut\n",
    "    \n",
    "    def get_text_lut(self, table_name: str) -> Dict[Any, Any]:\n",
    "        return self._text_luts[table_name]\n",
    "    \n",
    "    def _add_events_to_patient_dict(\n",
    "        self,\n",
    "        patient_dict: Dict[str, Patient],\n",
    "        group_df: pd.DataFrame,\n",
    "    ) -> Dict[str, Patient]:\n",
    "        #TODO(botelho3) Imported from PyHealth Base dataset githubf to\n",
    "        #support parse_prescription\n",
    "        \"\"\"Helper function which adds the events column of a df.groupby object to the patient dict.\n",
    "        \n",
    "        Will be called at the end of each `self.parse_[table_name]()` function.\n",
    "        Args:\n",
    "            patient_dict: a dict mapping patient_id to `Patient` object.\n",
    "            group_df: a df.groupby object, having two columns: patient_id and events.\n",
    "                - the patient_id column is the index of the patient\n",
    "                - the events column is a list of <Event> objects\n",
    "        Returns:\n",
    "            The updated patient dict.\n",
    "        \"\"\"\n",
    "        for _, events in group_df.items():\n",
    "            for event in events:\n",
    "                patient_dict = self._add_event_to_patient_dict(patient_dict, event)\n",
    "        return patient_dict\n",
    "\n",
    "    \n",
    "    def parse_prescriptions(self, patients: Dict[str, Patient]) -> Dict[str, Patient]:\n",
    "        \"\"\"Helper function which parses PRESCRIPTIONS table.\n",
    "        \n",
    "        TODO(botelho3) - we have to override this to include the text fields. The\n",
    "        prescriptions table does not link to a separate D_ICD_* table in MIMIC-III\n",
    "        thtat contains text descriptions of the prescription. The text descriptions\n",
    "        are in the columns of this table. Regular pyHealth ignores these columns. We\n",
    "        override this method to appent pyHealth Event objects containing the text\n",
    "        columns to each patient.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - PRESCRIPTIONS: https://mimic.mit.edu/docs/iii/tables/prescriptions/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The updated patients dict.\n",
    "        \"\"\"\n",
    "        table = \"PRESCRIPTIONS\"\n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            low_memory=False,\n",
    "            dtype={\"SUBJECT_ID\": str, \"HADM_ID\": str, \"NDC\": str,\n",
    "                   \"DRUG_TYPE\": str, \"DRUG\": str,\n",
    "                   \"PROD_STRENGTH\": str, \"ROUTE\": str, \"ENDDATE\": str},\n",
    "        )\n",
    "        # drop records of the other patients\n",
    "        df = df[df[\"SUBJECT_ID\"].isin(patients.keys())]\n",
    "        df = df.dropna()\n",
    "        # sort by start date and end date\n",
    "        df = df.sort_values(\n",
    "            [\"SUBJECT_ID\", \"HADM_ID\", \"STARTDATE\", \"ENDDATE\"], ascending=True\n",
    "        )\n",
    "        # group by patient and visit\n",
    "        group_df = df.groupby(\"SUBJECT_ID\")\n",
    "        \n",
    "        # parallel unit for prescription (per patient)\n",
    "        def prescription_unit(p_id, p_info):\n",
    "            events = []\n",
    "            for v_id, v_info in p_info.groupby(\"HADM_ID\"):\n",
    "                zipped = zip(v_info[\"STARTDATE\"], v_info[\"NDC\"], v_info[\"DRUG_TYPE\"],\n",
    "                             v_info[\"DRUG\"], v_info[\"PROD_STRENGTH\"], v_info[\"ROUTE\"],\n",
    "                             v_info[\"ENDDATE\"])\n",
    "                for startdate, code, dtype, dname, dose, route, enddate in zipped:\n",
    "                    if not type(startdate) == str:\n",
    "                        startdate = '2142-07-18 00:00:00'\n",
    "                    if not type(enddate) == str:\n",
    "                        enddate = '2142-07-18 00:00:00'\n",
    "                    assert(type(dname) is str)\n",
    "                    # if not type(enddate) is str:\n",
    "                    #     print(f'Not matching enddate {enddate} startdate {startdate}')\n",
    "                    #     print(f'dname {dname}, hadm_id {v_id}, p_id {p_id}')\n",
    "                    assert(type(startdate) is str)\n",
    "                    assert(type(enddate) is str)\n",
    "                    event = Event(\n",
    "                        code=code,\n",
    "                        table=table,\n",
    "                        vocabulary=\"NDC\",\n",
    "                        visit_id=v_id,\n",
    "                        patient_id=p_id,\n",
    "                        timestamp=strptime(startdate),\n",
    "                        dtype=dtype,\n",
    "                        dname=dname,\n",
    "                        dose=dose,\n",
    "                        route=route,\n",
    "                        duration=_compute_duration_minutes(startdate, enddate),\n",
    "                    )\n",
    "                    events.append(event)\n",
    "            return events\n",
    "\n",
    "                # parallel apply\n",
    "        group_df = group_df.parallel_apply(\n",
    "            lambda x: prescription_unit(x.SUBJECT_ID.unique()[0], x)\n",
    "        )\n",
    "\n",
    "        patients = self._add_events_to_patient_dict(patients, group_df)\n",
    "        return patients\n",
    "    \n",
    "    # Note the name has to match the table name exactly.\n",
    "    # See https://github.com/sunlabuiuc/PyHealth/blob/master/pyhealth/datasets/mimic3.py#L71.\n",
    "    def parse_d_icd_diagnoses(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_DIAGNOSIS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_DIAGNOSIS: https://mimic.mit.edu/docs/iii/tables/d_icd_diagnoses/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_DIAGNOSES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text diagnosis description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_DIAGNOSES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    def parse_d_labitems(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_LABITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_LABITEMS: https://mimic.mit.edu/docs/iii/tables/d_labitems/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_LABITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text lab measurement description lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_LABITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str, \"FLUID\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\", \"FLUID\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_items(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        # TODO(botelho3) - Note this may not be totally useable because the ITEMID\n",
    "        # uinqiue key only links to these tables using ITEMID\n",
    "        #   - INPUTEVENTS_MV \n",
    "        #   - OUTPUTEVENTS on ITEMID\n",
    "        #   - PROCEDUREEVENTS_MV on ITEMID\n",
    "        # \n",
    "        # Not to the tables we want e.g. \n",
    "        \"\"\"Helper function which parses D_ITEMS table.\n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ITEMS: https://mimic.mit.edu/docs/iii/tables/d_items/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ITEMS.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text inputs/output/procedure events lookup\n",
    "            for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ITEMS\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ITEMID\", \"LABEL\", \"CATEGORY\"],\n",
    "            dtype={\"ITEMID\": str, \"LABEL\": str, \"CATEGORY\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ITEMID\", \"LABEL\", \"CATEGORY\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ITEMID\"], ascending=True)\n",
    "       \n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    \n",
    "    \n",
    "    def parse_d_icd_procedures(self, patients: Dict[str, Patient]) -> Dict[str, Patient]: \n",
    "        \"\"\"Helper function which parses D_ICD_PROCEDURES table.\n",
    "        \n",
    "        Will be called in `self.parse_tables()`\n",
    "        Docs:\n",
    "            - D_ICD_PROCEDURES: https://mimic.mit.edu/docs/iii/tables/d_icd_procedures/\n",
    "        Args:\n",
    "            patients: a dict of `Patient` objects indexed by patient_id.\n",
    "        Returns:\n",
    "            The unchanged patients dict.\n",
    "        Note:\n",
    "            This function doesn't up date the patients dict like other part_*() functions.\n",
    "            Here we read the D_ICD_PROCEDURES.csv file containing ICD9_code -> text mappings\n",
    "            and store them in a dict `self._text_descriptions[table]`.\n",
    "            \n",
    "            The dict is used as a ICD9_code -> text procedure description lookup for DescEmb.\n",
    "        \"\"\"\n",
    "        table = \"D_ICD_PROCEDURES\"\n",
    "        print(f\"Parsing {table}\")\n",
    "        assert(table in self._valid_text_tables)\n",
    "        \n",
    "        # read table\n",
    "        df = pd.read_csv(\n",
    "            os.path.join(self.root, f\"{table}.csv\"),\n",
    "            usecols=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"],\n",
    "            dtype={\"ICD9_CODE\": str, \"SHORT_TITLE\": str, \"LONG_TITLE\": str}\n",
    "        )\n",
    "        \n",
    "        # drop rows with missing values\n",
    "        df = df.dropna(subset=[\"ICD9_CODE\", \"SHORT_TITLE\", \"LONG_TITLE\"])\n",
    "        # sort by sequence number (i.e., priority)\n",
    "        df = df.sort_values([\"ICD9_CODE\"], ascending=True)\n",
    "       \n",
    "        # print(df.head())\n",
    "        self._text_descriptions[table] = df.reset_index(drop=True).to_dict(orient='split')\n",
    "        \n",
    "        # We haven't altered the patients array, just return it.\n",
    "        return patients\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "427a42a6-441e-438c-96f4-b38ba82fd192",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Reading data from: `{DATA_DIR_}`')\n",
    "\n",
    "mimic3base = MIMIC3DatasetWrapper(\n",
    "    # root=\"https://storage.googleapis.com/pyhealth/Synthetic_MIMIC-III/\",\n",
    "    root=DATA_DIR_,\n",
    "    dataset_name='mimic_3_dataset',\n",
    "    tables=[\"D_ICD_DIAGNOSES\", \"D_ICD_PROCEDURES\", \"D_ITEMS\", \"D_LABITEMS\",\n",
    "            \"DIAGNOSES_ICD\", \"PROCEDURES_ICD\", \"PRESCRIPTIONS\"], # \"LABEVENTS\"],\n",
    "    # map all NDC codes to ATC 3-rd level codes in these tables\n",
    "    # See https://en.wikipedia.org/wiki/Anatomical_Therapeutic_Chemical_Classification_System.\n",
    "    code_mapping={\"NDC\": (\"ATC\", {\"target_kwargs\": {\"level\": 3}})},\n",
    "    # Reads a subset of the data. Disable for full training run.\n",
    "    dev = DEV_,\n",
    "    # Slow, rebuilds the dataset instead of caching.\n",
    "    refresh_cache=False,\n",
    ")\n",
    "\n",
    "mimic3base.stat()\n",
    "mimic3base.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cef4476-6bc8-4791-b246-4c329a80a2e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_names = mimic3base.get_all_tables()\n",
    "print(table_names)\n",
    "\n",
    "print('\\033[92m' '====Tables====\\n' '\\033[0m')\n",
    "# print(colored('====Tables====\\n', 'green'))\n",
    "# print(colored.fg('green') + '====Tables====\\n')\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_dict(t)\n",
    "    print(f\"Table: {t}\")\n",
    "    print(d['data'][:5])\n",
    "    print('\\n\\n')\n",
    "\n",
    "# Take the cached tables from the parse_tables function and build the {ICD9 -> (short_name, long_name)}\n",
    "# lookup tables.\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_dict(t)\n",
    "    d = d['data']\n",
    "    lut = {record[0]: record[1:] for record in d}\n",
    "    mimic3base.set_text_lut(t,  lut)\n",
    "    \n",
    "print('\\033[92m' '====Luts====\\n' '\\033[0m')\n",
    "# print(f'{colored.fg(\"green\")} ====Luts====\\n')\n",
    "for t in table_names:\n",
    "    d = mimic3base.get_text_lut(t)\n",
    "    print(f\"Lut {t}:\\n{dict(itertools.islice(d.items(), 2))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b8e28-23ac-4ac1-a9b6-ae546e28c905",
   "metadata": {},
   "source": [
    "### Tasks\n",
    "\n",
    "Declare tasks for 2 of the 5 prediction tasks specified in the paper. We will create dataloaders for each task that contain the ICD codes and the raw text for each (patient, visit)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a69f4f2-566a-428f-800a-8539fbb16a15",
   "metadata": {},
   "source": [
    "#### CodeEMB Pred tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08692aad-db67-487b-9b26-dbde0ab6adce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "def readmission_pred_task(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_task(MORTALITY_PER_VISIT_ICD_9_CODE_COUNT_, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient) - 1):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit = patient[i]\n",
    "        next_visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        if next_visit.discharge_status not in [0, 1]:\n",
    "            mortality_label = 0\n",
    "        else:\n",
    "            mortality_label = int(next_visit.discharge_status)\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": mortality_label,\n",
    "            }\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "def mortality_pred_task_cemb(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    each sample is a list of vists for 1 patient.\n",
    "    \"\"\"\n",
    "   \n",
    "    # TODO(botelho3) - stupid hack around the limitations of SampleDataset validator.\n",
    "    kMaxVisits = 5\n",
    "    if len(patient) < 1 or len(patient) > kMaxVisits:\n",
    "        return []\n",
    "        \n",
    "    samples = [{\n",
    "        \"visit_id\": patient[0].visit_id,\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"num_visits\": 0,\n",
    "        \"conditions\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures\": [[] for v in range(kMaxVisits)],\n",
    "        \"conditions_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"procedures_text\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs\": [[] for v in range(kMaxVisits)],\n",
    "        \"drugs_text\": [[] for v in range(kMaxVisits)],\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": 0,\n",
    "    }]\n",
    "    \n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "\n",
    "    # loop over all visits\n",
    "    out_idx = 0\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(diag_lut)\n",
    "        # if i == 0: print(proc_lut)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(diag_lut.get(cond)) for cond in conditions])\n",
    "        # print(proc_lut.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], str(d['dose']), d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        # Each field is a list of visits.\n",
    "        samples[0]['num_visits'] = samples[0]['num_visits'] + 1\n",
    "        samples[0]['conditions'][out_idx]=conditions\n",
    "        samples[0]['procedures'][out_idx]=procedures\n",
    "        samples[0]['conditions_text'][out_idx]= conditions_text\n",
    "        samples[0]['procedures_text'][out_idx] = procedures_text\n",
    "        samples[0]['drugs'][out_idx] = drugs\n",
    "        samples[0]['drugs_text'][out_idx] = drugs_text\n",
    "        out_idx = out_idx + 1\n",
    "    samples[0]['label'] = global_mortality_label\n",
    "   \n",
    "    # Record all unique codes and their frequency for LUT.\n",
    "    for visit in samples[0]['conditions']:\n",
    "        for code in visit:\n",
    "            CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "           \n",
    "    # If none of the samples met the criteria return an empty list.\n",
    "    if samples[0]['num_visits'] == 0:\n",
    "        return []\n",
    "\n",
    "    # Potentially multiply the sample n-times to increase dataset size.\n",
    "    samples.extend(\n",
    "        list(deepcopy(samples[0]) for s in range(SAMPLE_MULTIPLIER_-1))\n",
    "    )\n",
    "        \n",
    "    return samples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d30d309-9b95-4a1b-82ee-ae141960c868",
   "metadata": {},
   "source": [
    "#### DescEmb Pred Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "634ef5a4-fa6d-4720-a4e7-e91cc3746ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "#{code: idx for idx, code in enumerate(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys())}\n",
    "def readmission_pred_task_per_patient(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "   \n",
    "    # Length 1 patients by defn are not readmitted.\n",
    "    if len(patient) < 1:\n",
    "        return samples\n",
    "\n",
    "    # we will drop the last visit\n",
    "    global_readmission_label = 0\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        global_readmission_label |= readmission_label  \n",
    "       \n",
    "    for i in range(len(patient)):\n",
    "        visit: Visit = patient[i]\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs = [x.code for x in visit.get_event_list(table=\"PRESCRIPTIONS\")]\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_readmission_label,\n",
    "            }\n",
    "        )\n",
    "        \n",
    "\n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return samples\n",
    "    \n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data)) - 1\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size \n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs, drugs_pad = pad_field(\"drugs\", visits, '0')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    assert(drugs_pad == drugs_text_pad)\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs\": drugs,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_pad\": drugs_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_readmission_label,\n",
    "    }\n",
    "    # sample = {\n",
    "    #     \"patient_id\": patient.patient_id,\n",
    "    #     # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "    #     \"visit_id\": visits[0][\"visit_id\"],\n",
    "    #     \"num_visits\": num_visits,\n",
    "    #     # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "    #     \"conditions\": [[x[\"conditions\"] for x in visits]],\n",
    "    #     \"procedures\": [[x[\"procedures\"] for x in visits]],\n",
    "    #     \"conditions_text\": [[x[\"conditions_text\"] for x in visits]],\n",
    "    #     \"procedures_text\": [[x[\"procedures_text\"] for x in visits]],\n",
    "    #     \"drugs\": [[[x[\"drugs\"] for x in visits]]],\n",
    "    #     \"drugs_text\": [[x[\"drugs_text\"] for x in visits]],\n",
    "    #     # \"labevents\": labevents,\n",
    "    #     # \"labevents_text\": labevents_text\n",
    "    #     \"label\": global_readmission_label,\n",
    "    # }\n",
    "    for code in sample['conditions']:\n",
    "        READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_[code] = READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.get(code, 0) + 1\n",
    "\n",
    "    samples.extend(\n",
    "        list(deepcopy(sample) for s in range(SAMPLE_MULTIPLIER_))\n",
    "    )\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_task_per_patient(CODE_COUNT, patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    visits = []\n",
    "    kMaxListSize = 40\n",
    "\n",
    "    global_mortality_label = 0\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient)):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit: Visit = patient[i]\n",
    "        # next_visit: Visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        # if next_visit.discharge_status not in [0, 1]:\n",
    "        #     mortality_label = 0\n",
    "        # else:\n",
    "        #     mortality_label = int(next_visit.discharge_status)\n",
    "        mortality_label = int(visit.discharge_status)\n",
    "        global_mortality_label |= mortality_label\n",
    "        \n",
    "    # loop over all visits but the last one\n",
    "    for i, visit in enumerate(patient):\n",
    "        # visit: Visit.\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        # drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs = [x.code for x in visit.get_event_list(table=\"PRESCRIPTIONS\")]\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        if len(conditions) + len(procedures) + len(drugs_full) < 5:\n",
    "            # Exclude stays with less than 5 procedures.\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        diag_lut = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        proc_lut = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        conditions_text = [diag_lut.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [proc_lut.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples into a pyHealth Visit.\n",
    "        visits.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": global_mortality_label,\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    # Return empty list, didn't meet exclusion criteria.\n",
    "    num_visits = len(visits)\n",
    "    if num_visits < 1:\n",
    "        return [] \n",
    "   \n",
    "    # pyHealth requires that all list fields in sample are equal size.\n",
    "    def pad_field(field, visits, empty_val: Any):\n",
    "        l = [empty_val for x in range(kMaxListSize)]\n",
    "        data = [x[field] for x in visits]\n",
    "        data = list(itertools.chain.from_iterable(data))\n",
    "        slice_size = min(kMaxListSize, len(data))\n",
    "        l[:slice_size] = data[:slice_size]\n",
    "        return l, slice_size\n",
    "    \n",
    "    conditions, conditions_pad = pad_field(\"conditions\", visits, '0')\n",
    "    conditions_text, conditions_text_pad = pad_field(\"conditions_text\", visits, '')\n",
    "    procedures, procedures_pad = pad_field(\"procedures\", visits, '0')\n",
    "    procedures_text, procedures_text_pad = pad_field(\"procedures_text\", visits, '')\n",
    "    drugs, drugs_pad = pad_field(\"drugs\", visits, '0')\n",
    "    drugs_text, drugs_text_pad = pad_field(\"drugs_text\", visits, '')\n",
    "    assert(drugs_pad == drugs_text_pad)\n",
    "    sample = {\n",
    "        \"patient_id\": patient.patient_id,\n",
    "        # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "        \"visit_id\": visits[0][\"visit_id\"],\n",
    "        \"num_visits\": num_visits,\n",
    "        # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "        \"conditions\": conditions,\n",
    "        \"conditions_text\": conditions_text,\n",
    "        \"procedures\": procedures,\n",
    "        \"procedures_text\": procedures_text,\n",
    "        \"drugs\": drugs,\n",
    "        \"drugs_text\": drugs_text,\n",
    "        \n",
    "        \"conditions_pad\": conditions_pad,\n",
    "        \"procedures_pad\": procedures_pad,\n",
    "        \"conditions_text_pad\": conditions_text_pad,\n",
    "        \"procedures_text_pad\": procedures_text_pad,\n",
    "        \"drugs_pad\": drugs_pad,\n",
    "        \"drugs_text_pad\": drugs_text_pad,\n",
    "        # \"labevents\": labevents,\n",
    "        # \"labevents_text\": labevents_text\n",
    "        \"label\": global_mortality_label,\n",
    "    }\n",
    "    # sample = {\n",
    "    #     \"patient_id\": patient.patient_id,\n",
    "    #     # TODO(botelho3) Why does pyhealth require a visit id in the keys if we're combining vists?\n",
    "    #     \"visit_id\": visits[0][\"visit_id\"],\n",
    "    #     \"num_visits\": num_visits,\n",
    "    #     # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "    #     \"conditions\": [[x[\"conditions\"] for x in visits]],\n",
    "    #     \"procedures\": [[x[\"procedures\"] for x in visits]],\n",
    "    #     \"conditions_text\": [[x[\"conditions_text\"] for x in visits]],\n",
    "    #     \"procedures_text\": [[x[\"procedures_text\"] for x in visits]],\n",
    "    #     \"drugs\": [[[x[\"drugs\"] for x in visits]]],\n",
    "    #     \"drugs_text\": [[x[\"drugs_text\"] for x in visits]],\n",
    "    #     # \"labevents\": labevents,\n",
    "    #     # \"labevents_text\": labevents_text\n",
    "    #     \"label\": global_mortality_label,\n",
    "    # }\n",
    "   \n",
    "   \n",
    "    # For every condition in the sample (all visits). Record frequency.\n",
    "    # Will be used to build code->index LUT.\n",
    "    for code in sample['conditions']:\n",
    "        CODE_COUNT[code] = CODE_COUNT.get(code, 0) + 1\n",
    "\n",
    "    if SAMPLE_MULTIPLIER_:\n",
    "        samples.extend(\n",
    "            list(deepcopy(sample) for s in range(SAMPLE_MULTIPLIER_))\n",
    "        )\n",
    "        \n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371b36c-571d-482e-86b2-bbfb92522a24",
   "metadata": {},
   "source": [
    "#### Test Load Readmission Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d047ff-6412-462c-9294-a5ba2d3bdba2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# set_task() returns a SampleEHRDataset object\n",
    "readm_dataset = mimic3base.set_task(readmission_pred_task_per_patient)\n",
    "READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(READMISSION_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "readm_dataset.stat()\n",
    "readm_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "print(f\"READMISSION_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(READMISSION_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "      f\"{READMISSION_PER_PATIENT_ICD_9_CODE2IDX_}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2738557-c9c4-4a33-9fd6-b2dd00bd1d22",
   "metadata": {},
   "source": [
    "#### Test Load Mortality Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70586b66-a814-4898-892b-d1bdd339ce7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_ = {}\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {}\n",
    "task_fn = functools.partial(mortality_pred_task_per_patient, MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_)\n",
    "mor_dataset = mimic3base.set_task(task_fn, task_name=mortality_pred_task_per_patient.__name__)\n",
    "MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORTALITY_PER_PATIENT_ICD_9_CODE_COUNT_.keys()))\n",
    "}\n",
    "mor_dataset.stat()\n",
    "mor_dataset.samples[1]\n",
    "# TODO(botelho3) could try a freq codes limit on this.\n",
    "print(f\"MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_ len: {len(MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_)}\\n\"\n",
    "      f\"{MORTALITY_PER_PATIENT_ICD_9_CODE2IDX_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67dcd2f-cb6f-4b21-b692-4526d6aff390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The original authors tackled 5 tasks\n",
    "#   1. readmission\n",
    "#   2. mortality\n",
    "#   3. an ICU stay exceeding three days\n",
    "#   4. an ICU stay exceeding seven days\n",
    "#   5. diagnosis prediction\n",
    "\n",
    "def readmission_pred_no_visit_task(patient, time_window=3):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # we will drop the last visit\n",
    "    for i in range(len(patient) - 1):\n",
    "        visit: Visit = patient[i]\n",
    "        next_visit: Visit = patient[i + 1]\n",
    "\n",
    "        # step 1: define the readmission label \n",
    "        # get time difference between current visit and next visit\n",
    "        time_diff = (next_visit.encounter_time - visit.encounter_time).days\n",
    "        readmission_label = 1 if time_diff < time_window else 0\n",
    "        \n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        \n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0:\n",
    "            continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "            \n",
    "        # step 4: assemble the samples\n",
    "        # TODO: should also exclude visit with age < 18\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": readmission_label,\n",
    "            }\n",
    "        )\n",
    "    # no cohort selection\n",
    "    return samples\n",
    "\n",
    "\n",
    "def mortality_pred_no_visit_task(patient):\n",
    "    \"\"\"\n",
    "    patient is a <pyhealth.data.Patient> object\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "\n",
    "    # loop over all visits but the last one\n",
    "    for i in range(len(patient) - 1):\n",
    "\n",
    "        # visit and next_visit are both <pyhealth.data.Visit> objects\n",
    "        # there are no vists.attr_dict keys\n",
    "        visit = patient[i]\n",
    "        next_visit = patient[i + 1]\n",
    "        # if i == 0: print(visit)\n",
    "\n",
    "        # step 1: define the mortality_label\n",
    "        if next_visit.discharge_status not in [0, 1]:\n",
    "            mortality_label = 0\n",
    "        else:\n",
    "            mortality_label = int(next_visit.discharge_status)\n",
    "\n",
    "        # step 2: get code-based feature information\n",
    "        conditions = visit.get_code_list(table=\"DIAGNOSES_ICD\")\n",
    "        procedures = visit.get_code_list(table=\"PROCEDURES_ICD\")\n",
    "        drugs = visit.get_code_list(table=\"PRESCRIPTIONS\")\n",
    "        drugs_full = visit.get_event_list(table=\"PRESCRIPTIONS\")\n",
    "        # if i == 0: print([d.attr_dict for d in drugs_full])\n",
    "        # if i == 0: print(conditions)\n",
    "        # if i == 0: print(procedures)\n",
    "        # if i == 0: print(drugs)\n",
    "        # TODO(botelho3) - add this datasource back in once we have full MIMIC-III dataset.\n",
    "        # labevents = visit.get_code_list(table=\"LABEVENTS\")\n",
    "\n",
    "        # step 3: exclusion criteria: visits without condition, procedure, or drug\n",
    "        if len(conditions) * len(procedures) == 0 * len(drugs_full) == 0: continue\n",
    "        \n",
    "        # step 3.5: build text lists from the ICD codes\n",
    "        d_diag = mimic3base.get_text_lut(\"D_ICD_DIAGNOSES\")\n",
    "        d_proc = mimic3base.get_text_lut(\"D_ICD_PROCEDURES\")\n",
    "        # if i == 0: print(d_diag)\n",
    "        # if i == 0: print(d_proc)\n",
    "        # Index 0 is shortname, index 1 is longname.\n",
    "        # print([str(cond) + ' ' + str(d_diag.get(cond)) for cond in conditions])\n",
    "        # print(d_proc.get(procedures[0]))\n",
    "        conditions_text = [d_diag.get(cond,(\"\", \"\"))[1] for cond in conditions]\n",
    "        procedures_text = [d_proc.get(proc,(\"\", \"\"))[1] for proc in procedures]\n",
    "        drugs_text = [' '.join([d['dname'], d['dtype'], d['dose'], d['route'], str(d['duration'])])\n",
    "                      for d in [d.attr_dict for d in drugs_full]]\n",
    "        # TODO(botelho3) - add the labevents data source back in once we have full MIMIC-III dataset.\n",
    "        # labevents_text =\n",
    "        \n",
    "        # step 4: assemble the samples\n",
    "        samples.append(\n",
    "            {\n",
    "                \"visit_id\": visit.visit_id,\n",
    "                \"patient_id\": patient.patient_id,\n",
    "                # the following keys can be the \"feature_keys\" or \"label_key\" for initializing downstream ML model\n",
    "                \"conditions\": conditions,\n",
    "                \"procedures\": procedures,\n",
    "                \"conditions_text\": conditions_text,\n",
    "                \"procedures_text\": procedures_text,\n",
    "                \"drugs\": drugs,\n",
    "                \"drugs_text\": drugs_text,\n",
    "                # \"labevents\": labevents,\n",
    "                # \"labevents_text\": labevents_text\n",
    "                \"label\": mortality_label,\n",
    "            }\n",
    "        )\n",
    "    return samples\n",
    "\n",
    "\n",
    "# mimic3sample = mimic3base.set_task(task_fn=drug_recommendation_mimic3_fn) # use default task\n",
    "# train_ds, val_ds, test_ds = split_by_patient(mimic3sample, [0.8, 0.1, 0.1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0281a-f95b-4991-abd5-5824fc37c520",
   "metadata": {},
   "source": [
    "### DataLoaders and Collate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a9be7a-3736-4bda-af61-5dcf331bb664",
   "metadata": {},
   "source": [
    "#### Bert Collate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924c8e3c-0482-4688-b2d6-d83d490060a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def per_visit_collate_fn(data):\n",
    "#     \"\"\"\n",
    "#     Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "#         sequences to the sample shape (max # visits, len(freq_codes)). The padding infomation\n",
    "#         is stored in `mask`.\n",
    "    \n",
    "#     Arguments:\n",
    "#         data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "#     Outputs:\n",
    "#         x: a tensor of shape (# patiens, max # visits, len(freq_codes)) of type torch.float\n",
    "#         masks: a tensor of shape (# patiens, max # visits) of type torch.bool\n",
    "#         y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "#     Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "#         using: `sequences, labels = zip(*data)`\n",
    "#     \"\"\"\n",
    "\n",
    "#     sequences, labels = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "#     num_patients = len(sequences)\n",
    "#     num_visits = [len(patient) for patient in sequences]\n",
    "#     max_num_visits = max(num_visits)\n",
    "    \n",
    "#     x = torch.zeros((num_patients, max_num_visits, len(freq_codes)), dtype=torch.float)    \n",
    "#     for i_patient, patient in enumerate(sequences):\n",
    "#         kMaxVisits = len(patient)\n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             l = len(visit)\n",
    "#             for code in visit:\n",
    "#                 \"\"\"\n",
    "#                 TODO: 1. check if code is in freq_codes;\n",
    "#                       2. obtain the code index using code2idx;\n",
    "#                       3. set the correspoindg element in x to 1.\n",
    "#                 \"\"\"\n",
    "#                 try:\n",
    "#                     idx = code2idx[code]\n",
    "#                     x[i_patient, j_visit, idx] = 1\n",
    "#                 except KeyError as e:\n",
    "#                     pass\n",
    "    \n",
    "#     masks = torch.sum(x, dim=-1) > 0\n",
    "    \n",
    "#     return x, masks, y\n",
    "\n",
    "# def collate_fn(data):\n",
    "#     \"\"\"\n",
    "#     TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "#         sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "#         is stored in `mask`.\n",
    "    \n",
    "#     Arguments:\n",
    "#         data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "#     Outputs:\n",
    "#         x: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.long\n",
    "#         masks: a tensor of shape (# patiens, max # visits, max # diagnosis codes) of type torch.bool\n",
    "#         rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "#         rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "#         y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "#     Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "#         using: `sequences, labels = zip(*data)`\n",
    "#     \"\"\"\n",
    "\n",
    "#     sequences, labels = zip(*data)\n",
    "\n",
    "#     y = torch.tensor(labels, dtype=torch.float)\n",
    "    \n",
    "#     num_patients = len(sequences)\n",
    "#     num_visits = [len(patient) for patient in sequences]\n",
    "#     num_codes = [len(visit) for patient in sequences for visit in patient]\n",
    "\n",
    "#     max_num_visits = max(num_visits)\n",
    "#     max_num_codes = max(num_codes)\n",
    "    \n",
    "#     x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "#     masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "#     for i_patient, patient in enumerate(sequences):\n",
    "#         kMaxVisits = len(patient)\n",
    "#         for j_visit, visit in enumerate(patient):\n",
    "#             \"\"\"\n",
    "#             TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "#             \"\"\"\n",
    "#             l = len(visit)\n",
    "#             x[i_patient, j_visit, 0:l] = torch.LongTensor(visit)\n",
    "#             rev_x[i_patient, kMaxVisits-1-j_visit, 0:l] = torch.LongTensor(visit)\n",
    "#             masks[i_patient, j_visit, 0:l] = 1\n",
    "#             rev_masks[i_patient, kMaxVisits-1-j_visit, 0:l] = 1\n",
    "#         # print(f\"------ v: {kMaxVisits} ------\")\n",
    "#         # print(x[i_patient, :, ])\n",
    "#         # print(rev_x[i_patient, :, ])\n",
    "#         # print(masks[i_patient, :, ])\n",
    "#         # print(rev_masks[i_patient, :, ])\n",
    "    \n",
    "#     return x, masks, rev_x, rev_masks, y\n",
    "\n",
    "\n",
    "def bert_per_patient_collate_function(data):\n",
    "    \"\"\"\n",
    "    Collates a tensor of (batch_size, seq_len, bert_emb_len) i.e. (32, <events per patient>, 768)\n",
    "    Need to pad the second dimension to max(<variable_per_patient>).\n",
    "    \n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "        sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "        is stored in `mask`.\n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "        x: a tensor of shape (batch_size, max #conditions, max embedding size) of type torch.float\n",
    "        masks: a tensor of shape (batch_size, max #conditions, max embedding_size) of type torch.bool\n",
    "        rev_x: same as x but in reversed time. This will be used in our RNN model for masking \n",
    "        rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "        y: a tensor of shape (batch_size) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\n",
    "        using: `sequences, labels = zip(*data)`\n",
    "    \"\"\"\n",
    "    # print(f\"bert_per_patient_collate_function data[0] {data[0]}\")\n",
    "    sequences, labels = zip(*data)\n",
    "    \n",
    "    # Quick stats on the amount of memory in each batch.\n",
    "    sizes = [t.element_size() * t.nelement() for t in sequences]\n",
    "    print(f'BertCollate tensor sizes\\n'\n",
    "          f'cum_size:{sum(sizes) / 1.0e6} MB\\n'\n",
    "          f'sizes:{sizes}\\n'\n",
    "          f'{[tuple(x.shape) for x in sequences]}\\n')\n",
    "    \n",
    "    # Convert output labels for each sample in batch to tensor.\n",
    "    # shape: (batch_size, 1)\n",
    "    y = torch.tensor(labels, dtype=torch.float)\n",
    "   \n",
    "    num_patients = len(sequences)\n",
    "    num_events = [patient.shape[0] for patient in sequences]\n",
    "    embedding_length = [patient.shape[1] for patient in sequences]\n",
    "\n",
    "    max_num_events = max(num_events)\n",
    "    max_embedding_length = max(embedding_length)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    rev_x = torch.zeros((num_patients, max_num_events, max_embedding_length), dtype=torch.float)\n",
    "    # Mask dimensions are 1 less than inputs.\n",
    "    masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_events), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(sequences):\n",
    "        # Patient (#events, 768)\n",
    "        j_visits = patient.shape[0]\n",
    "        # for j_visit, visit in enumerate(patient):\n",
    "        \"\"\"\n",
    "        TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "        \"\"\"\n",
    "        # l = len(visit)\n",
    "        x[i_patient, :j_visits, :] = patient[:, :].unsqueeze(0)\n",
    "        # The tensor is (seq_length, emb_size). Leave embeddings,\n",
    "        # flip temporal order of code/event sequence.\n",
    "        rev_x[i_patient, :j_visits, :] = torch.flip(patient, dims=[0]).unsqueeze(0)\n",
    "        masks[i_patient, :j_visits] = 1\n",
    "        rev_masks[i_patient, :j_visits] = 1\n",
    "      \n",
    "        # TODO(botelho3) - comment this out to reduce spew.\n",
    "        # if i_patient == 0:\n",
    "        #     print(f\"------ p: {i_patient} ------\")\n",
    "        #     print(x[i_patient, :, :25])\n",
    "        #     print(rev_x[i_patient, :, :25])\n",
    "        #     print(masks[i_patient, :, :25])\n",
    "        #     print(rev_masks[i_patient, :, :25])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32036013-ce5d-407b-af7a-312cf2bc5f15",
   "metadata": {},
   "source": [
    "#### Bert Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6afd1f5-0f3b-4f4b-9c85-f74bb367ef88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Could use DistilBERT to decrease embedding time.\n",
    "# https://huggingface.co/docs/transformers/model_doc/distilbert?highlight=distilberttokenizerfast#distilbert\n",
    "\n",
    "class BertTextEmbedTransform(object):\n",
    "    \"\"\"Transform a sample's (a single visit's) text into 1 embedding vector.\n",
    "    \n",
    "    The embeddings of each text field are combined by embedding\n",
    "    each separately then summing.\n",
    "\n",
    "    Args:\n",
    "        output_size (tuple or int): Desired output size. If tuple, output is\n",
    "            matched to output_size. If int, smaller of image edges is matched\n",
    "            to output_size keeping aspect ratio the same.\n",
    "    \"\"\"\n",
    "    # https://stackoverflow.com/questions/69517460/bert-get-sentence-embedding\n",
    "    # https://huggingface.co/docs/transformers/internal/tokenization_utils#transformers.tokenization_utils_base.PreTrainedTokenizerBase\n",
    "    # https://gmihaila.github.io/tutorial_notebooks/bert_inner_workings/\n",
    "\n",
    "    def __init__(self, bert_model: Any, embedding_size: int, use_tokenizer_fast: bool):\n",
    "        assert isinstance(embedding_size, (int, tuple))\n",
    "        self.use_gpu = BERT_USE_GPU_\n",
    "        self.cuda = GPU_STR_\n",
    "        self.cache_dir = os.path.join(os.getcwd(), CACHE_DIR_)\n",
    "        self.bert_config = BertConfig()\n",
    "        # self.bert_config = AutoConfig.from_pretrained(\"bert-base-uncased\")\n",
    "        # self.bert_model = BertModel(self.bert_config).from_pretrained('bert-base-uncased', config=self.bert_config)\n",
    "        # self.bert_model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "        with torch.no_grad():\n",
    "            self.bert_model = BertModel(self.bert_config).from_pretrained('bert-base-uncased')\n",
    "        if self.use_gpu:\n",
    "            self.bert_model.to(self.cuda)\n",
    "        self.bert_config = self.bert_model.config\n",
    "        self.bert_model.eval()\n",
    "        # We unfortunately can't put the tokenizer on the GPU.\n",
    "        # https://stackoverflow.com/questions/66096703/running-huggingface-bert-tokenizer-on-gpu\n",
    "        if use_tokenizer_fast:\n",
    "            self.tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "        else:\n",
    "            self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', model_max_length=100)\n",
    "    \n",
    "    def _tokenize_text(self, text: str) -> str:\n",
    "        tokenized_text = self.tokenizer.tokenize(text)\n",
    "        return tokenized_text\n",
    "    \n",
    "    def _get_embeddings_of_sentences_with_mask(self, field, pad) -> torch.tensor:\n",
    "        return self._get_embeddings_of_sentences(field[:pad])\n",
    "        \n",
    "    \n",
    "    def _get_embeddings_of_sentences(self, sentences: List[str]) -> torch.tensor:\n",
    "        # tokenized_sentences = [self.tokenizer.tokenize(t, padding=True) for t in sentences]\n",
    "        # Tokenize the input sentence with attention masks.\n",
    "        batch_enc = self.tokenizer.batch_encode_plus(sentences, padding=True,\n",
    "                                return_attention_mask=True, return_length=True)\n",
    "        # print(f'input sentence: {sentences[0]}\\n'\n",
    "        #       f'token sentence: {self.tokenizer.decode(batch_enc[\"input_ids\"][0])}\\n')\n",
    "        batch_enc_tensor = batch_enc.convert_to_tensors(tensor_type=TensorType.PYTORCH)\n",
    "        if self.use_gpu:\n",
    "            batch_enc_tensor.to(self.cuda)\n",
    "      \n",
    "        # Run BERT model forward pass on tokenized input.\n",
    "        with torch.no_grad():\n",
    "            embeddings = self.bert_model(input_ids=batch_enc_tensor['input_ids'],\n",
    "                                         attention_mask=batch_enc_tensor['attention_mask'],\n",
    "                                         token_type_ids=batch_enc_tensor['token_type_ids'],\n",
    "                                         # could turn off so we're faster\n",
    "                                         output_attentions=True)\n",
    "        # embeddings, _ = self.bert_model(**batch_enc)\n",
    "        # print(f'embeddings:\\n {dir(embeddings)}')\n",
    "        #attention = encoded['attention_mask'].reshape((lhs.size()[0], lhs.size()[1], -1)).expand(-1, -1, 768)\n",
    "       \n",
    "        # These may be on GPU.\n",
    "        return embeddings.last_hidden_state, embeddings.attentions \n",
    "    \n",
    "    def __call__(self, sample):\n",
    "        len_embeddings = (len(sample['conditions_text']) +\n",
    "                          len(sample['procedures_text']) +\n",
    "                          len(sample['drugs_text']))\n",
    "      \n",
    "        sample_text = []\n",
    "        sample_masks = []\n",
    "        if sample.get('conditions_text_pad'):\n",
    "            pad = sample['conditions_text_pad']\n",
    "            sample_text.extend(sample['conditions_text'][:pad])\n",
    "        if sample.get('procedures_text_pad'):\n",
    "            pad = sample['procedures_text_pad']\n",
    "            sample_text.extend(sample['procedures_text'][:pad])\n",
    "        if sample.get('drugs_text_pad'):\n",
    "            pad = sample['drugs_text_pad']\n",
    "            sample_text.extend(sample['drugs_text'][:pad])\n",
    "           \n",
    "        sample_embeddings, sample_attentions = self._get_embeddings_of_sentences(sample_text)\n",
    "        \n",
    "        # https://stackoverflow.com/questions/61323621/how-to-understand-hidden-states-of-the-returns-in-bertmodelhuggingface-transfo\n",
    "        # https://datascience.stackexchange.com/questions/66207/what-is-purpose-of-the-cls-token-and-why-is-its-encoding-output-important\n",
    "        # Take only the last hidden state embeddings from BERT.\n",
    "        # We need to take index 0, because it's the CLS token prepended to our sentence.\n",
    "        # The [CLS] represents the meaning of the whole sentence.\n",
    "        # embeddings = torch.squeeze(sample_embeddings[:, -1, :], dim=1)\n",
    "        embeddings = torch.squeeze(sample_embeddings[:, 0, :], dim=1)\n",
    "        if self.use_gpu:\n",
    "            assert(embeddings.device == torch.device('cuda:0'))\n",
    "        else:\n",
    "            assert(embeddings.device == torch.device('cpu'))\n",
    "        if self.use_gpu:\n",
    "            embeddings = embeddings.to('cpu')\n",
    "        \n",
    "        # We could multiply by attentions here:\n",
    "        # attentions = torch.squeeze(sample_attentions[:, -1, :], dim=1)\n",
    "        # embeddings = embeddings * attentions\n",
    "\n",
    "        # The 1st dimension is seq length. The second dimension is embedding length of each sentence.\n",
    "        assert(embeddings.shape[-1] == self.bert_config.hidden_size)\n",
    "        assert(len(embeddings.shape) == 2)\n",
    "        assert(embeddings.dtype == torch.float)\n",
    "        \n",
    "        # Return the ((model_inputs), label) \n",
    "        return (embeddings, sample['label'])\n",
    "    \n",
    "    \n",
    "class TextEmbedDataset(SampleDataset):\n",
    "    '''The BERT text embedding process is very slow. We want to avoid it.\n",
    "   \n",
    "    To prevent re-processing of the same input cache the sample locally.\n",
    "    Some suggestions here:\n",
    "        https://stackoverflow.com/questions/61393613/pytorch-speed-up-data-loading.\n",
    "        https://discuss.pytorch.org/t/best-practice-to-cache-the-entire-dataset-during-first-epoch/19608\n",
    "        \n",
    "    1. Preprocess and write the preprocessed text back out to disk.\n",
    "    2. Cache the transform output in a hashtable. See functools.lru_cache().\n",
    "    3. https://pytorch.org/data/main/ ?\n",
    "    \n",
    "    Some concerns related to num_workers > 1, i.e. multiprocessing enabled.\n",
    "    See torch.save() to cache a tensor.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, dataset: SampleDataset, transform=None):\n",
    "        \"\"\"Wraps a SampleEHRDataset with transforms.\n",
    "        Arguments:\n",
    "            dataset: dataset to transform\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        # self.dataset = dataset\n",
    "        self.transform = transform\n",
    "        self.transformed = [False for x in dataset]\n",
    "        super().__init__([x for x in dataset])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            assert(False)\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Cache the transformed version of the data.\n",
    "        if self.transform and not self.transformed[idx]:\n",
    "            self.samples[idx] = self.transform(self.samples[idx])\n",
    "            self.transformed[idx] = True\n",
    "\n",
    "        return self.samples[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1056b60a-5861-4e47-b694-891053cc8470",
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "\n",
    "# Create the transform that will take each sample (visit) in the dataset\n",
    "# and convert the text description of the visit into a single embedding.\n",
    "bert_xform = BertTextEmbedTransform(None, EMBEDDING_DIM_, use_tokenizer_fast=True)\n",
    "BERT_EMBEDDING_SIZE = bert_xform.bert_config.hidden_size\n",
    "\n",
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "# Add a transform to convert the visit into an embedding.\n",
    "MORT_DEMB_CODE2IDX = {}\n",
    "MORT_DEMB_CODE_COUNT = {}\n",
    "task_fn = functools.partial(mortality_pred_task_per_patient, MORT_DEMB_CODE_COUNT)\n",
    "mortality_demb_dataset = mimic3base.set_task(task_fn, task_name=mortality_pred_task_per_patient.__name__)\n",
    "MORT_DEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(MORT_DEMB_CODE_COUNT.keys()))\n",
    "}\n",
    "# Wrap the default pyHealth dataset class in our own wrapper. The wrapper takes each\n",
    "# sample and applies BERT to xform text->pytorch.tensor.\n",
    "mortality_demb_dataset = TextEmbedDataset(mortality_demb_dataset, transform=bert_xform)\n",
    "mort_demb_train_ds, mort_demb_val_ds, mort_demb_test_ds = split_by_patient(mortality_demb_dataset, [0.8, 0.1, 0.1])\n",
    "\n",
    "# create dataloaders (torch.data.DataLoader)\n",
    "# mort_train_loader = get_dataloader(train_ds, batch_size=32, shuffle=True, collate_fn)\n",
    "# mort_val_loader = get_dataloader(val_ds, batch_size=32, shuffle=False)\n",
    "# mort_test_loader = get_dataloader(test_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "mort_demb_train_loader = DataLoader(\n",
    "    mort_demb_train_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n",
    "mort_demb_val_loader = DataLoader(\n",
    "    mort_demb_val_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n",
    "mort_demb_test_loader = DataLoader(\n",
    "    mort_demb_test_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=bert_per_patient_collate_function,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "035fccee-d5da-428c-a54a-0678a2d19b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify DataLoader properties.\n",
    "# Quick test without running the whole RNN training process.\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(mort_demb_train_loader)\n",
    "# for _ in loader_iter:\n",
    "#     pass\n",
    "try:\n",
    "    x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "except StopIteration as e:\n",
    "    print(e)\n",
    "\n",
    "assert x.dtype == torch.float\n",
    "assert rev_x.dtype == torch.float\n",
    "assert y.dtype == torch.float\n",
    "assert masks.dtype == torch.bool\n",
    "assert rev_masks.dtype == torch.bool\n",
    "\n",
    "assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "assert y.shape == (BATCH_SIZE_, 1)\n",
    "assert masks.shape == (BATCH_SIZE_, 10, 3)\n",
    "\n",
    "# assert x[0][0].sum() == 9\n",
    "# assert masks[0].sum() == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "506c1852-2654-4d7c-9caa-4cd29ae6fd91",
   "metadata": {},
   "source": [
    "#### CodeEmb Collate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992467f7-d834-4d4a-872a-714389fa203c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def code_emb_per_visit_collate_function(code2idx: Dict[str, int], data: List[Any]) -> Tuple[Any]:\n",
    "    \"\"\"\n",
    "    TODO: Collate the the list of samples into batches. For each patient, you need to pad the diagnosis\n",
    "    sequences to the sample shape (max # visits, max # diagnosis codes). The padding infomation\n",
    "    is stored in `mask`.\n",
    "                                   \n",
    "    \n",
    "    Arguments:\n",
    "        data: a list of samples fetched from `CustomDataset`\n",
    "        \n",
    "    Outputs:\n",
    "    x: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.long\n",
    "    masks: a tensor of shape (# patients, max # visits, max # diagnosis codes) of type torch.bool\n",
    "    rev_x: same as x but in reversed time. This will be used in our RNN model for masking\n",
    "    rev_masks: same as mask but in reversed time. This will be used in our RNN model for masking\n",
    "    y: a tensor of shape (# patiens) of type torch.float\n",
    "        \n",
    "    Note that you can obtains the list of diagnosis codes and the list of hf labels\\n\",\n",
    "    using: `sequences, labels = zip(*data)`\\n\",\n",
    "    \"\"\"\n",
    "    \n",
    "    samples = data\n",
    "    \n",
    "    y = torch.tensor([s['label'] for s in samples], dtype=torch.float)\n",
    "    \n",
    "    num_patients = len(samples)\n",
    "    num_visits = [patient['num_visits'] for patient in samples]\n",
    "    num_codes = []\n",
    "    for patient_idx, _ in enumerate(num_visits):\n",
    "        num_codes.extend([len(visit) for visit in samples[patient_idx]['conditions']])\n",
    "        \n",
    "    # print(f'num samples: {len(samples)}')\n",
    "    # print(f'num visits: {num_visits}')\n",
    "    # print(f'num codes: {num_codes}')\n",
    "    # print(f'code_emb_per_patient_collate_function data[0]:\\n{data[0]}')\n",
    " \n",
    "    max_num_visits = max(num_visits)\n",
    "    max_num_codes = max(num_codes)\n",
    "    # max_num_codes = len(MORTALITY_PER_VISIT_ICD_9_CODE2IDX_)\n",
    "    assert(max_num_codes > 0)\n",
    "    \n",
    "    x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    rev_x = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.long)\n",
    "    masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    rev_masks = torch.zeros((num_patients, max_num_visits, max_num_codes), dtype=torch.bool)\n",
    "    for i_patient, patient in enumerate(samples):\n",
    "        nvisits = patient['num_visits']\n",
    "        for j_visit in range(nvisits):\n",
    "            \"\"\"\n",
    "            TODO: update `x`, `rev_x`, `masks`, and `rev_masks`\n",
    "            \"\"\"\n",
    "            codes = patient['conditions'][j_visit] \n",
    "            indices = [code2idx[code] for code in codes]\n",
    "            l = len(codes)\n",
    "            if len(indices) < 1:\n",
    "                print(f'No code indices after lookup')\n",
    "                print(f'codes {codes}'+'\\n'+f'indices{indices}')\n",
    "                print(f'patient\\n{patient}')\n",
    "                assert(len(indices) >= 1)\n",
    "            # for idx in indices:\n",
    "            x[i_patient, j_visit, 0:l] = torch.tensor(indices)\n",
    "            rev_x[i_patient, nvisits-1-j_visit, 0:l] = torch.tensor(indices)\n",
    "            masks[i_patient, j_visit, 0:l] = 1\n",
    "            rev_masks[i_patient, nvisits-1-j_visit, 0:l] = 1\n",
    "        # if i_patient == 0:\n",
    "        #     print(f\"------ code_emb_per_visit_collate_function p: {i_patient} ------\")\n",
    "        #     print(x[i_patient, :, ])\n",
    "        #     print(rev_x[i_patient, :, ])\n",
    "        #     print(masks[i_patient, :, ])\n",
    "        #     print(rev_masks[i_patient, :, ])\n",
    "    \n",
    "    return x, masks, rev_x, rev_masks, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b9670e0-b22c-4384-b8ad-f0fca7f1a80e",
   "metadata": {},
   "source": [
    "#### CodeEmb Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde4ae73-f079-47ae-ae6f-890d0c5fde07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mortality Task Datasets: set_task -> SampleEHRDataset(SampleBaseDataset)\n",
    "MORT_CEMB_CODE2IDX = {}\n",
    "MORT_CEMB_CODE_COUNT = {}\n",
    "mortality_cemb_ds = mimic3base.set_task(\n",
    "    task_fn=functools.partial(mortality_pred_task_cemb, MORT_CEMB_CODE_COUNT),\n",
    "    task_name=mortality_pred_task_cemb.__name__)\n",
    "# The set_task(...) function iterates over all samples.\n",
    "# Applying the task to each before returning a new dataset.\n",
    "# Since all samples have been processed we have observed all codes and can\n",
    "# build the code->index LUT.\n",
    "MORT_CEMB_CODE2IDX = {\n",
    "    code: idx for idx, code in enumerate(sorted(set(MORT_CEMB_CODE_COUNT.keys())))\n",
    "}\n",
    "\n",
    "# We need to provide the code->index LUT to the collate function.\n",
    "wrap_code_emb_per_visit_collate_function = functools.partial(\n",
    "    code_emb_per_visit_collate_function,\n",
    "    MORT_CEMB_CODE2IDX)\n",
    "\n",
    "# Split the dataset into train, val, and test.\n",
    "mort_cemb_train_ds, mort_cemb_val_ds, mort_cemb_test_ds = split_by_patient(mortality_cemb_ds, [0.8, 0.1, 0.1])\n",
    "mort_cemb_train_loader = DataLoader(\n",
    "    mort_cemb_train_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n",
    "mort_cemb_val_loader = DataLoader(\n",
    "    mort_cemb_val_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n",
    "mort_cemb_test_loader = DataLoader(\n",
    "    mort_cemb_test_ds,\n",
    "    batch_size=BATCH_SIZE_,\n",
    "    shuffle=SHUFFLE_,\n",
    "    collate_fn=wrap_code_emb_per_visit_collate_function,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320ac8e5-cab2-4540-b072-43a5b7404dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify DataLoader properties.\n",
    "# Quick test without running the whole RNN training process.\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# loader = DataLoader(mort_, batch_size=10, collate_fn=collate_fn)\n",
    "loader_iter = iter(mort_cemb_train_loader)\n",
    "for _ in loader_iter:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    x, masks, rev_x, rev_masks, y = next(loader_iter)\n",
    "except StopIteration as e:\n",
    "    print(e)\n",
    "\n",
    "# assert x.dtype == torch.float\n",
    "# assert rev_x.dtype == torch.float\n",
    "# assert y.dtype == torch.float\n",
    "# assert masks.dtype == torch.bool\n",
    "# assert rev_masks.dtype == torch.bool\n",
    "\n",
    "# assert x.shape == (BATCH_SIZE_, 3, 105)\n",
    "# assert y.shape == (BATCH_SIZE_, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc740d9c-bb55-4e54-b877-f1354ea793fc",
   "metadata": {},
   "source": [
    "## Model Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b9414b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, roc_curve, precision_recall_curve\n",
    "\n",
    "\n",
    "def eval_model(model, val_loader):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: evaluate the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        val_loader: validation dataloader\n",
    "        \n",
    "    Outputs:\n",
    "        precision: overall precision score\n",
    "        recall: overall recall score\n",
    "        f1: overall f1 score\n",
    "        roc_auc: overall roc_auc score\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "        \n",
    "    HINT: checkout https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    model.eval()\n",
    "    y_pred = torch.LongTensor()\n",
    "    y_score = torch.Tensor()\n",
    "    y_true = torch.LongTensor()\n",
    "    model.eval()\n",
    "    for x, masks, rev_x, rev_masks, y in val_loader:\n",
    "        y_hat = model(x, masks, rev_x, rev_masks)\n",
    "        y_score = torch.cat((y_score,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_hat = (y_hat > 0.5).int()\n",
    "        y_pred = torch.cat((y_pred,  y_hat.detach().to('cpu')), dim=0)\n",
    "        y_true = torch.cat((y_true, y.detach().to('cpu')), dim=0)\n",
    "    \"\"\"\n",
    "    TODO:\n",
    "        Calculate precision, recall, f1, and roc auc scores.\n",
    "        Use `average='binary'` for calculating precision, recall, and fscore.\n",
    "    \"\"\"\n",
    "    # print(f'y_pred: {torch.sum(y_pred)}, y_true {torch.sum(y_true)}\\n'\n",
    "    #       f'y_score\\n{y_score}\\n'\n",
    "    #       f'y_pred\\n{y_pred}\\n'\n",
    "    #       f'y_true\\n{y_true}\\n')\n",
    "    \n",
    "    p, r, f, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    roc_auc = roc_auc_score(y_true, y_score)\n",
    "    rcurve = roc_curve(y_true, y_score)\n",
    "    precision_curve, recall_curve, thresholds = precision_recall_curve(y_true, y_score)\n",
    "    \n",
    "    \n",
    "\n",
    "    # your code here\n",
    "    return p, r, f, roc_auc, rcurve, precision_curve, recall_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82cae503",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, val_loader, n_epochs, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    TODO: train the model.\n",
    "    \n",
    "    Arguments:\n",
    "        model: the RNN model\n",
    "        train_loader: training dataloder\n",
    "        val_loader: validation dataloader\n",
    "        n_epochs: total number of epochs\n",
    "        \n",
    "    You need to call `eval_model()` at the end of each training epoch to see how well the model performs \n",
    "    on validation data.\n",
    "        \n",
    "    Note that please pass all four arguments to the model so that we can use this function for both \n",
    "    models. (Use `model(x, masks, rev_x, rev_masks)`.)\n",
    "    \"\"\"\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for x, masks, rev_x, rev_masks, y in train_loader:\n",
    "            \"\"\"\n",
    "            TODO:\n",
    "                1. zero grad\n",
    "                2. model forward\n",
    "                3. calculate loss\n",
    "                4. loss backward\n",
    "                5. optimizer step\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(x, masks, rev_x, rev_masks)\n",
    "            loss = criterion(outputs, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # your code here\n",
    "            train_loss += loss.item()\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        print('Epoch: {} \\t Training Loss: {:.6f}'.format(epoch+1, train_loss))\n",
    "        p, r, f, roc_auc, rcurve, _, _ = eval_model(model, val_loader)\n",
    "        print('Epoch: {} \\t Validation p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "              .format(epoch+1, p, r, f, roc_auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e77bd4-a894-45ea-b621-e6da4d0029ee",
   "metadata": {},
   "source": [
    "### Model: CodeEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d556f942-8df2-4f8b-a89f-98731840ec3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cemb_sum_embeddings_with_mask(x, masks):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: the embeddings of diagnosis sequence of shape\n",
    "           (batch_size, # visits, # diagnosis codes, embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "    \n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, # visits, embedding_dim)\n",
    "    '''\n",
    "    tmp = torch.zeros(x.shape)\n",
    "    m = torch.sum(masks, dim=2) > 0\n",
    "    tmp[m, :, :] = 1\n",
    "    tmp = x * tmp\n",
    "    tmp = torch.sum(tmp, dim=2)\n",
    "    \n",
    "    a = x\n",
    "    a[~masks] = 0\n",
    "    tmp = torch.sum(a, dim=2)\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def cemb_get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape (batch_size, # visits, embedding_dim=128)\n",
    "        masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim=128)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "          and then use this length vector as index to select the last visit.\n",
    "    \"\"\"\n",
    "    m = torch.sum(masks, dim=2)\n",
    "    m[m > 0] = 1\n",
    "    m = torch.sum(m, dim=1)\n",
    "    m[m > 0] = m[m > 0] - 1\n",
    "   \n",
    "    # (batch_size,)\n",
    "    tmp1 = m\n",
    "    # (batch_size,1,1)\n",
    "    tmp = torch.reshape(m, (-1,1,1))\n",
    "    # (batch_size,1,embedding_dim)\n",
    "    tmp = tmp.expand(-1, -1, hidden_states.shape[2])\n",
    "    # print(tmp)\n",
    "    # print(tmp.shape)\n",
    "    last_hidden_state = torch.gather(hidden_states, axis=1, index=tmp)\n",
    "    # print(last_hidden_state.shape)\n",
    "    last_hidden_state = torch.squeeze(last_hidden_state)\n",
    "    \n",
    "    # print(last_hidden_state[3, :])\n",
    "    # print(torch.squeeze(hidden_states[3, tmp1[3], :]))\n",
    "    assert(torch.equal(last_hidden_state[0, :], torch.squeeze(hidden_states[0, tmp1[0], :])))\n",
    "    \n",
    "    return last_hidden_state\n",
    "\n",
    "\n",
    "class CembNaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_codes):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            1. Define the embedding layer using `nn.Embedding`. Set `embDimSize` to 128.\n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        \n",
    "        self.embedding = nn.Embedding(num_embeddings=num_codes, embedding_dim=128) \n",
    "        self.rnn = nn.GRU(input_size=128, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(input_size=128, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=256, out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, # visits, # diagnosis codes)\n",
    "            masks: the padding masks of shape (batch_size, # visits, # diagnosis codes)\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        # print(f\"pre embedding: {x.shape}\")\n",
    "        e = self.embedding(x)\n",
    "        # print(f\"post embedding: {e.shape}\")\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        e = cemb_sum_embeddings_with_mask(e, masks)\n",
    "        # print(f\"post sum_embeddings_with_mask: {e.shape}\")\n",
    "        \n",
    "        # 3. Pass the embeddings through the RNN layer;\n",
    "        output, _ = self.rnn(e)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = cemb_get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order (rev_x), and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "        # 1. Pass the sequence through the embedding layer;\n",
    "        rev_e = self.embedding(rev_x)\n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        rev_e = cemb_sum_embeddings_with_mask(rev_e, rev_masks)\n",
    "        \n",
    "        # 3. Pass the embegginds through the RNN layer;\n",
    "        output, _ = self.rnn(rev_e)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n_rev = cemb_get_last_visit(output, rev_masks)\n",
    "        \n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "        probs = self.sigmoid(logits)\n",
    "        return probs.view(batch_size)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04566514-f29b-4683-802f-f370867a907d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the model here\n",
    "mort_code_naive_rnn = CembNaiveRNN(num_codes = len(MORT_CEMB_CODE2IDX))\n",
    "mort_code_naive_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "938907a3-04c8-40f7-a772-f66d923e0832",
   "metadata": {},
   "source": [
    "### Model: DescEmb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a307f782",
   "metadata": {},
   "source": [
    "RNN running and visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe233a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demb_sum_embeddings_with_mask(x, masks):\n",
    "    '''\n",
    "    Inputs:\n",
    "        x: the embeddings of diagnosis sequence of shape (batch_size, # events(diag+proc+presc), embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # events(diag+proc+presc))\n",
    "    Outputs:\n",
    "        sum_embeddings: the sum of embeddings of shape (batch_size, embeddings_dim)\n",
    "    '''\n",
    "    # tmp = torch.zeros(x.shape)\n",
    "    # m = torch.sum(masks, dim=1) > 0\n",
    "    # tmp[m, :] = 1\n",
    "    # tmp = x * tmp\n",
    "    # tmp = torch.sum(tmp, dim=1)\n",
    "    return x\n",
    "    \n",
    "    a = x\n",
    "    a[~masks, :] = 0\n",
    "    # tmp = torch.sum(a, dim=1)\n",
    "    tmp = a\n",
    "    return tmp\n",
    "\n",
    "\n",
    "def demb_get_last_visit(hidden_states, masks):\n",
    "    \"\"\"\n",
    "    TODO: obtain the hidden state for the last true visit (not padding visits)\n",
    "\n",
    "    Arguments:\n",
    "        hidden_states: the hidden states of each visit of shape\n",
    "                       (batch_size, # events(diag+proc+presc), embedding_dim)\n",
    "        masks: the padding masks of shape (batch_size, # events(diag+proc+presc))\n",
    "\n",
    "    Outputs:\n",
    "        last_hidden_state: the hidden state for the last true visit of shape (batch_size, embedding_dim)\n",
    "        \n",
    "    NOTE: DO NOT use for loop.\n",
    "    \n",
    "    HINT: First convert the mask to a vector of shape (batch_size,) containing the true visit length; \n",
    "          and then use this length vector as index to select the last visit.\n",
    "    \"\"\"\n",
    "    m = torch.sum(masks, dim=1)\n",
    "    # m[m > 0] = 1\n",
    "    # m = torch.sum(m, dim=1)\n",
    "    m[m > 0] = m[m > 0] - 1\n",
    "    # print(f'selecting {m}')\n",
    "    \n",
    "    tmp1 = m\n",
    "    tmp = torch.reshape(m, (-1,1,1))\n",
    "    tmp = tmp.expand(-1, -1, hidden_states.shape[2])\n",
    "    last_hidden_state = torch.gather(hidden_states, axis=1, index=tmp)\n",
    "    last_hidden_state = torch.squeeze(last_hidden_state)\n",
    "    # print(f'tmp.shape {tmp.shape}\\n'\n",
    "    #       f'last_hidden_state {last_hidden_state.shape}\\n{last_hidden_state}')\n",
    "    assert(torch.equal(last_hidden_state[0, :], torch.squeeze(hidden_states[0, tmp1[0], :])))\n",
    "    \n",
    "    return last_hidden_state\n",
    "\n",
    "\n",
    "class DembNaiveRNN(nn.Module):\n",
    "    \n",
    "    \"\"\"\n",
    "    TODO: implement the naive RNN model above.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bert_emb_size:int=BERT_EMBEDDING_SIZE):\n",
    "        super().__init__()\n",
    "        \"\"\"\n",
    "        TODO: \n",
    "            2. Define the RNN using `nn.GRU()`; Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            2. Define the RNN for the reverse direction using `nn.GRU()`;\n",
    "               Set `hidden_size` to 128. Set `batch_first` to True.\n",
    "            3. Define the linear layers using `nn.Linear()`; Set `in_features` to 256, and `out_features` to 1.\n",
    "            4. Define the final activation layer using `nn.Sigmoid().\n",
    "\n",
    "        Arguments:\n",
    "            num_codes: total number of diagnosis codes\n",
    "        \"\"\"\n",
    "        self.bert_emb_size = bert_emb_size\n",
    "        # self.embedding = nn.Embedding(num_embeddings=self.bert_emb_size, embedding_dim=128) \n",
    "        self.rnn = nn.GRU(input_size=self.bert_emb_size, hidden_size=128, batch_first=True)\n",
    "        self.rev_rnn = nn.GRU(input_size=self.bert_emb_size, hidden_size=128, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=256,out_features=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        # Contiguous memory.\n",
    "        self.rnn.flatten_parameters()\n",
    "        self.rev_rnn.flatten_parameters()\n",
    "        \n",
    "        \n",
    "    def forward(self, x, masks, rev_x, rev_masks):\n",
    "        \"\"\"\n",
    "        Arguments:\n",
    "            x: the diagnosis sequence of shape (batch_size, #events(diag+proc+presc), embedding_dim)\n",
    "            masks: the padding masks of shape (batch_size, #events(diag+proc+presc))\n",
    "\n",
    "        Outputs:\n",
    "            probs: probabilities of shape (batch_size)\n",
    "        \"\"\"\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "        x = demb_sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "        # 3. Pass the embeddings through the RNN layer;\n",
    "        output, _ = self.rnn(x)\n",
    "        # 4. Obtain the hidden state at the last visit.\n",
    "        true_h_n = demb_get_last_visit(output, masks)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO:\n",
    "            5. Do the step 1-4 again for the reverse order (rev_x), and concatenate the hidden\n",
    "               states for both directions;\n",
    "        \"\"\"\n",
    "        # xr = self.embedding(rev_x)\n",
    "        xr = demb_sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "        routput, _ = self.rev_rnn(xr)\n",
    "        true_h_n_rev = demb_get_last_visit(routput, rev_masks)\n",
    "        \n",
    "        # 6. Pass the hidden state through the linear and activation layers.\n",
    "        logits =  self.fc(torch.cat([true_h_n, true_h_n_rev], 1))\n",
    "        probs = self.sigmoid(logits)\n",
    "        assert(probs.shape == (batch_size, 1))\n",
    "        return probs.view(batch_size)\n",
    "    \n",
    "        \n",
    "#     def forward_not_working(self, x, masks, rev_x, rev_masks):\n",
    "#         \"\"\"\n",
    "#         Arguments:\n",
    "#             x: the diagnosis sequence of shape (batch_size, #events(diag+proc+presc), embedding_dim)\n",
    "#             masks: the padding masks of shape (batch_size, #events(diag+proc+presc))\n",
    "\n",
    "#         Outputs:\n",
    "#             probs: probabilities of shape (batch_size)\n",
    "#         \"\"\"\n",
    "#         self.rnn.flatten_parameters()\n",
    "#         self.rev_rnn.flatten_parameters()\n",
    "        \n",
    "#         batch_size = x.shape[0]\n",
    "#         # e = self.embedding(x)\n",
    "        \n",
    "#         # 2. Sum the embeddings for each diagnosis code up for a visit of a patient.\n",
    "#         x = demb_sum_embeddings_with_mask(x, masks)\n",
    "        \n",
    "#         # 3. Pass the embeddings through the RNN layer;\n",
    "#         output, _ = self.rnn(x)\n",
    "#         # 4. Obtain the hidden state at the last visit.\n",
    "#         true_h_n = demb_get_last_visit(output, masks)\n",
    "        \n",
    "#         \"\"\"\n",
    "#         TODO:\n",
    "#             5. Do the step 1-4 again for the reverse order (rev_x), and concatenate the hidden\n",
    "#                states for both directions;\n",
    "#         \"\"\"\n",
    "#         # xr = self.embedding(rev_x)\n",
    "#         xr = demb_sum_embeddings_with_mask(rev_x, rev_masks)\n",
    "#         routput, _ = self.rev_rnn(xr)\n",
    "#         true_h_n_rev = demb_get_last_visit(routput, rev_masks)\n",
    "        \n",
    "#         # 6. Pass the hidden state through the linear and activation layers.\n",
    "#         logits = self.fc(torch.cat([true_h_n, true_h_n_rev], 1))        \n",
    "#         probs = self.sigmoid(logits)\n",
    "#         assert(probs.shape == (batch_size, 1))\n",
    "#         return probs.view(batch_size)\n",
    "    \n",
    "\n",
    "# load the model here\n",
    "mort_desc_naive_rnn = DembNaiveRNN()\n",
    "mort_desc_naive_rnn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeef88e-bc4b-454e-adb5-d65eee2169bf",
   "metadata": {},
   "source": [
    "### Train: CodeEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "479ded72",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training CodeEmb.')\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mort_code_naive_rnn.parameters(), lr=0.001)\n",
    "# number of epochs to train the model\n",
    "n_epochs =10 \n",
    "train(mort_code_naive_rnn, mort_cemb_train_loader, mort_cemb_val_loader,\n",
    "      n_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264fde9e-4bcc-4914-b6f4-6f9a8e8b8e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve, prec_curve, rec_curve = eval_model(mort_code_naive_rnn, mort_cemb_val_loader)\n",
    "print('Validation Set p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "      .format(p, r, f, roc_auc))\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec_curve, prec_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769dec9-05ef-42cd-be38-076c25d5e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve,  prec_curve, rec_curve = eval_model(mort_code_naive_rnn, mort_cemb_test_loader)\n",
    "print('Test Set p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "      .format(p, r, f, roc_auc))\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec_curve, prec_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b02354-bdfd-4064-8ffe-7d0dc807c8a1",
   "metadata": {},
   "source": [
    "### Train: DescEmb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d00785-0639-4365-b5dc-b00e4cc3ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.Adam(mort_desc_naive_rnn.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11370bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of epochs to train the model\n",
    "n_epochs = 5\n",
    "train(mort_desc_naive_rnn, mort_demb_train_loader, mort_demb_val_loader,\n",
    "      n_epochs, criterion, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ce9a2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve,  prec_curve, rec_curve = eval_model(mort_desc_naive_rnn, mort_demb_val_loader)\n",
    "print('Validation Set p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "      .format(p, r, f, roc_auc))\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec_curve, prec_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8341a4ae-e258-43ad-a0b1-ff9af60b18b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "p, r, f, roc_auc, rcurve,  prec_curve, rec_curve = eval_model(mort_desc_naive_rnn, mort_demb_test_loader)\n",
    "print('Test Set p: {:.2f}, r:{:.2f}, f: {:.2f}, roc_auc: {:.2f}'\n",
    "      .format(p, r, f, roc_auc))\n",
    "\n",
    "plt.plot(rcurve[0], rcurve[1])\n",
    "plt.show()\n",
    "\n",
    "plt.plot(rec_curve, prec_curve)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe2c2d9-c161-4b9d-8182-86384faea929",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Load BERT Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0edec-1570-4640-9926-d908b456f957",
   "metadata": {},
   "source": [
    "See instructions here: \n",
    "- https://pypi.org/project/pytorch-pretrained-bert/#examples\n",
    "- https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n",
    "- Could also get it from pytorch transformers library: https://pytorch.org/hub/huggingface_pytorch-transformers/\n",
    "- https://www.analyticsvidhya.com/blog/2021/05/all-you-need-to-know-about-bert/\n",
    "- https://pytorch.org/tutorials/beginner/nlp/word_embeddings_tutorial.html - general text embedding.\n",
    "- https://machinelearningmastery.com/handle-long-sequences-long-short-term-memory-recurrent-neural-networks/ handling long seq\n",
    "- Encoder/Decoder training for embedding vocab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2d58de-bf9a-4b57-977f-6aa3ac75e84f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# https://neptune.ai/blog/how-to-code-bert-using-pytorch-tutorial\n",
    "class BERTClassification(nn.Module):\n",
    "    def __init__ (self):\n",
    "        super(BERTClassification, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.bert_drop = nn.Dropout(0.4)\n",
    "        self.out = nn.Linear(768, 1)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, pooledOut = self.bert(ids, attention_mask = mask,\n",
    "                                token_type_ids=token_type_ids)\n",
    "        bertOut = self.bert_drop(pooledOut)\n",
    "        output = self.out(bertOut)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40665f4-db92-4567-b0f9-001bd35e9284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "\n",
    "# TODO(botelho3)`bert-base-uncased` is big. Load `bert-tiny` instead from the filesystem?\n",
    "# Model available at https://huggingface.co/prajjwal1/bert-tiny.\n",
    "# model = BERT_CLASS.from_pretrained(PRE_TRAINED_MODEL_NAME_OR_PATH, cache_dir=None)\n",
    "\n",
    "# Tokenized input\n",
    "text = \"[CLS] Who was Jim Henson ? [SEP] Jim Henson was a puppeteer [SEP]\"\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "\n",
    "# Mask a token that we will try to predict back with `BertForMaskedLM`\n",
    "masked_index = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "assert tokenized_text == ['[CLS]', 'who', 'was', 'jim', 'henson', '?', '[SEP]', 'jim', '[MASK]', 'was', 'a', 'puppet', '##eer', '[SEP]']\n",
    "\n",
    "# Convert token to vocabulary indices\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "# Define sentence A and B indices associated to 1st and 2nd sentences (see paper)\n",
    "segments_ids = [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1]\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "segments_tensors = torch.tensor([segments_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7539b72-a871-4d52-947b-075b5c4be2ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de132943-2c3e-405b-b2f8-5d348472672c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "if USE_GPU_:\n",
    "    tokens_tensor = tokens_tensor.to(GPU_STRING_)\n",
    "    segments_tensors = segments_tensors.to(GPU_STRING_)\n",
    "    model.to(GPU_STRING_)\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions = model(tokens_tensor, segments_tensors)\n",
    "\n",
    "# confirm we were able to predict 'henson'\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()\n",
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "assert predicted_token == 'henson'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02efb18a-626d-42d4-b19d-ba4b8f752a00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use Dataset Transforms to overwrite the old Dataset with a new transformed dataset.\n",
    "# Can either construct a new class BertDataset(Dataset): __init__(self, old_dataset)\n",
    "# https://pytorch.org/tutorials/beginner/data_loading_tutorial.html#dataset-class\n",
    "# Or can leverage transfomrs\n",
    "# class Rescale(object): __init__(self) -> class BertTransform() __init__(bert_model), __call__(self, sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "toc-autonumbering": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
